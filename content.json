{"pages":[{"title":"","text":"About Me分享出自《被讨厌的勇气》一书中的一段话： 关于自己的人生你能做到的就只有 “ 选择自己认为最好的道路 “ 。另一方面，别人如何评价你的选择，那是别人的课题，你根本无法左右。基本上，一切人际关系矛盾都起因于对别人的课题妄加干涉或者自己的课题被别人妄加干涉。只要能够进行课题分离，人际关系就会发生巨大改变。而辨别究竟是谁的课题的方法非常简单，只需要考虑一下”某种选择所带来的结果最终要由谁来承担？” So，但行好事，莫问前程～共勉！！ ->>>>>>>>>>>>>>>>>>>> 个人信息 90后一枚，正努力学习数据分析相关技能 信息管理与信息系统专业 爱好：运动、数独、炉石传说 本站信息 博客主要记录关于数据分析的技能学习的笔记类文章，好记性不如烂笔头嘛～ 另外会记录一下面试练习题以及一些自己遇到的坑(已解决或者未解决的),由于表达能力有限，有疑问处欢迎留言。 本站文章若无特别说明，均为原创，转载请注明来源。 关于博客 网站采用的hexo-theme-amazing主题 追求尽可能的简洁，清晰，易用。 在hexo-theme-amazing主题之上进行了部分修改。 感谢以下开源项目贡献者(不完全统计，如有遗漏欢迎留言)： Hexo Author:Hexo contributors Icarus Author:ppoffice amazing Author:removeif gitalk Author:Gitalk contributors","link":"/about/index.html"},{"title":"","text":"来而不往非礼也畅所欲言，有留必应","link":"/message/index.html"}],"posts":[{"title":"Matplotlib - 常用图表 &amp; python表格样式","text":"摘要Matplotlib库的使用，包括常用图表的绘制，以及表格样式 前言Matplotlib 是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。 通俗地说，matplotlib可能是数据分析中最常用的绘图Python包了。它可以对Python中的数据进行快速的可视化，并以多种格式输出。接下来，我们将以互动的方式介绍matplotlib中的大多数情况 导入模块本文基于Jupiter notebook环境下进行举例介绍，先导入使用到的python模块 1234import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 初步认识先从一维数组和二维数组来简单认识一下Matplotlib 1、一维数组​ 借助numpy模块构建一个一维数组，并生成折线图 1234567891011121314151617181920212223242526272829303132333435363738# 数据构建ts = pd.Series(np.random.randn(1000),index=pd.date_range('1/1/2018',periods=1000))ts = ts.cumsum()# 参数设置ts.plot( # line(折线图),bar(柱状图),barh(柱状图-横),kde(密度图) kind = 'line', # 图例标签，Dataframe格式以列名为label label = 'nb', # 风格字符串，这里包括了linestyle，marker，color style = '--g.', color = 'b', alpha = 0.6, grid = True, # 是否以index作为横坐标轴 use_index = True, # 横坐标旋转角度 rot = 45, # y轴界限 ylim = [-50,50], # y轴刻度值 yticks = list(range(-50,50,10)), figsize = (12,8), title = 'normal', # 是否显示图例，一般直接用plt.legend() legend = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x19d3c90&gt; 2、二维数组1234567891011121314151617181920# 数据构建df = pd.DataFrame(np.random.randn(1000,4),index=ts.index,columns=list('abcd'))# 返回累计和df = df.cumsum()# 参数设置df.plot(kind = 'line', style = '--.', grid = True, alpha = 0.3, use_index = True, rot = 30, figsize = (12,8), title = True, legend = True, subplots = False, colormap = 'Greens') &lt;matplotlib.axes._subplots.AxesSubplot at 0xd2cc490&gt; 常用图表下面介绍matplotlib在数据分析中的一些常用图表的绘制以及参数设置 柱状图plt.plot(kind = ‘bar/barh’) / plt.bar() 1234567891011121314151617181920212223# 数据、画布构建fig,axes = plt.subplots(4,1,figsize = (18,16))s = pd.Series(np.random.randint(0,10,16),index=list('abcdefghijklmnop'))df = pd.DataFrame(np.random.rand(10,3),columns=['a','b','c'])# 单系列s.plot(kind = 'bar',ax = axes[0], rot = 0)# 多系列df.plot(kind = 'bar',ax = axes[1])# 多系列堆叠图df.plot(kind = 'bar',stacked = True,ax = axes[2])# 另一种写法df.plot.bar(ax = axes[3])# plt.axis('tight') &lt;matplotlib.axes._subplots.AxesSubplot at 0x14008550&gt; 堆叠图123456789101112131415161718192021222324plt.figure(figsize = (12,8))x = np.arange(10)y1 = np.random.rand(10)y2 = -np.random.rand(10)plt.bar(x,y1,width = 1,facecolor = 'yellowgreen',edgecolor = 'white',yerr = y1*0.1)plt.bar(x,y2,width = 1,facecolor = 'lightskyblue',edgecolor = 'white',yerr = y2*0.1)# 参数解析：# width:宽度比例# facecolor:柱状图里填充的颜色，edgecolor:边框的颜色# left - 每个柱x轴左边界，bottom - 每个柱y轴下边界 → bottom扩展可化成甘特图 Gantt Chart# align：决定整个bar图分布，默认left表示从左边界开始绘制，center会将图绘制在中间位置# fig.tight_layout()plt.grid()for i,j in zip(x,y1): plt.text(i-0.15,0.05,'%.2f' % j ,color = 'white')for i,j in zip(x,y2): plt.text(i-0.15,-0.1,'%.2f' % -j ,color = 'white')# zip() 函数用于将可迭代对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表 外嵌图表图与表的结合，能更全面的展示数据，在展示了数据的可读性的同时，也具备了 1234567891011121314151617181920212223242526272829303132333435# 外嵌图表 plt.table()data = [[ 66386, 174296, 75131, 577908, 32015], [ 35713,21312,32133,12414,57565], [ 36588,56856,58756,98356,87589], [35356,36432,68886,87875,95636], [21415,56936,68587,98793,87935]]columns = ('Freeze','Wind','Flood','Quake','Hail')rows = ['%d year' % x for x in (100,50,20,10,5)]df = pd.DataFrame(data,columns=columns,index=rows)print(df)df.plot(kind = 'bar',grid = True,colormap = 'Blues_r',stacked = True,figsize = (12,8))# 创建堆叠图plt.table(cellText = data, cellLoc = 'center', cellColours = None, rowLabels = rows, rowColours = plt.cm.BuPu(np.linspace(0,0.5,5))[::-1],#BuPu相当于colormap colLabels = columns, colColours = plt.cm.Reds(np.linspace(0,0.5,5))[::-1], rowLoc = 'right', loc = 'bottom')# 参数解析：# cellText：表格文本# cellLoc：cell内文本对齐位置# rowLabels：行标签# colLabels：列标签# rowLoc：行标签对齐位置# loc：表格位置 → left,right,top,bottomplt.xticks([]) Freeze Wind Flood Quake Hail 100 year 66386 174296 75131 577908 32015 50 year 35713 21312 32133 12414 57565 20 year 36588 56856 58756 98356 87589 10 year 35356 36432 68886 87875 95636 5 year 21415 56936 68587 98793 87935 ([], &lt;a list of 0 Text xticklabel objects&gt;) 面积图1234567891011121314# 面积图: plot.areafig,axes = plt.subplots(2,1,figsize = (12,8))df1 = pd.DataFrame(np.random.rand(10,4),columns = ['a','b','c','d'])df2 = pd.DataFrame(np.random.randn(10,4),columns = ['a','b','c','d'])df1.plot.area(colormap = 'Greens_r',alpha = 0.6,ax = axes[0])df2.plot.area(stacked = False,colormap = 'Set2',alpha = 0.6,ax = axes[1])# stacked : 是否堆叠，默认情况下，区域图被堆叠# 为了产生堆积面积图，每列必须是全部为正值或全部为负值# 当数据有NaN时，自动填充0，所以图标签需要清洗掉缺失值 &lt;matplotlib.axes._subplots.AxesSubplot at 0x115591f0&gt; 填图123456789101112# 填图x1 = np.linspace(0,5*np.pi,1000)y3 = np.sin(x1)y5 = np.sin(2*x1)axes[1].fill_between(x1,y3,y5,color = 'b',alpha = 0.5,label = 'area')#填充两个函数之间的区域，使用fill_between函数for i in range(2): axes[i].legend() axes[i].grid() 饼图1234567891011121314151617181920212223242526272829303132333435363738# 饼图s = pd.Series(3*np.random.rand(4),index=['a','b','c','d'],name = 'series')plt.axis('equal')plt.pie(s, # 指定每部分的偏移量 explode=[0.1,0,0,0], # 标签 labels = s.index, # 颜色 colors = ['r','g','b','c'], # 饼图上的数据标签显示方式 autopct='%.2f%%', # 每个饼切片的中心和通过autopct生成的文本开始之间的比例 pctdistance=0.6, # 被画饼标记的直径，默认值：1.1 labeldistance=1.2, # 阴影 shadow = True, # 开始角度 startangle=0, # 半径 radius=1.5, # 图框 frame=False )# counterclock:指定指针方向，顺时针或者逆时针print(s) a 0.791267 b 2.460981 c 2.458505 d 2.777957 Name: series, dtype: float64 直方图1234567891011121314151617181920212223# 直方图 plt.hist()s = pd.Series(np.random.randn(1000))s.hist( # 箱子个数 bins = 20, # 风格：bar,barstacked,step,stepfilled histtype = 'bar', # 对齐方式：left,right,mid align = 'mid', # 水平还是垂直：'horizontal','vertical' orientation = 'vertical', alpha = 0.5, # 标准化 normed = True )s.plot(kind = 'kde',style = 'k--') &lt;matplotlib.axes._subplots.AxesSubplot at 0x15226d10&gt; 堆叠直方图1234567891011121314151617# 堆叠直方图plt.figure(num = 1)df = pd.DataFrame({'a':np.random.randn(1000) + 1, 'b':np.random.randn(1000), 'c':np.random.randn(1000) - 1, 'd':np.random.randn(1000) - 2}, columns=['a','b','c','d'])df.plot.hist(stacked = True, bins = 20, colormap = 'Greens_r', alpha = 0.5, grid = True, edgecolor = 'black')df.hist(bins = 50)# 直接生成多个直方图 array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1510E290&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x15745CD0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x15761AF0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1577CA10&gt;]], dtype=object) &lt;Figure size 432x288 with 0 Axes&gt; 散点图123456789101112131415161718# 散点图 # plt.scatter('x', 'y', 's=None', 'c=None', 'marker=None', 'cmap=None', 'norm=None','vmin=None', 'vmax=None', # 'alpha=None', 'linewidths=None', 'verts=None', 'edgecolors=None', '*', 'data=None', '**kwargs'）plt.figure(figsize = (8,6))x = np.random.randn(1000)y = np.random.randn(1000)plt.scatter(x,y,marker = '.', s = np.random.randn(1000)*100, c = y*100, cmap = 'Reds', alpha = 0.8)plt.grid()# vmin,vmax：亮度设置，标量 矩阵散点图123456789101112# 矩阵散点图df = pd.DataFrame(np.random.randn(100,4),columns=['a','b','c','d'])pd.scatter_matrix(df,figsize = (10,6), marker = 'o', diagonal='kde', alpha = 0.5, range_padding=0.1)# diagonal:({'hist','kde'})，必须只能在两个中选其一， → 每个指标的频率图# range_padding : (float,可选),图像在x轴、y轴原点附近的留白，值越大，留白距离越大，图像远离坐标原点 极坐标图123456789101112131415161718192021222324252627# 极坐标图s = pd.Series(np.arange(20))theta = np.arange(0,2*np.pi,0.02)# print(s.head())# print(theta[:10])# 创建数据fig = plt.figure(figsize = (12,8))ax1 = plt.subplot(121,projection = 'polar')ax2 = plt.subplot(122)# projection = 'polar' → 创建极坐标图# ax = fig.add_subplot(111,polar = True)ax1.plot(theta,theta*3,linestyle = '--',lw = 1)ax1.plot(s,linestyle = '--',marker = '.',lw = 2)ax2.plot(theta,theta*3,linestyle = '--',lw = 1)ax2.plot(s)plt.grid()# 创建极坐标图，参数1为角度(弧度制)，参数2为value# lw → 线宽 1234567891011121314151617181920212223242526272829# 极坐标参数设置theta = np.arange(0,2*np.pi,0.02)plt.figure(figsize = (8,4))ax1 = plt.subplot(121,projection = 'polar')ax2 = plt.subplot(122,projection = 'polar')ax1.plot(theta,theta/6,'--',lw = 2)ax2.plot(theta,theta/6,'--',lw = 2)# set_theta_direction : 坐标轴正方形，默认逆时针ax2.set_theta_direction(-1)# set_thetagrids : 设置极坐标角度网格线显示及标签 → 网格和标签数量一致ax2.set_thetagrids(np.arange(0.0,360.0,90),['a','b','c','d'])# set_rgrids : 设置极径网格线显示，其中参数必须是正数ax2.set_rgrids(np.arange(0.2,2,0.4))# set_theta_offset : 设置角度偏移,逆时针,弧度制ax2.set_theta_offset(np.pi/2)# set_rlim : 设置显示的极径范围ax2.set_rlim(0.2,1.2)# set_rmax : 设置显示的极径最大值ax2.set_rmax(2)# set_rticks : 设置极径网格线的显示范围ax2.set_rticks(np.arange(0.1,1.5,0.2)) 雷达图123456789101112131415161718192021# 雷达图1 - 极坐标的折线图/填图# plt.plot画出的雷达图首尾不相连plt.figure(figsize = (12,8))ax1 = plt.subplot(111,projection = 'polar')ax1.set_title('radar map\\n')ax1.set_rlim(0,12)data1 = np.random.randint(1,10,10)data2 = np.random.randint(1,10,10)data3 = np.random.randint(1,10,10)theta = np.arange(0,2*np.pi,2*np.pi/10)ax1.plot(theta,data1,'--',label = 'data1')ax1.fill(theta,data1,alpha = 0.2)ax1.plot(theta,data2,'--',label = 'data1')ax1.fill(theta,data2,alpha = 0.2)ax1.plot(theta,data3,'--',label = 'data1')ax1.fill(theta,data3,alpha = 0.2) [&lt;matplotlib.patches.Polygon at 0xe585fd0&gt;] 雷达图进阶使用​ 雷达图与极坐标图、填图的组合使用 123456789101112131415161718192021# 雷达图2 - 极坐标的折线图/填图 # plt.polar() → 首尾闭合labels = np.array(['a','b','c','d','e','f'])dataLenth = 6data1 = np.random.randint(0,10,6)data2 = np.random.randint(0,10,6)angles = np.linspace(0,2*np.pi,dataLenth,endpoint=False) #分割圆周长data1 = np.concatenate((data1,[data1[0]])) #闭合data2 = np.concatenate((data2,[data2[0]]))angles = np.concatenate((angles,[angles[0]]))plt.polar(angles,data1,'o-',linewidth = 1)plt.fill(angles,data1,alpha = 0.25)plt.polar(angles,data2,'o-',linewidth = 1)plt.fill(angles,data2,alpha = 0.25)plt.thetagrids(angles * 180/np.pi,labels)plt.ylim(0,10) (0, 10) 极轴图12345678910111213141516# 极轴图 - 极坐标的柱状图plt.figure(figsize = (12,8))ax1 = plt.subplot(111,projection = 'polar')ax1.set_title('rader map\\n')ax1.set_rlim(0,12)data = np.random.randint(1,10,10)theta = np.arange(0,2*np.pi,2*np.pi/10)bar = ax1.bar(theta,data,alpha = 0.5)for r,bar in zip(data,bar): bar.set_facecolor(plt.cm.jet(r/10.))plt.thetagrids(np.arange(0.0,360.0,90),[]) (&lt;a list of 8 Line2D thetagridline objects&gt;, &lt;a list of 4 Text thetagridlabel objects&gt;) 箱型图​ 箱型图：又称箱线图、盒须图，是一种用作显示一组数据分散情况资料的统计图 123456789101112131415161718192021222324252627# 1、中位数：一组数据平均分成两份，中间的数# 2、上四分位数Q1：是将序列平均分成四份，计算(n+1)/4与(n-1)/4两种，一般使用(n+1)/4# 3、下四分位数Q3：是将序列平均分为四份，计算(1+n)/4*3 = 0.75# 4、内限 → T形的盒须就是内限，最大值区间Q3+1.5IQR,最小区间Q1-1.5IQR(IQR = Q3 - Q1)# 5、外限 → T形的盒须就是内限，最大值区间Q3+3IQR,最小区间Q1-3IQR(IQR = Q3 - Q1)# 6、异常值：内限之外→中度异常，外限之外→极度异常fig,axes = plt.subplots(2,1,figsize = (12,8))df = pd.DataFrame(np.random.rand(10,5),columns=['a','b','c','d','e'])color = dict(boxes = 'DarkGreen',whiskers = 'DarkOrange',medians = 'DarkBlue', caps = 'Gray')df.plot.box(ylim = [0,1.2], grid = True, color = color, ax = axes[0])df.plot.box(vert = False, positions = [1,4,5,6,8], ax = axes[1], grid = True, color = color) &lt;matplotlib.axes._subplots.AxesSubplot at 0x109bccd0&gt; 箱型图-11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# .boxplotdf = pd.DataFrame(np.random.rand(10,5),columns=['a','b','c','d','e'])plt.figure(figsize = (12,8))# 构建箱型图f = df.boxplot( # 异常点的形状 sym = 'o', # 是否垂直 vert = True, # IQR，默认1.5，可以设置区间[5,95]，代表上下边缘为数据的95%和5% whis = 1.5, # 上下四分位框内是否填充 patch_artist = True, # 是否有均值线及其形状 meanline = False,showmeans = True, # 是否显示箱线 showbox = True, # 是否显示边缘线 showcaps = True, # 是否显示异常值 showfliers = True, # 中间箱体是否缺口 notch = False, # 返回类型为字典 return_type = 'dict')plt.title('boxplot')print(f)# 其余参数设置for box in f['boxes']: # 箱体边框颜色 box.set(color = 'b',linewidth = 1) # 箱体内部填充颜色 box.set(facecolor = 'b',alpha = 0.5)for whisker in f['whiskers']: whisker.set(color = 'k',linewidth = 0.5,linestyle = '-')for cap in f['caps']: cap.set(color = 'gray',linewidth = 2)for median in f['medians']: median.set(color = 'DarkBlue',linewidth = 2)for flier in f['fliers']: flier.set(marker = 'o',color = 'y',alpha = 0.5)# 参数解析：# boxes：箱线# medians：中位线的横线# whiskers：从box到error bar之间的竖线# fliers：异常值# caps：error bar横线# means：均值的横线 箱型图-2123456789101112131415# plt.boxplot() 绘制# 分组汇总df = pd.DataFrame(np.random.rand(10,2),columns = ['col1','col2'])df['x'] = pd.Series(['a','a','a','a','a','b','b','b','b','b'])df['y'] = pd.Series(['a','b','a','b','a','b','a','b','a','b'])print(df.head())# df.boxplot(by = 'x')df.boxplot(column = ['col1','col2'],by = ['x','y'])# columns:按照数据的列分子图# by：按照列分组做箱型图 col1 col2 x y 0 0.507773 0.223859 a a 1 0.128340 0.482120 a b 2 0.955340 0.912310 a a 3 0.170645 0.949025 a b 4 0.821798 0.059242 a a array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x13B3BA30&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1115FED0&gt;], dtype=object) python的表格样式1、表格样式创建1234567891011121314151617181920212223# 样式创建# 1、styler.applymap: elementwise → 按元素方式处理df，也就是按照每一个值处理# 2、styler.apply: column- / row- / tabel_wise → 按行/列处理dfdf = pd.DataFrame(np.random.randn(10,4),columns=['a','b','c','d'])sty = df.style# print(sty,type(sty))def color_neg_red(val): if val &lt; 0: color = 'red' else: color = 'black' return('color:%s' % color)df.style.applymap(color_neg_red)# 创建样式方法，使得小于0的数变成红色# style.applymap() → 自动调用其中的函数 2、样式处理12345678910111213141516171819202122# 按列/行处理样式：style.apply()df = pd.DataFrame(np.random.randn(10,4),columns=['a','b','c','d'])sty = df.styledef highlight_max(s): is_max = s == s.max() lst = [] for v in is_max: if v: lst.append('background-color: yellow') else: lst.append('') return(lst)# df.style.apply(highlight_max,axis = 0,subset = ['b','c'])# subset:选择索引进行函数处理df.style.apply(highlight_max,axis = 1, subset = pd.IndexSlice[2:5,['b','d']])# df[2:5].style.apply(highlight_max,subset = ['b','d']) 3、内容显示123456789101112131415# 表格显示控制df = pd.DataFrame(np.random.randn(10,4),columns=['a','b','c','d'])# df.head().style.format('{:.2%}') #显示百分比# df.head().style.format('{:.4f}') #显示小数点# df.head().style.format(&quot;{:+.2f}&quot;) #显示正负数# 分列显示df.head().style.format({'b':&quot;{:.2%}&quot;,'c':&quot;{:+.3f}&quot;,'d':&quot;{:.3f}&quot;}) 表格进阶1、应用 — 空值定位1234567# 内置样式调用# 1、定位空值df = pd.DataFrame(np.random.rand(5,4),columns=list('abcd'))df['a'][2] = np.nandf.style.highlight_null(null_color = 'yellow') 2、应用 — 色彩映射1234# 2、色彩映射df = pd.DataFrame(np.random.rand(10,4),columns=list('abcd'))df.style.background_gradient(cmap = 'Greens',axis = 1,low = 0,high = 1) 3、应用 — 条形图显示12345# 3、条形图df = pd.DataFrame(np.random.rand(10,4),columns=list('abcd'))df.style.bar(subset = ['a','b'],color = '#d65f5f',width = 100)# width: 最长长度在格子的占比 4、应用 — 分段式显示12345678# 分段式构建样式df = pd.DataFrame(np.random.rand(10,4),columns=list('abcd'))df['a'][3] = np.nandf['b'][7] = np.nandf.style.\\ bar(subset = ['a','b'],color = '#d33f5f',width = 100).\\ highlight_null(null_color = 'yellow') 总结以上为数据分析中matplotlib库中的常用图表的绘制以及各个图表的参数说明，记录下来方便查漏补缺的同时，可供随时翻阅，有哪些地方出现错误或者疑义，欢迎讨论，感谢阅读～ 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/09/Matplotlib%E5%85%A5%E9%97%A8%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"MySQL数据库 - 初步认识","text":"摘要有关MySQL数据库的基础知识，包括一些专用名称解析以及必会知识点，有待更正与完善～ 初识MySQL数据库我们在编写任何程序之前，都需要事先写好基于网络操作一台主机上文件的程序（socket服务端与客户端程序），于是有人将此类程序写成一个专门的处理软件，这就是mysql等数据库管理软件的由来，但mysql解决的不仅仅是数据共享的问题，还有查询效率，安全性等一系列问题。 总之，把程序员从数据管理中解脱出来，专注于自己的程序逻辑的编写~。 专有名词数据（Data）：描述事物的符号记录称为数据，描述事物的符号既可以是数字，也可以是文字、图片，图像、声音、语言等，数据由多种表现形式，它们都可以经过数字化后存入计算机；在计算机中描述一个事物，就需要抽取这一事物的典型特征，组成一条记录，就相当于文件里的一行内容。 数据库（Databases，简称DB）：数据库库即存放数据的仓库，只不过这个仓库是在计算机存储设备上，而且数据是按一定的格式存放的；数据库是长期存放在计算机内、有组织、可共享的数据即可。数据库中的数据按一定的数据模型组织、描述和储存，具有较小的冗余度、较高的数据独立性和易扩展性，并可为各种用户共享。 数据库管理系统（DataBase Management System 简称DBMS）在了解了Data与DB的概念后，如何科学地组织和存储数据，如何高效获取和维护数据成了关键~~ 这就用到了一个系统软件—数据库管理系统如MySQL、Oracle、SQLite、Access、MS SQL Server 常见的数据库模型分为关系型数据库（MySQL、Oracle、SQL Server….）和非关系型数据库（文档存储数据库MongoDB；键值存储数据库Redis、Memcached、列存储数据库HBase、图形数据库Neo4J） 数据类型 数字： 整型： tinyint [(m)] [unsigned] [zerofill]：小整数，数据类型用于保存一些范围的整数 数值范围： ​ 有符号：-128 ～ 127 ​ 无符号：0 ～ 255 注意： MySQL中无布尔值，使用tinyint(1)构造。 int [(m)] [unsigned] [zerofill]：整数，数据类型用于保存一些范围的整数 数值范围： ​ 有符号：-2147483648 ～ 2147483647 ​ 无符号：0 ～ 4294967295 bigint [(m)] [unsigned] [zerofill]：大整数，数据类型用于保存一些范围的整数 数值范围： ​ 有符号：-9223372036854775808 ～ 9223372036854775807 ​ 无符号：0 ～ 18446744073709551615 zerofill 使用说明：例如 int(5)表示当数值宽度小于 5 位的时候在数字前面加’0’填满宽度,如果不显示指定宽度则默认为 int(11)，zerofill**默认为int(10)**。 注:当使用zerofill 时，默认会自动加unsigned（无符号）属性，使用unsigned属性后，数值范围是原值的2倍，例如，有符号为-128～+127，无符号为0~256。 小数：m是数字总个数，d是小数点后个数。m最大值为255，d最大值为30。 ​ float [(M,D)] [unsigned] [zerofill]：单精度浮点数（非准确小数值） ​ 特性：随着小数的增多，精度变得不准确 ​ double [(M,D)] [unsigned] [zerofill]：双精度浮点数（非准确小数值）。 ​ 特性：随着小数的增多，精度比float要高，但也会变得不准确 ​ decimal [(M[,D])] [unsigned] [zerofill]：准确的小数值。 ​ m是数字总个数（负号不算），d是小数点后个数。 m最大值为65，d最大值为30。 ​ 特性：随着小数的增多，精度始终准确。decaimal能够存储精确值的原因在于其内部按照字符串存储。 字符串 char（10）：定长 不够的给你补上。特点：简单粗暴，浪费空间，存取速度快。 varchar：变长 传几个给你写几个，但不要超过字符个数精准。特点：节省空间，但存取速度慢。 大于255字符，可以考虑将文件路径存放到数据库中，即数据库中只存路径或者url。 时间类型 year ：年 date ：年月日 time ：时分秒 datetime ：年月日时分秒 枚举类型与集合类型：字段的值只能在给定范围中选择，如单选框，多选框 enum 单选 只能在给定的范围内选一个值，如性别 sex 男male/女female set 多选 在给定的范围内可以选择一个或一个以上的值（爱好1,爱好2,爱好3…） 索引索引简介索引（Index）是帮助MySQL高效获取数据的数据结构。可以得到索引的本质：索引是数据结构。 数据本身之外，数据库还维护着一个满足特定查找算法的数据结构，这些数据结构以某种方式指向数据，这样就可以在这些数据结构的基础上实现高级查找算法，这种数据结构就是索引。 所以，索引就是一种帮助MySQL高效获取数据的排好序的快速查找的数据结构。 索引优劣 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上 优势：类似大学图书馆建书目索引，提高数据检索的效率，降低数据库的IO成本；通过索引列对数据进行排序，降低数据排序的成本，降低了CPU的消耗 劣势：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，都会调整因为更新所带来的键值变化后的索引信息；实际上索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录，所以索引列也是要占用空间的。 索引结构 各种结构探寻：https://www.cs.usfca.edu/~galles/visualization/Algorithms.html 二叉树（红黑树：也叫二叉平衡树） HASH B - TREE 度(Degree)-节点的数据存储个数 叶节点具有相同的深度 叶节点的指针为空 节点中的数据key从左到右递增排列 B + TREE（B - TREE演变） 非叶子节点不存储data,只存储key,可以增大度 叶子节点不存储指针 顺序访问指针,提高区间访问的性能 B+Tree索引的性能分析一般使用磁盘I/O次数评价索引结构的优劣，根据索引获取一条数据使用的I/O次数越少越优。 预读：磁盘一般会顺序向后读取一定长度的数据(页的整数倍)放入内存 局部性原理：当一个数据被用到时,其附近的数据也通常会马上被使用 B+Tree节点的大小设为等于一个页,每次新建节点之间申请一个页的空间，这样就保证了一个节点物理上页存储在一个页里就实现了一个节点的载入只需一次I/O B+Tree的度d一般会超过100,因此h非常小(一般为1到3之间,极限到5) 一般操作系统的最小存储单元为页,1页大小为4K ‘SHOW GLOBAL STATUS like ‘Innodb_page_size’语句可以查看mysql文件页大小 索引分类 单值索引：即一个索引只包含单个列，一个表可以有多个单列索引 唯一索引：索引列的值必须唯一，但允许有空值 主键索引：设定为主键后数据库会自动建立索引，innodb为聚簇索引 复合索引：即一个索引包含多个列 MySQL常见存储引擎 MyISAM索引实现(非聚集) MyISAM索引文件和数据文件时分离的 InnoDB索引实现(聚集) 表数据文件本身就是按B+Tree组织的一个索引结构文件 聚集索引 - 叶节点包含了完整的数据记录 InnoDB表必须有主键,并且推荐使用整型的自增主键 非主键索引结构叶子节点存储的时主键值(一致性和节省存储空间) 完整型约束完整型约束的作用：用于保证数据的完整性和一致性 是否允许为空，默认NULL，可设置NOT NULL，字段不允许为空，必须赋值 字段是否有默认值，缺省的默认值是NULL，如果插入记录时不给字段赋值，此字段使用默认值 是否为key ：主键 primary key、外键 foreign key、索引 (index,unique…) PRIMARY KEY (PK) 标识该字段为该表的主键，可以唯一的标识记录（不为空且唯一） FOREIGN KEY (FK) 标识该字段为该表的外键 NOT NULL 标识该字段不能为空 UNIQUE KEY (UK) 标识该字段的值是唯一的 单列唯一 ：在字段后加unique，指的是这个字段的记录是唯一的不能重复 联合唯一 ：例如ip和端口均是唯一的，这种叫联合唯一 AUTO_INCREMENT 标识该字段的值自动增长（整数类型，而且为主键） DEFAULT 为该字段设置默认值 查询优化单表使用索引以及常见的索引失效 全值匹配 最佳左前缀法则：如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。 不在索引列上做任何操作（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描 存储引擎不能使用索引中范围条件右边的列； mysql 在使用不等于(!= 或者&lt;&gt;)的时候无法使用索引会导致全表扫描； is not null 也无法使用索引,但是is null是可以使用索引的； like以通配符开头(‘%abc…’)mysql索引失效会变成全表扫描的操作； 字符串不加单引号也会引起索引失效 建议： 对于单键索引，尽量选择针对当前query过滤性更好的索引；在选择组合索引的时候，当前Query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。 在选择组合索引的时候，尽量选择可以能够包含当前query中的where字句中更多字段的索引； 在选择组合索引的时候，如果某个字段可能出现范围查询时，尽量把这个字段放在索引次序的最后面； 关联查询优化 保证被驱动表的join字段已经被索引 left join时，选择小表作为驱动表（也就是主表），大表作为被驱动表（从表） inner join时，mysql会自动将小结果集的表选为驱动表 子查询尽量不要放在被驱动表，有可能使用不到索引 能够直接多表关联的尽量直接关联，不使用子查询 子查询优化 尽量不要使用not in 或者 not exists，用left outer join on xxx is null 替代； 排序分组优化～待补充 pymysql使用1234567891011121314151617181920212223242526272829303132333435import pymysqluser=input('user&gt;&gt;: ').strip()pwd=input('password&gt;&gt;: ').strip()# 建立链接conn=pymysql.connect( host='192.168.1.123', port=3306, user='root', password='123', db='db10', charset='utf8')# 拿到游标cursor=conn.cursor()# 执行sql语句# sql='select * from userinfo where user = &quot;%s&quot; and pwd=&quot;%s&quot;' %(user,pwd)# print(sql)# rows=cursor.execute(sql)sql='select * from userinfo where user = %s and pwd=%s'#由execute作为拼接，不用你自己去拼接了，在拼接过程中给你过滤掉这种非法操作rows=cursor.execute(sql,(user,pwd)) #提交给游标执行 execute这个接口拿到的是2 rows in set (0.00 sec) 2那个行数，如果值不为0说明就输对了cursor.close()conn.close()# 进行判断if rows: print('登录成功')else: print('登录失败') 写在后面以上是学习MySQL数据库的一下学习笔记，记录下来以供随时翻阅，查漏补缺，感谢阅读～","link":"/2020/08/03/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%20-%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86/"},{"title":"博客文章属性","text":"摘要博客文章属性解析.. post 模板本主题下post模板内容 -– thumbnail: title: 博客文章属性 date: 1622012314000 tags: -test1 -test2 categories: -test3 -test4 toc: true recommend: 1 keywords: categories-java uniqueId: 1622012314000/博客文章属性.html mathJax: false -– &gt; 摘要 首页显示摘要内容（替换成自己的） 正文内容（替换成自己的） 参考文章: 参考链接 参数 参数 名称解析 thumbnail 文首缩略图，值：图片url title 文章标题 date 文章创建日期 tags 标签，无顺序，无层级 categories 分类，有顺序，体现分类层级，例：[python,library] toc 启用章内索引，值：true/false recommend 文章推荐，值必须大于0,且值越大越靠前，相等取最新，最多5条 keywords categories-java uniqueId 1622012314000/博客文章属性.html mathJax 数学公式渲染，值：false/true encrypt 文章加密，值：false/true；加密文章最好设置top：-1,将其排在最后 &lt;!--more--&gt; 文章折叠 参考文章: 参考链接 - Icarus用户指南","link":"/2021/05/26/blog_post/"},{"title":"Seaborn - 基础风格展示 &amp; 调色盘","text":"摘要Seaborn库的使用，包括一些基础风格的参数设置，以及调色盘(各色系的设置)～ 写在前面Matplotlib试着让简单的事情更加简单，困难的事情变得可能，而Seaborn就是让困难的东西更加简单。 实际上，Seaborn 是在matplotlib的基础上进行了更高级的 API 封装，从而使得作图更加容易 用Matplotlib最大的困难是其默认的各种参数，而Seaborn则完全避免了这一问题。 seaborn是针对统计绘图的，一般来说，seaborn能满足数据分析大部分的基本绘图需求，就能做出很具有吸引力的图，如果需要复杂的自定义图形，还是要Matplotlib。 官网：http://seaborn.pydata.org/index.html 安装：直接 pip3 install seaborn 即可 seaborn风格设置 Seaborn模块自带许多定制的主题和高级的接口，包括对图表整体颜色、比例等进行风格设置,包括颜色色板等，调用其系统风格即可实现各种不同风格的数据可视化 导入模块 本文基于jupyter notebook环境，先导入使用到的python模块，并创建一个正弦函数及绘制图表 12345import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 1234567# 创建正弦函数及图表def sinplot(flip = 1): x = np.linspace(0,14,100) for i in range(1,7): plt.plot(x,np.sin(x + i * .5) * (7 - i) * flip)sinplot() set set( ) ：通过设置参数可以用来设置背景，调色板等，最为常用。 123456# tips：在jupyter notebook中，一旦设定了风格,所有图表的创建均自动带有风格,重启服务后才会重置为无风格状态# 使用默认设置sns.set()sinplot()plt.grid(linestyle = '--') set_style set_style() ：设置主题，即切换seaborn图表风格 Seaborn有五个预设好的主题： darkgrid , whitegrid , dark , white ,和 ticks 默认： darkgrid 12345678910111213141516fig = plt.figure(figsize = (6,6))# whitegrid主题sns.set_style('whitegrid')ax1 = fig.add_subplot(2,1,1)data = np.random.normal(size = (20,6)) + np.arange(6) / 2sns.boxplot(data = data)plt.title('style -- whitegrid')# dark主题sns.set_style('dark')ax2 = fig.add_subplot(2,1,2)sinplot() despine despine() ：设置图表坐标轴，可以根据需求将坐标轴的展现与否进行设置，更好的讲故事～ 12345678910111213141516171819202122232425262728293031# despine() -- 设置图表坐标轴# sns.despine('fig=None', 'ax=None', 'top=True', 'right=True', 'left=False', 'bottom=False', 'offset=None', 'trim=False')sns.set_style('ticks')fig = plt.figure(figsize = (6,9))plt.subplots_adjust(hspace=0.3)ax1 = fig.add_subplot(3,1,1)sinplot()# 删除上、右坐标轴sns.despine()# 创建小提琴图ax2 = fig.add_subplot(3,1,2)sns.violinplot(data = data)# sns.despine(offset = 10,trim = True)# offset: 与坐标轴之间的偏移# trim：为True时,将坐标轴限制在数据最大值最小值ax3 = fig.add_subplot(3,1,3)sns.boxplot(data = data,palette='deep')# sns.despine(left = True,right = False)# top,right,left,bottom: 布尔型,为True时不显示 1&lt;matplotlib.axes._subplots.AxesSubplot at 0xeddb830&gt; axes_style axes_style() ：设置局部图表风格，配合with使用可以很方便的将想要表达的图表更加凸显出来 1234567891011121314# with：只在sns这个图表中设置风格，其他图表风格还是与之前设置的一致# 设置局部图表风格,用with做代码块区分with sns.axes_style('darkgrid'): plt.subplot(211) sinplot()# 设置外部表格风格sns.set_style('whitegrid')plt.subplot(212)sinplot() set_context set_context() ：设置显示的比例尺度，方便在不用显示器或不同分辨率下设置不同的显示比例 seaborn内置四种显示比例：’paper’ ‘notebook’ ‘talk’ ‘poster’，左往右依次 变大 1234# 默认为notebooksns.set_context('talk')sinplot() 调色盘 seaborn的调色盘用于对图表整体颜色、比例等进行风格设置,包括颜色色板等 调色盘分为三类 Sequential：按顺序渐变的。 - Light colours for low data, dark for high data Diverging：彼此之间差异变化较大的。 - Light colours for mid-range data, low and high contrasting dark colours Qualitative：这个用于最大程度地显示不同类之间的差别。 - Colours designed to give maximum visual difference between classes 一般调用seaborn内置系统风格和简单设置参数即能实现很炫酷的数据可视化 color_palette seaborn.color_palette(palette=None, n_colors =None, desat =None) palette：None，string或sequence，可选，默认有6种主题：deep,muted, pastel, bright, dark, colorblind n_colors：颜色个数 desat：每种颜色去饱和的比例 12current_palette = sns.color_palette()sns.palplot(current_palette) 123456# sns.color_palette(pal, size=1)# pal → 颜色风格, size → 颜色色块个数# Reds/Reds_r 代表 颜色风格反转(不是所有颜色都可以反转)sns.palplot(sns.color_palette('hls',8)) 123# 分组颜色设置 - 'Paired'sns.palplot(sns.color_palette('Paired',8)) husl_palette 设置亮度和饱和度 1234567# 设置亮度、饱和度# 1、husl_palette([n_colors,h,s,l])# 2、hls_palette([n_colors,h,s,l])# l - 亮度、s - 饱和度sns.palplot(sns.hls_palette(8,l = .7,s =1)) cubehelix_palette 按照线性增长计算,设置颜色 123456789101112131415# sns.cubehelix_palette('n_colors=6', 'start=0', 'rot=0.4', 'gamma=1.0', 'hue=0.8', # 'light=0.85', 'dark=0.15', 'reverse=False', 'as_cmap=False')sns.palplot(sns.cubehelix_palette(8,gamma = 2))sns.palplot(sns.cubehelix_palette(8,start = 1.2,rot = -.5))sns.palplot(sns.cubehelix_palette(8,start = 2,rot = 0,dark = 0,light = .95,reverse = True))# 参数解析：# n_colors：颜色个数# start：值区间在0 - 3 , 开始颜色# rot：颜色旋转角度# gamma：颜色伽马值,值越大颜色越暗# dark,light：值区间在0 - 1,颜色深浅# reverse：布尔值,默认为False,由浅到深 dark_palette / light_palette 深色/浅色调色板 seaborn.dark_palette（color，n_colors =6，reverse =False，as_cmap = False，input =’rgb’） color ：高值的颜色 n_colors ：颜色个数 reverse ：默认为False as_cmap ：如果为True，则返回matplotlib colormap；为False，则返回list input：{‘rgb’，’hls’，’husl’，’xkcd’} 12345678910111213141516# dark_palette() / light_palette()# 按照green做浅色调色盘sns.palplot(sns.light_palette('green'))# 按照green做深色调色盘sns.palplot(sns.dark_palette('green',reverse = True))# 设置cmap为Greens风格sns.palplot(sns.color_palette('Greens'))# reverse → 转制颜色sns.palplot(sns.color_palette('Greens_r')) diverging_palette diverging_palette ：创建分散颜色 1234567891011121314# 创建分散颜色# sns.diverging_palette('h_neg', 'h_pos', 's=75', 'l=50', 'sep=10', 'n=6', # &quot;center='light'&quot;, 'as_cmap=False')sns.palplot(sns.diverging_palette(145,280,s = 85,l = 25,n = 7))# 参数解析:# h_neg, h_pos：起始/终止颜色值# s ：值区间0 - 100,饱和度# l ：值区间0 - 100,亮度# n ：颜色个数# center ：中心颜色为浅色还是深色'light' , 'dark' ,默认为light 应用热力图12345678# 例子plt.figure(figsize = (12,8))x = np.arange(25).reshape(5,5)cmap = sns.diverging_palette(200,20,sep = 20,as_cmap = True)sns.heatmap(x,cmap=cmap)# heatmap : 热力图 &lt;matplotlib.axes._subplots.AxesSubplot at 0x5b28c50&gt; 风格演示123456789sns.set_style('whitegrid')with sns.color_palette('PuBuGn_d'): plt.subplot(211) sinplot()sns.set_palette('husl')plt.subplot(212)sinplot() 总结本文介绍了python的绘图模块seaborn的基础认知，也就是绘图的基础风格设置与调色盘的基础知识，感谢阅读 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/11/Seaborn%E9%A3%8E%E6%A0%BC%E8%AE%BE%E7%BD%AE%E4%B8%8E%E8%B0%83%E8%89%B2%E7%9B%98/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/06/21/hello-world/"},{"title":"python数据类型 - 列表与字典","text":"摘要python数据类型之列表(List)&amp;字典(Dictionary),基础特性以及常用场景～ 写在前面上篇博文介绍了python中最基本的数据类型之一字符串，接下来介绍另外两个最常用的python数据类型：列表与字典，包括其特性与一些必须会的操作函数。在数据分析过程中，这两种数据结构也是用的比较多的，所以掌握这两种基础数据类型的操作是必不可少的～ 列表(List) 列表的表示： [] 列表的元素类型：可以是任意数据类型的python对象类型，包括数字、字符串、列表、布尔值.. 列表内的元素以逗号为分割，逗号与逗号之间即是一个整体 列表是一种有序的集合，可以随时添加和删除其中的元素 列表的索引从0开始 列表的高级特性索引 列表的索引是从0开始的 12345678# 索引取值li = [1,23,8,'giu',['时代','shkj',1737],True]print(li[3])# 输出giu 可以根据索引对列表的元素进行修改与删除 1234567891011121314151617# 根据索引进行列表内元素修改li[2] = 20print(li)#输出[1, 23, 20, 'giu', ['时代', 'shkj', 1737], True]# 根据索引列表内元素删除del li[1]print(li)# 输出[1, 20, 'giu', ['时代', 'shkj', 1737], True] 切片 切片的输出结果也是list 12345678# 切片取值li = [1,23,8,'giu',['时代','shkj',1737],True]print(li[2:5])# 输出[8, 'giu', ['时代', 'shkj', 1737]] 使用切片对列表的多个元素进行修改与删除 1234567891011121314151617# 根据切片进行列表内元素修改li[1:3] = [11,32]print(li)# 输出[1, 11, 32, 'giu', ['时代', 'shkj', 1737], True]# 根据切片列表内元素删除del li[2:5]print(li)# 输出[1, 23, True] in 操作 需要注意的是：列表内的元素以逗号为分割，即是一个整体，当元素是一个列表时，列表内的元素不能单独被in操作识别 1234567891011121314151617# in 操作li = [12,'wudu',[83,'wudi',8]]v = 11 in liprint(v)# 输出Falseli = [12,'wudu',[83,'wudi',8]]v = 'wudi' in liprint(v)# 输出False 常用方法append append : 追加，在原来的列表最后追加输入的参数(输入参数将作为一个整体追加) 123456789# append li = ['qwe','hsdj','sad','aw','12',12]v = li.append('无敌')print(li)# 输出['qwe', 'hsdj', 'sad', 'aw', '12', 12, '无敌'] extend 扩展原列表，与append 的不同之处在于：append输入的参数作为整体追加到列表末尾，而extend输入的参数内部for循环，逐个加入列表 123456789101112131415li = ['qwe','hsdj','aw']for i in [33,'sdh']: li.append(i)print(li)# 输出['qwe', 'hsdj', 'aw', 33, 'sdh']li.extend('nbeu')print(li)# 输出['qwe', 'hsdj', 'aw', 33, 'sdh', 'n', 'b', 'e', 'u'] insert 插入，在指定索引位置插入，第一个参数为索引位置，第二个参数为插入的值 1234567li = [11,22,33,23,32]li.insert(2,668)print(li)# 输出[11, 22, 668, 33, 23, 32] copy 列表拷贝(复制)，属于浅拷贝 123456789# copyli = ['qwe','hsdj','sad','aw','12',12]v = li.copy()print(v)# 输出['qwe', 'hsdj', 'sad', 'aw', '12', 12] 列表 - 删除 除了上面介绍的根据索引和切片对列表元素进行删除以外，还有以下三种方法： 12345678910111213141516171819202122232425262728293031323334# pop# 删除，默认状况下删除列表最后一个值，输入的参数为需要删除的索引位置li = [11,22,33,23,32]v = li.pop(3)print(li)# 特点是，能获取到所删除的值print(v)# 输出[11, 22, 33, 32]23# remove# 删除列表中的指定的值，左边优先li = [11,22,33,23,32]li.remove(22)print(li)# clear# 清空列表li = ['qwe','hsdj','sad','aw','12',12]li.clear()print(li) 内置方法1234567891011121314151617181920212223242526272829303132# count# 计算列表内元素出现的个数li = ['qwe',12,'hsdj','aw','aw','12',12]v = li.count(12)print(v)# index# 根据值获取当前值的索引位置，由左往右，后置位参数为开始寻找的区间li = [11,22,33,23,32,11,13,12]v = li.index(11)v1 = li.index(11,1,6)print(v,v1)# reverse# 将当前列表进行反转li = [11,22,33,23,32]li.reverse()print(li)# sort# 从小到大排序，reverse=True为从大到小排序li = [11,22,33,23,32]li.sort(reverse=True)print(li) 字典(Dictionary)必知 字典的表示：{} 由键值对组成，以逗号分割，且是无序的，具有极快的查找速度 列表、字典不能作为字典的key，字典的value可以是任何类型值 字典是一种可变容器模型，且可存储任意类型对象，如字符串、数字、元组等其他容器模型 字典中键是唯一的，如果重复最后的一个键值对会替换前面的，值不需要唯一 必会1234567# 创建字典info = { 1:'sad', 'k1': ['sad',12,['sd','dw']], (1,22):'123' } 获取键 获取字典中键值对的键 12345678for item in info.keys(): print(item)# 输出1k1(1, 22) 获取值 获取字典中键值对的值 12345678for item in info.values(): print(item)# 输出sad['sad', 12, ['sd', 'dw']]123 获取键与值 同时获取键值对的键与值 12345678for k,v in info.items(): print(k,v) # 输出1 sadk1 ['sad', 12, ['sd', 'dw']](1, 22) 123 get方法 根据传入的键获得值，后置位参数为：如果键不存在时的返回值 123456v = info.get('k',111)print(v)# 输出111 字典 - 更新 update ：更新字典内的键值对的两种方法 12345678# 字典形式传入info.update({'k1':'1111','k3':'v3'})# 赋值形式传入info.update(k1=112,k5='d',k6=668)print(dic) in操作1234567891011 dic = { 'k1':'v1', 'k2':'v2' } v = 'k1' in dic v1 = 'v1' in dic.values() print(v,v1)# 输出True True 内置方法1234567891011121314151617181920212223242526272829303132333435363738394041# .pop# 删除指定键的值，后置位参数为，如果指定键不存在的返回值v = dic.pop('k111','不存在')print(dic,v)# .popitem# 随机删除一个键值对，并获得该键值对，支持将键值对分别赋值给键和值k,v = dic.popitem()print(dic,k,v)# .setdefault# 根据传入的key设置值，若key已存在，不进行设置，并获取当前已存在的key的值# 若key不存在，插入键值对，后置位参数默认为none，并获取当前生成的键值对的key的值dic.setdefault('k3')print(dic)# .clear 删除字典dic.clear()print(dic)# .copy 复制字典v = dic.copy()print(v)# .fromkeys 根据传入的序列，创建字典，并指定统一的值v = dict.fromkeys(['sadj',123,'999'],333)print(v)# dict.setdefault(key, default=None) # 和get()类似, 但如果键不已经存在于字典中，将会添加键并将值设为default 写在后面本篇博客为学习数据分析时，学习python基本使用的笔记整理，很是浅显，不过学习是一件值得开心的事，不论所学是深或浅，千里之行始于足下嘛，只要在走，就不怕远～ 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/08/python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%20-%20%E5%88%97%E8%A1%A8%E4%B8%8E%E5%AD%97%E5%85%B8/"},{"title":"python数据类型 - 字符串","text":"摘要python数据类型之字符串(String),基础特性以及常用场景～ 前言这篇文章主要介绍python在数据分析中的字符串的常用魔法,结合实例形式总结字符串常用魔法的概念、功能、使用方法以及相关注意事项。 常用方法join 字符串拼接：将字符串中的每一个元素按照指定的分隔符进行拼接 1234567test = '你是风儿我是沙'v = ' '.join(test)# 输出你是风儿我是沙你 是 风 儿 我 是 沙 字符串拼接拓展： 连接少量字符串时，可以使用 + 号操作符 或者 f-string 连接大量字符串时，推荐使用 join 和 f-string find 子序列查找：根据传入的参数寻找子序列，找到第一个后，获取其位置 参数：sub 指定要查找的子序列 ​ start和end 指定寻找的区间 1234567test = 'hello world'v = test.find('o',3,-1)v1 = test.find('o',1,4)# 输出4-1 通过上面的实例及输出，可以发现： 在找到第一个符合传入的参数sub后，停止查找并获取sub在字符串中所在的位置(或者说下标) 通过输出v和v1，得到的位置(下标)是在整个字符串中的位置，而当返回为-1时，则为传入的指定区间的末尾，并非整个字符串的末尾 split与rsplit 字符串分割：以传入参数为分割符，默认将字符串全部分割 split：默认从左往右进行分割；rsplit：指定从右往左进行分割 参数：sep 指定分割符 ​ maxsplit 指定分割次数 12345678test = 'hello world'v = test.split('l',2)v1 = test.split('l',10)# 输出['he', '', 'o world']['he', '', 'o wor', 'd'] 通过输出对比，不难发现split需要注意的地方有： 分割时，传入参数的匹配可视为贪婪匹配，如实例中v的输出为连续的 l 分割符视为一个整体进行前后分割 结果输出并不会输出分割符 分割次数若大于字符串中含有的分割符个数，不会报错，仅输出为最大分割次数所得到的结果 关于字符串分割拓展： partition：以参数为分割符，且在拿到第一个参数时，将字符串分割成3份 rpartition：以参数为分割符，由右向左识别且在拿到第一个参数时，将字符串分割成3份 splitlines：只能根据换行符进行分割，传入的参数为True和False，结果为是否保留换行符显示 replace 替换：字符串中的指定子序列与传入参数进行替换 参数：old 指定被替换的子序列 ​ new 传入替换的参数 ​ count 替换个数(次数)，默认为全部替换 123456test = 'hello world'v = test.replace('hello','hi',2)# 输出hi world 字符串替换的另一种方法 maketrans定义对应关系，translate进行替换 1234567s = str.maketrans('aeiou','12345')v = 'osahodioashoioiwueuroiuapfhahweiuip'result = v.translate(s)# 输出4s1h4d341sh4343w525r4351pfh1hw2353p strip 移除指定的字符串，并优先最多匹配，默认(即不传入参数时)情况下，去除左右空白以及 \\t 、\\n lstrip：指定从左往右进行匹配并移除 rstrip：指定从右往左进行匹配并移除 12345678910111213141516test = 'xaxexxaleaxaae'v = test.strip('xa')v1 = test.lstrip('xa')v2 = test.rstrip('xae')# 输出vexxaleaxaae# 输出v1exxaleaxaa# 输出v2xaxexxal strip使用总结： 所谓优先最多匹配，指的是先传入参数为整体进行匹配，如果匹配不上再分解为单个字符或字符组合去进行匹配，如v2中传入的参数为xae，并指定从右往左进行匹配，匹配时优先顺序为 xae — xa、ae — x、a、e strip是指从字符串的左右两边进行匹配移除，并停止于所谓匹配优先顺序中的所有情况都匹配不上 isupper和islower isupper：判断字符串是否全部为大写 islower：判断字符串是否全部为小写 1234567test = 'ASDDss'v1 = test.isupper()v2 = test.islower()# 输出False False 输出结果为布尔值，转换为大写用：upper；转换为小写用：lower startswith和endswith startswith：判断是否以传入的参数开头 endswith：判断是否以传入的参数结尾 1234567891011test = 'djfkjahfkja'v = test.startswith('dj')v1 = test.endswith('aj')# 输出vTrue# 输出v1 False 输出结果为布尔值 其余方法判断型 isalnum：判断字符串中是否只包含字母和数字（汉字也算） isalpha：字符串中是否只包含字母（汉字也算） isdecimal、isdigit、isnumeric 字符串是否是数字，isdigit（包含一些特殊字符的数字）比isdecimal(十进制小数)牛逼点 isnumeric(包括汉字的数字也能识别)👈👈👈👈👈三者之中最牛，但使用最多是isdecimal isidentifier 标识符：查看字符串是否由字母，数字，下划线组成，数字不能为首 isprintable：判断字符串中是否存在不可显示的字符，例如：\\t 制表符 \\n 换行符 isspace：判断是否全部是空格 istitle：判断是否为标题，标题的定义为每个字符首字母为大写；转换为标题用：title center、ljust、rjust 设置字符串的宽度，并使用字符填充 center将内容居中，ljust将内容放左边，rjust将内容放右边 参数：width 指定宽度 ​ fillchar 指定填充字符 123456test = 'hello world'v1 = test.center(21,'*')# 输出*****hello world***** expandtabs 断句：匹配 \\t ，将字符串以6个为一组进行断句，并在匹配到 \\t 的位置，不够6个时以空格补满 参数：tabsize 指定宽度 123456789101112131415161718s = 'hfaushfhoahsfoih\\tuisagfiu\\tjdsba'v = s.expandtabs(6)# 输出hfaushfhoahsfoih uisagfiu jdsba# 实例应用text = 'username\\tage\\tpassw\\tsex\\n张三\\t12\\t123\\t男\\n李四\\t19\\t123\\t男\\n陈陈\\t18\\t123\\t女\\n'v = text.expandtabs(20)# 输出结果username age passw sex张三 12 123 男李四 19 123 男陈陈 18 123 女 swapcase 大小写转换：与lower、upper不同之处在于，swapcase把字符串中每一个元素进行转换，而并非统一转换成大写或小写 123456test = 'LAsjSi'v = test.swapcase()# 输出laSJsI 总结以上为数据分析中python字符串自带的一下常用方法，重温一遍并记录下来，以便查漏补缺，有哪些地方出现错误或者疑义，欢迎讨论，感谢阅读～ 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/07/python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%20-%20%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"title":"如何用图表直观显示数据的分布情况？","text":"摘要数据分布可视化图表绘制，包括直方图、密度图、柱状图、折线图的一些实例～ 写在前面今天主要聊聊关于数据分布情况的可视化所用到的一部分图表 什么是分布数据：定量化的数据，而非定性化的数据，一般指只是数值的数据 什么是分布数据可视化：查看数据分布情况以及数据分布统计时做的图表可视化 直方图 直方图主要反映数据分布情况 导入模块12345import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 设置绘图风格123456789# 设置风格、尺度sns.set_style('darkgrid')sns.set_context('paper')# 警告处理import warningswarnings.filterwarnings('ignore') 绘制直方图-1123456789101112131415161718192021# 创建数据rs = np.random.RandomState(10)s = pd.Series(rs.randn(100) * 100)# 绘制直方图及基本参数设置sns.distplot(s,bins = 10,hist = True,kde = True,norm_hist = False, rug = True,vertical = False, color = 'y',label = 'distplot',axlabel = 'x')plt.legend()# 参数解析：# bins → 箱子数量# hist、kde → 是否显示箱/密度曲线# norm_hist → 直方图是否按照密度来显示# rug → 是否显示数据分布# vertical → 是否水平显示# color → 设置颜色# label → 设置图例# axlabel → x轴标注 &lt;matplotlib.legend.Legend at 0xf049090&gt; 绘制直方图-2123456789101112sns.distplot(s,rug = True, # 设置数据频率分布 rug_kws = {'color':'g'}, # 设置密度曲线颜色、线宽、标注、线形 kde_kws = {'color':'k','lw':1,'label':'KDE','linestyle':'--'}, # 设置箱子风格、线宽、透明度、颜色 # 风格包括：'bar' 'barstacked' 'step' 'stepfilled' hist_kws = {'histtype':'step','linewidth':'1','alpha':1,'color':'g'} ) &lt;matplotlib.axes._subplots.AxesSubplot at 0xf4fc9f0&gt; 密度图 密度图与直方图用途一致，都是用于反映数据的分布情况，可视化效果炫酷一些，看个人喜好选择 单样本密度图12345678910111213141516171819202122# 密度图 - kdeplot()sns.kdeplot(s, # 是否填充 shade = True, color = 'r', vertical = False)sns.kdeplot(s,bw = 5,label = 'bw: 0.2', linestyle = '-',linewidth = 1.2,alpha = 0.5)sns.kdeplot(s,bw = 20,label = 'bw: 2', linestyle = '-',linewidth = 1.2,alpha = 0.5)# 参数解析：# bw：控制拟合的程度,类似直方图的箱数# 数据频率分布图sns.rugplot(s,height = 0.1,color = 'k',alpha = 0.6) &lt;matplotlib.axes._subplots.AxesSubplot at 0xca6b90&gt; 多样本密度图-1123456789101112131415161718192021222324252627# 密度图 - 两个样本数据密度分布图rs = np.random.RandomState(2)df = pd.DataFrame(rs.randn(100,2), columns = ['a','b'])sns.kdeplot(df['a'],df['b'], # 是否显示颜色图例 cbar = True, # 是否填充 shade = True, # 设置调色盘 cmap = 'Reds', # 最外围颜色是否显示 shade_lowest = False, # 曲线个数(如果非常多,则会越平滑) n_levels = 10, )# x,y轴设置sns.rugplot(df['a'],color = 'g',axis = 'x',alpha = 0.5)sns.rugplot(df['b'],color = 'r',axis = 'y',alpha = 0.5) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10bf0170&gt; 多样本密度图-21234567891011121314# 密度图 - 两个样本数据密度分布 # 多个密度图rs1 = np.random.RandomState(2)rs2 = np.random.RandomState(5)df1 = pd.DataFrame(rs1.randn(100,2)+2,columns=['a','b'])df2 = pd.DataFrame(rs2.randn(100,2)-2,columns=['a','b'])sns.kdeplot(df1['a'],df1['b'],cmap = 'Greens',shade = True, shade_lowest = False)sns.kdeplot(df2['a'],df2['b'],cmap = 'Blues',shade = True, shade_lowest = False) &lt;matplotlib.axes._subplots.AxesSubplot at 0x11147bf0&gt; 柱状图 barplot ：绘制柱状图 分布统计柱状图123456789101112131415161718192021222324252627# 置信区间：样本均值 + 抽样误差titanic = sns.load_dataset('titanic')plt.figure(figsize = (15,7))sns.barplot(x = 'sex',y = 'survived',hue = 'class',data = titanic, palette = 'hls', order = ['male','female'], capsize = 0.05, saturation = 8, # 误差线颜色 errcolor = 'gray', # 误差线宽度 errwidth = 2, # 置信区间误差 → 0-100内,'sd' None ci = 'sd', )# print(titanic.groupby(['sex','class']).mean()['survived'])# print('----------------------------------------------')# print(titanic.groupby(['sex','class']).std()['survived'])titanic.head() 柱状图拆分12345678# 二次拆分tips = sns.load_dataset('tips')plt.figure(figsize = (15,7))sns.barplot(x = 'day',y = 'total_bill',hue = 'sex',data = tips, palette = 'hls',edgecolor = 'w')tips.groupby(['day','sex']).mean() 实例应用12345678910111213141516# 例子crashes = sns.load_dataset('car_crashes').sort_values('total',ascending = False)print(crashes.head())fig,axes = plt.subplots(figsize = (12,17))sns.set_color_codes('pastel')sns.barplot(x = 'total',y = 'abbrev',data = crashes, label = 'Total',color = 'b',edgecolor = 'w')sns.set_color_codes('muted')sns.barplot(x = 'alcohol',y = 'abbrev',data = crashes, label = 'Alcohol-involved',color = 'b',edgecolor = 'w')axes.legend(ncol = 2,loc = 'lower-right')sns.despine(left = True,bottom = True) 12345678910111213 total speeding alcohol not_distracted no_previous ins_premium \\40 23.9 9.082 9.799 22.944 19.359 858.97 34 23.9 5.497 10.038 23.661 20.554 688.75 48 23.8 8.092 6.664 23.086 20.706 992.61 3 22.4 4.032 5.824 21.056 21.280 827.34 17 21.4 4.066 4.922 16.692 16.264 872.51 ins_losses abbrev 40 116.29 SC 34 109.72 ND 48 152.56 WV 3 142.39 AR 17 137.13 KY 计数柱状图123456789# countplot() - 计数柱状图plt.figure(figsize = (12,9))# sns.countplot(x = 'class',hue = 'who',data = titanic,palette='magma')sns.countplot(y = 'class',hue = 'who',data = titanic,palette='magma')# x / y → 以x或者y轴绘图(横向、竖向) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x15dc5cb0&gt; 折线图 pointplot：绘制折线图，置信区间的直观显示 123456789# pointplot() - 折线图 - 置信区间估计sns.pointplot(x = 'time',y = 'total_bill',hue = 'smoker',data = tips, palette = 'hls', dodge = True, join = True, markers = ['o','x'],linestyle = ['-','--'])tips.groupby(['time','smoker']).mean()['total_bill'] 123456time smokerLunch Yes 17.399130 No 17.050889Dinner Yes 21.859429 No 20.095660Name: total_bill, dtype: float64 总结以上为研究数据分布时的可视化图表中较为常用的几种图表展示，看起来不难，实际上需要注意的是对图表背后数据结构的理解，图表可视化只是一种手段，真正重心还是在能否真正理解背后的数据逻辑，感谢阅读～ 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/19/%E5%8F%AF%E8%A7%86%E5%8C%96%20-%20%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E5%9B%BE%E8%A1%A8part1/"},{"title":"一眼就get到数据分布情况的图表？","text":"摘要数据分布可视化图表绘制，包括散点图、蜂窝图、箱型图、小提琴图、LV图表的一些实例～ 前言上篇文章介绍了数据分布情况的可视化的四种图表 (直方图、密度图、柱状图、折线图)的 展示方法，下面将介绍另外几种直观显示数据分布情况的可视化图表 散点图 也称为「点图」、「散布图」或「X-Y 点图」。 所谓的散点图 (Scatterplot) 就是在笛卡尔座标上放置一系列的数据点，用来显示两个变量的数值（每个轴上显示一个变量），并检测两个变量之间的关系或相关性是否存在。 可以很直观的观察到数据的分布情况 导入模块12345678910import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline# 警告处理import warningswarnings.filterwarnings('ignore') 散点图 + 分布图12345678910111213141516171819# 1、综合散点图 - jointplot()rs = np.random.RandomState(2)df = pd.DataFrame(rs.randn(200,2),columns = ['a','b'])sns.jointplot(x = df['a'],y = df['b'], data = df, color = 'k', s = 50,edgecolor = 'w',linewidth = 1, # 类型有'reg','resid','kde','hex','scatter' kind = 'scatter', space = 0.3, size = 8, ratio = 5, # 类型为kde时不能设置marginal_kws marginal_kws = dict(bins = 15,rug = True) ) &lt;seaborn.axisgrid.JointGrid at 0x5759510&gt; 六边形图 / 蜂窝图12345678# 构建数据df = pd.DataFrame(rs.randn(500,2),columns = ['a','b'])with sns.axes_style('white'): sns.jointplot(x = df['a'],y = df['b'],kind = 'hex', color = 'k', marginal_kws = dict(bins = 20)) 密度图 + 散点图12345678910# 密度图rs = np.random.RandomState(15)df = pd.DataFrame(rs.randn(300,2),columns = ['a','b'])g = sns.jointplot(x = df['a'],y = df['b'],data = df, kind = 'kde', color = 'k', shade_lowest = False)g.plot_joint(plt.scatter,c = 'w', s = 30,linewidth = 1,marker = '+') &lt;seaborn.axisgrid.JointGrid at 0x1431ec90&gt; 综合散点图1234567891011121314151617181920# 综合散点图 - JointGrid()# 可拆分绘制的散点图 plot_joint() / ax_marg_x.hist() / ax_marg_y.hist()sns.set_style('white')tips = sns.load_dataset('tips')print('tips.head()')g = sns.JointGrid(x = 'total_bill',y = 'tip',data = tips)g.plot_joint(plt.scatter,color = 'm',edgecolor = 'white')g.ax_marg_x.hist(tips['total_bill'],color = 'b',alpha = 0.6, bins = np.arange(0,60,3))g.ax_marg_y.hist(tips['tip'],color = 'r',alpha = 0.6, orientation = 'horizontal',bins = np.arange(0,12,1))from scipy import statsg.annotate(stats.pearsonr)plt.grid(linestyle = '--') 拆分绘制 - 散点图 分别绘制散点图和直方图 1234567891011# 可拆分绘制的散点图# plot_joint() / plot_marginals()# 直方图g = sns.JointGrid(x = 'total_bill' , y = 'tip' , data = tips)g = g.plot_joint(plt.scatter,color = 'g',s = 40,edgecolor = 'white')plt.grid(linestyle = '--')g.plot_marginals(sns.distplot,kde = True,color = 'g') &lt;seaborn.axisgrid.JointGrid at 0x154c0690&gt; 拆分绘制 - 密度图 分别绘制密度图和面积图 1234567891011# 可拆分绘制的散点图# plot_joint() / plot_marginals()# 密度图g = sns.JointGrid(x = 'total_bill' , y = 'tip' , data = tips)g = g.plot_joint(sns.kdeplot,cmap = 'Reds_r')plt.grid(linestyle = '--')g.plot_marginals(sns.kdeplot,shade = True,color = 'r') &lt;seaborn.axisgrid.JointGrid at 0x154b13b0&gt; 矩阵散点图 散点图矩阵是组织成网格（矩阵）形式的散点图集合。每个散点图显示一对变量之间的关系。 123456789101112131415161718192021222324252627# 矩阵散点图 - pairplot()# 定义风格sns.set_style('white')# 引入数据iris = sns.load_dataset('iris')print(iris.head())sns.pairplot(iris, # 散点图 / 回归分布图('scatter','reg') kind = 'scatter', # 直方图 / 密度图'hist','kde' diag_kind = 'hist', # 按照某一字段进行分类 hue = 'species', # 设置调色板 palette = 'husl', # 设置不同系列的点样式(参考分类个数) markers = ['o','s','D'], size = 3) sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa &lt;seaborn.axisgrid.PairGrid at 0x15511310&gt; 局部变量对比12345# 只提取局部变量进行对比sns.pairplot(iris,vars = ['sepal_width','sepal_length'], kind = 'reg',diag_kind = 'kde', hue = 'species',palette = 'husl',size = 5) &lt;seaborn.axisgrid.PairGrid at 0x1d94b510&gt; 多类显示12345# 其他参数设置sns.pairplot(iris , diag_kind = 'kde',markers = '+', plot_kws = dict(s = 50,edgecolor = 'b',linewidth = 1), diag_kws = dict(shade = True)) &lt;seaborn.axisgrid.PairGrid at 0x14feffb0&gt; 拆分绘制 - 散点图21234567891011121314151617181920# 可拆分绘制的散点图 - PairGrid()# map_diag() / map_offdiag()g = sns.PairGrid(iris,hue = 'species',palette = 'hls', vars = ['sepal_length','sepal_width','petal_length','petal_width'], size = 3)# 创建一个绘图表格区域,设置好x,y对应的数据,按照species分类g.map_diag(plt.hist, histtype = 'barstacked',linewidth = 1,edgecolor = 'w')# 对角线图表,plt.hist / sns.kdeplotg.map_offdiag(plt.scatter,edgecolor = 'w',s = 40,linewidth = 1)# 其他图表g.add_legend()# 添加图例 &lt;seaborn.axisgrid.PairGrid at 0x21652e90&gt; 分类散点图绘制分类散点图12345678910111213141516171819# stripplot() - 按照不同类别对样本数据进行分布散点图绘制sns.set_style('whitegrid')sns.set_context('paper')tips = sns.load_dataset('tips')print(tips.head())sns.stripplot(x = 'day', # 设置分组统计字段 y = 'total_bill', # 数据分布统计字段 data = tips, jitter = True, # 当点数据重合较多时,用该参数调整,可以设置间距如：jitter = 0.1 size = 5,edgecolor = 'w',linewidth = 1,marker = 'o') total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 &lt;matplotlib.axes._subplots.AxesSubplot at 0xc12130&gt; 二次分类1234# 通过hue参数再分类sns.stripplot(x = 'sex',y = 'total_bill',hue = 'day', data = tips,jitter = True) &lt;matplotlib.axes._subplots.AxesSubplot at 0xf54fed0&gt; 二次拆分1234567891011# 二次拆分sns.stripplot(x = 'sex',y = 'total_bill',hue = 'day', data = tips,jitter = True, # 设置调色盘 palette = 'Set2', # 是否拆分 dodge = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0xf5524d0&gt; 二次筛选1234567# 筛选分类类别print(tips['day'].value_counts())sns.stripplot(x = 'total_bill',y = 'day',data = tips,jitter = True, order = ['Sat','Sun'])# order → 筛选类别 Sat 87 Sun 76 Thur 62 Fri 19 Name: day, dtype: int64 &lt;matplotlib.axes._subplots.AxesSubplot at 0xb840b0&gt; 分簇散点图12345# swarmplot() - 分簇散点图sns.swarmplot(x = 'day',y = 'total_bill',data = tips, size = 5,edgecolor = 'w',linewidth = 1,marker = 'o', palette = 'Reds') &lt;matplotlib.axes._subplots.AxesSubplot at 0xa53c70&gt; 箱型图 boxplot ：又称为盒须图、盒式图或箱线图，是一种用作显示一组数据分散情况资料的统计图。因形状如箱子而得名 初步绘制123456789101112131415161718192021222324252627# 定义风格sns.set_style('whitegrid')sns.set_context('paper')# 引入数据tips = sns.load_dataset('tips')sns.boxplot(x = 'day',y = 'total_bill',data = tips, linewidth = 2, width = 0.5, # 异常点大小 fliersize = 3, # 调色板 palette = 'hls', # 设置IQR whis = 1.5, # 是否以中值做凹槽 notch = False, order = ['Thur','Fri','Sat','Sun'])# sns.swarmplot(x = 'day',y = 'total_bill',data = tips,color = 'k',size = 3,alpha = 0.7) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x614e2b0&gt; 二次分类12345# 二次分类sns.boxplot(x = 'day',y = 'total_bill',data = tips,hue = 'sex',palette = 'Reds')# sns.swarmplot(x = 'day',y = 'total_bill',data = tips,color = 'k',size = 3,alpha = 0.7) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x62316f0&gt; 小提琴图 小提琴图其实是箱线图与核密度图的结合，通过小提琴图能更容易看出哪些位置的密度较高，即数据分布的区域 初步绘制123456789101112131415161718192021222324# 小提琴图 - violinplot()sns.violinplot(x = 'day',y = 'total_bill',data = tips, # 线宽 linewidth = 2, # 箱之间的间隔比例 width = 0.8, # 调色盘 palette = 'hls', order = ['Thur','Fri','Sat','Sun'], # 小提琴图的宽度：area-面积相同,count-样本数量决定宽度,width-宽度一样 scale = 'area', # 设置小提琴图边线的平滑度,值越大越平滑 gridsize = 50, # 设置内部显示类型：'box' 'quartile' 'point' 'stick' None inner = 'box', # bw = 0.8 ) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x10c7eab0&gt; 二次分类12345678# 二次分类sns.violinplot(x = 'day',y = 'total_bill',data = tips, hue = 'smoker',palette = 'muted', # 设置是否拆分小提琴图 split = True, inner = 'box') 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x110572d0&gt; 混合图表 小提琴图结合散点图 12345# 小提琴图 - 结合散点图sns.violinplot(x = 'day',y = 'total_bill',data = tips,palette = 'hls',inner = None)sns.swarmplot(x = 'day',y = 'total_bill',data = tips,color = 'w',alpha = 0.5)# 插入散点图 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x11224ed0&gt; LV图表12345678910111213141516# lvplot() - lv图表plt.figure(figsize = (15,7))sns.lvplot(x = 'day',y = 'total_bill',data = tips,palette = 'mako', width = 0.8, hue = 'smoker', linewidth = 12, # 设置框的大小 → 'linear','exonential','area' scale = 'area', # 设置框的数量 → 'proportion','tukey','trustworthy' k_depth = 'proportion', )sns.swarmplot(x = 'day',y = 'total_bill',data = tips,color = 'k',size = 6,alpha = 0.8) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x11263f30&gt; 写在最后这两篇博文列举了对数据分布情况的基础可视化图表，当然只是很基础的图表展示，记录下来以便随时翻阅，查漏补缺，还是那句话，任何图表可视化重点是数据结构与其逻辑，真正理解了数据才能更好的选择图表去展示，感谢阅读～ 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/22/%E5%8F%AF%E8%A7%86%E5%8C%96%20-%20%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E5%9B%BE%E8%A1%A8part2/"},{"title":"如何通过对比得出数据差异？以及什么是二八定律","text":"摘要如何得出数据的差异？如何通过二八定律定位问题的决定性因素？ 写在前面上次学习了数据特征分析中的分布分析，今天继续学习数据特征分析中另外两种分析方法，也就是对比分析以及帕累托分析法。 对比分析原理任何事物都既有共性特征，又有个性特征。只有通过对比，才能分辨出事物的性质、变化、发展与别的事物的异同，从而深刻地认识事物的本质和规律。 概念 对比分析通常是把两个互相关系的指标数据进行比较，运用数字展示和说明研究对象规模的大小，水平的高低，速度的快慢，以及各种关系是否协调。 一般有以下几种比较方式： 绝对数比较(相减) / 相对数比较(相除) 结构分析、比例分析、空间比较分析、动态对比分析 实例123456# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 123456# 构建数据data = pd.DataFrame(np.random.rand(30,2)*1000, columns = ['A_sale','B_sale'], index = pd.period_range('20180801','20180830'))data.head() A_sale B_sale 2018-08-01 373.252264 530.096409 2018-08-02 170.119238 288.210134 2018-08-03 755.505567 51.239224 2018-08-04 767.666050 744.299464 2018-08-05 602.851166 676.580069 12345678910111213141516plt.rc('font',family = 'simhei',size = 15)data.plot(kind = 'line', style = '--', alpha = 0.7, figsize = (16,6), title = 'AB产品销量对比-折线图', grid = True)# plt.rc('font',family = 'simhei',size = 15)data.plot(kind = 'bar', width = 0.8, alpha = 0.6, figsize = (16,6), title = 'AB产品销量对比-柱状图', grid = True) 123456789101112131415161718192021# 绝对数比较 - 相减x = range(len(data))y1 = data['A_sale']y2 = -data['B_sale']fig3 = plt.figure(figsize = (16,9))plt.subplots_adjust(hspace=0.3)ax1 = fig3.add_subplot(2,1,1)plt.bar(x,y1,width = 1,facecolor = 'r',edgecolor = 'k')plt.bar(x,y2,width = 1,facecolor = 'g',edgecolor = 'k')plt.grid()ax2 = fig3.add_subplot(2,1,2)y3 = data['A_sale'] - data['B_sale']plt.plot(x,y3,'--go')plt.grid()plt.xlim(0,len(data))plt.axhline(0,color = 'r',linestyle = '-')ax2.set_xticklabels(data.index[::5]) 帕累托分析基本概念 帕累托分析法，是一种得到广泛应用的统计学分析方法。 帕累托分析法应用了“帕累托法则”──关于做20%的事可以产生整个工作80%的效果的法则──通俗地说，柏拉图分析的结果得出百分之八十的后果是由占百分之二十的主要原因造成的，因此帕累托分析法又称二八定律 原因和结果、投入和产出、努力和报酬之间本来就存在着无法解释的不平衡。 一般来说，投入和努力可以分成两种不同类型：多数，他们只能造成少许的影响；少数，他们造成主要的、重大的影响；例如一个公司：80%的利润来自于20%的畅销产品，而其他80%的产品只产生了20%的利润 思路：通过二八原则，去寻找关键的那20%决定性因素 实例123456789101112131415161718192021222324252627282930313233# 创建数据，10个品类产品的销售额data = pd.Series(np.random.randn(10)*1200+3000, index=list('abcdefghij'))print(data)print('---------------------------')# 对数据进行排序data.sort_values(ascending = False,inplace = True)# 营收柱状图plt.figure(figsize = (16,6))plt.rc('font',family = 'simhei',size = 15)data.plot(kind = 'bar',color = 'g',alpha = 0.6,width = 0.7)plt.ylabel('营收_元')p = data.cumsum()/data.sum()key = p[p&gt;0.8].index[0]key_num = data.index.tolist().index(key)print('超过80%累计占比的节点值索引为：',key)print('超过80%累计占比的节点值索引位置为：',key_num)print('-------------------------')p.plot(style = '--ko',color = 'y',secondary_y = True)plt.axvline(key_num,color = 'r',linestyle = '--',alpha = 0.8)plt.text(key_num+0.2,p[key],'累计占比为：%.3f%%' % (p[key]*100),color = 'r')plt.ylabel('营收_占比')key_product = data.loc[:key]print('核心产品为：')print(key_product) 123456789101112131415161718192021222324a 3640.475899b 4259.570101c 1132.691082d 2852.087642e 2411.036428f 3438.507881g 2130.585464h 3009.455301i 1048.431800j 1804.076351dtype: float64---------------------------超过80%累计占比的节点值索引为： g超过80%累计占比的节点值索引位置为： 6-------------------------核心产品为：b 4259.570101a 3640.475899f 3438.507881h 3009.455301d 2852.087642e 2411.036428g 2130.585464dtype: float64 总结帕累托分析法也叫主次因素分析法，原先是项目管理中常用的一种方法。它是根据事物在技术和经济方面的主要特征，进行分类排队，分清重点和一般，从而有区别地确定管理方式的一种分的方法。在数据分析中，帕累托分析法可以快速获取数据的各个部分的贡献度，从而制定分析方向与计划。","link":"/2020/09/29/%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90&%E5%B8%95%E7%B4%AF%E6%89%98%E5%88%86%E6%9E%90/"},{"title":"数据分析中的度量和维度","text":"摘要通过业务指标，了解数据分析中的度量和维度到底是什么？ 写在前面 数据分析和运营脱离不开关系。业务的洞悉决定了数据分析结果的上限，数据技巧只是逼近它。so，每个数据分析都应该洞察业务指标。本文将记录下学习指标和维度的基本认知，并了解在不同的业务场景下的一些指标。 指标通俗来讲，指标是量化衡量的标准，或者说是衡量事物发展程度的单位或方法，也称为度量。例如：人口数、GDP、收入、用户数、利润率、留存率、覆盖率等。很多公司都有自己的KPI指标体系，就是通过几个关键指标来衡量公司业务运营情况的好坏。 指标可以分为绝对数指标和相对数指标，绝对数指标反映的是规模大小的指标，如人口数、GDP、收入、用户数，而相对数指标主要用来反映质量好坏的指标，如利润率、留存率、覆盖率等。在分析一个事物发展程度就可以从数量跟质量两个角度入手分析，以全面衡量事物发展程度。 而一般指标用于衡量事物发展程度，那这个程度是好还是坏，需要通过不同维度来进行对比，才能知道是好还是坏。基本上，在数据分析中指标(度量)与维度都是配合使用来对需求进行分析。 市场营销指标 客户/用户生命周期 生命周期：企业/产品和消费者在整个业务关系阶段的周期 不同业务划分的阶段不同。 传统营销中，分为：潜在用户、兴趣用户、新客户、老/熟客户、流失客户 用户价值 用户价值有不同的评估方式 指数法 例如：用户贡献 = 产出量 / 投入量 * 100% 或者金融行业的：存款 + 贷款 + 信用卡 + 年费 + … - 风险 - 流失 RFM模型 RFM模型：指用户生命周期中，衡量客户价值的立方体模型。 根据RFM模型将用户划分成多个群体，需要注意的是：MF是在同时间区间内的数据 R ：最近一次消费时间 M ：同时间区间内的总消费金额 F ：同时间区间内的消费频次 用户分群 用户分析也就是所谓的营销矩阵，类似于多维度分析和象限法 它是市场营销中的一种常见策略，在提取用户的几个核心维度后，并用象限法将其归纳和分类 产品运营指标AARRR AARRR模型是进行用户分析的经典模型，是一个典型的漏斗结构。它从生命周期的角度，描述了用户进入平台需经历的五个环节，最终获取商业价值 Acquisition：用户获取 渠道到达量：俗称曝光量。有多少人看到了产品推广相关的线索 渠道转化率：有多少用户因为曝光而心动Cost Per，包含CPM、CPC、CPS、CPD、CPT等 渠道ROI：推广营销的熟悉KPI，投资回报率，利润 / 投资 * 100% 日应用下载量：app的下载量，这里指点击下载，不代表下载完成 日新增用户数：以用户注册提交资料为基准 日新增用户数：以用户注册提交资料为基准 获客成本：为获取一个用户需要支付的成本 一次会话用户数占比：指新用户下载完app，仅打开过产品一次，且该次使用市场在两分钟以内 Activation：用户活跃 日 / 周 / 月活跃用户应用下载量：活跃标准是用户用过产品 活跃用户占比：活跃用户数在总用户数的比例，衡量的是产品的健康程度 用户会话session次数：用户打开产品操作和使用，知道推出产品的整个周期。5分钟内没操作默认会话操作结束。 用户访问时长：一次会话的持续时间 用户平均访问会话次数：一段时间内的用户平均产生会话次数 Retention：用户留存 用户在某段时间内使用产品，过了一段时间后仍旧继续使用的用户 次日留存率 七日留存率 Revenue：营收 付费用户数：花了钱的 付费用户数占比：每日付费用户占活跃用书数比，液可以计算总付费用户占总用户数比 ARPU：某时间段内每位用户平均收入（支付金额） ARPPU：某时间段内每位付费用户平均收入，排除了未付费的 客单价：每一位用户平均购买商品的金额。销售总额 / 顾客总数 LTV：用户生命周期价值，和市场营销的客户价值接近，经常用在游戏运营和电商运营中 LTV = ARPU * 1 / 流失率 Refer：传播 K因子：每一个用户能够带来几个新用户。(K因子 = 用户数 * 平均邀请人数 * 邀请转化率) 用户分享率：某功能 / 页面中，分享用户数占浏览页面人数之比 活动 / 邀请曝光量：线上传播活动中，该页面被人浏览的次数。 RARRA RARRA模型是基于AARRR模型基础上重新定义的一种增长模型，因为传统的增长模型，在现在的市场环境下，其实很容易让企业偏离初心，不去追求产品带给用户的核心价值，而是去追求一些无意义的虚荣指标，进而迅速走向衰亡。而RARRA模型恰好符合产品的核心价值一定是体现在用户留存上的这一定律。 用户留存Retention：为用户提供价值，让用户回访 提高用户留存是产品增长的基础。故第一步，评估产品当中当前留存率情况和主要用户流失节点，进而提高用户留存 计算你的N天留存率，以查看有多少用户返回你的产品并准确确定用户主要流失节点，从而进行集中优化和改善 用户激活Activation：确保新用户在首次启动时看到你的产品价值 有效的用户引导可以帮助用户花更少的时间搞清楚如何使用你的产品 简单的演练和可视化提示，帮助用户尽快体验产品价值和优势，加快用户激活 需要注意的是：好的用户引导应该保持简单，而不是覆盖每一个功能，过于繁琐 用户推荐Referral：让用户分享、讨论你的产品 通过激励手段，让已经留存下来的忠诚用户将你的产品推荐给周边的用户，达到快速扩展你的用户群的目的并为潜在用户同样提供激励措施 此外，用户推荐的每次获客成本通常比其他渠道的获客成本要低得多，推荐用户的留存率通常也会更高。 例如病毒式营销 — 通过为当前用户和推荐用户提供现金返还或折扣券/优惠券等推荐奖励机制 商业变现Revenue：一个好的商业模式是可以赚钱的 客户的留存时间越长，他们对你的业务的价值就越大，可以带来提供稳定、可预测的收入增长。 可以通过提高用户的终身价值来提高留存时间 识别追加销售和交叉销售机会； 了解用户的需求可以帮助你确定路线图的优先顺序，从而专注于实际促进用户留存和业务增长的方面 用户拉新Acquisition 鼓励老用户带来新用户 通过群组分析找出哪些获客渠道的效果最适合你的产品，进行再优化 一旦你证明你可以留住用户，你就可以加大营销力度，在渠道顶部添加新用户 ，并开始以指数方式为你的产品进行黑客增长。 用户行为用户行为分析就是通过对这些数据进行统计、分析，从中发现用户使用产品的规律，并将这些规律与网站的营销策略、产品功能、运营策略相结合，发现营销、产品和运营中可能存在的问题，解决这些问题就能优化用户体验、实现更精细和精准的运营与营销，让产品获得更好的增长。 功能使用 功能使用率 / 渗透率：使用某功能的用户占总活跃数之比 比如点赞数、评论、收藏、关注、搜索、添加好友等 用户会话 会话session：用户在一次访问过程中，从开始到结束的整个过程。 在网页端，30分钟内没有操作默认会话操作结束 用户路径 路径图：用户在一次会话的过程中，其访问产品内部的浏览轨迹。通过此，可以加工出关键路径转换率 电子商务指标购物篮分析 购物篮分析中最知名的想必是关联度，简单理解是，买了某类商品的用户更有可能买哪些其他东西。 关联分析有两个核心指标，置信度和支持度。支持度表示某商品A和某商品B同时在购物篮中的比例，置信度表示买了商品A和人有多少同时买了B，表示为A→B。 笔单价：用户每次购买支付的金额，即每笔订单的支出 件单价：商品的平均价格 成交率：支付成功的用户在总的客流量中的占比 购物篮系数：平均每笔订单中，卖出了多少商品。购物篮系数是多多益善，它也和商品关联规则有关系 复购率：一段时间内多次消费的用户占总消费用户数之比 回购率：一段时间内消费过的用户，在下一段时间内仍旧消费的占比（留存或忠诚度） 流量指标 浏览量和访客量 PV：浏览次数 UV：一定时间内访问网页的人数，正式名称独立访客数，在同一天内，不管用户访问了多少网页，他都只算一个独立访客。（在技术上，UV会通过cookie或IP衡量） 访客行为 新老访客占比：衡量网站的生命力 访客时间：衡量内容只狼不是看内容的UV，而是看内容的访问时间 访问平均访问页数：衡量网站对访客的吸引力，是访问的深度 来源：访客从哪里来，技术上，通过来源网站的参数提取，可以区分SEM，SEO或者外链等 用户行为转换率：用户在网站上进行了相应操作的用户在总访客数上的占比 首页访客占比：只看了首页的用户，在总访客数上的占比 退出率：从该页退出的页面访问数 / 进入该页的访问数 （营销中使用） 跳出率：浏览单页即退出的次数 / 访问次数 （网页产品结构中使用） 跳出率一般衡量各个落地页，营销页等；退出率则更偏向产品，任何页面都有退出率 指标生成 指标一般需要经过加和、平均等汇总计算方式得到，并且是需要在一定的前提条件进行汇总计算，如时间、地点、范围，也就是我们常说的统计口径与范围。 访客访问时长 + UV = 重度访问用户占比 （浏览时间5分钟以上的用户在整个访客中占比） 用户会话次数 + 成交率 = 有效消费会话占比 （用户在所有的会话中，其中有多少次有消费） 机器学习 维度简言之，维度是事物或现象的某种特征。如性别、地区、时间等都是维度 维度可以分为定性维度跟定量维度，也就是根据数据类型来划分，数据类型为字符型(文本型)数据，就是定性维度，如地区、性别都是定性维度;数据类型为数值型数据的，就为定量维度，如收入、年龄、消费等 一般我们对定量维度需要做数值分组处理，也就是数值型数据离散化，这样做的目的是为了使规律更加明显，因为分组越细，规律就越不明显，最后细到成最原始的流水数据，那就无规律可循。 时间维度 时间是一种常用、特殊的维度，通过时间前后的对比，就可以知道事物的发展是好是坏 纵比例如用户数环比上月增长10%、同比去年同期增长20%，这就是时间上的对比，也称为纵比 横比例如不同国家人口数、GDP的比较，不同省份收入、用户数的比较、不同公司、不同部门之间的比较，这些都是同级单位之间的比较，简称横比 总结以上是关于不同业务场景下的指标(度量)的一些学习笔记，指标和维度是数据分析中的基础，但又很重要，需要在了解了基本认知的同时，也要掌握理解两者之间的关系，届时再展开数据分析工作就会容易很多。 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/29/%E6%8C%87%E6%A0%87%E4%B8%8E%E7%BB%B4%E5%BA%A6/"},{"title":"什么是趋势分析、特征工程、因子分析 ？","text":"摘要关于数据分析中的趋势分析、特征工程、因子分析这些专有名词的基本概念～ 前言以下为数据分析过程中常见的一些专用名词解析，记录下来以便随时翻阅，并进行查漏补缺～ 趋势分析集中趋势 集中趋势分析是指一组数据项某一中心值靠拢的程度，它反映了一组数据中心点的位置所在。 主要靠均值、中数、众数等统计指标来表示数据的集中趋势 均值(连续值)：也称平均数，它是全部数据的算术平均。均值在统计学中具有重要的地位，是集中趋势的最主要测度值。 中位数(异常值)：是一组数据排序后处于中间位置上的变量值 众数：是一组数据中出现次数最多的变量值。众数主要用于测度分类数据的集中趋势，当然也适用于作为顺序数据以及数值型数据集中趋势的测度值。一般情况下，只有在数据量较大的情况下，众数才有意义。 离中趋势 离中趋势是指一组数据中各数据值以不同程度的距离偏离其中心（平均数）的趋势，又称标志变动度。 主要靠极差、四分差、平均差、方差、标准差等统计指标来研究数据的离中趋势。 极差(全距)：极差=最大变量值-最小变量值 分位差：是从一组数据中剔除了一部分极端值之后重新计算的类似于极差的指标。常用的有四分位差等 四分位差=（第三个四分位数-第一个四分位数）/ 2 平均差：是数据组中各数据值与其算术平均数离差绝对值的算术平均数。平均差异大，表明各标志值与算术平均数的差异程度越大，该算术平均数的代表性就越小；平均差越小，表明各标志值与算术平均数的差异程度越小，该算术平均数的代表性就越大。 方差：数据组中各数据值与其算术平均数离差平方的算术平均数。 标准差：方差的平方根就是标准差。 正态分布的离中趋势：数据落在左右一倍标准差内的概率为69%，落在正负1.96倍的概率为95%，落在正负2.58倍的概率为99% 数据分布 偏态系数：数据平均值偏离状态的以一种衡量，值为正为正偏，为负为负偏 峰态系数：数据分布集中强度的衡量，值越大，顶越尖（正太分布的峰态系数一般是3） 正态分布 正态分布又名高斯分布，若随机变量X服从一个数学期望为μ、方差为σ^2 的高斯分布，记为N(μ，σ^2)。其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。 我们通常所说的标准正态分布是μ = 0,σ = 1的正态分布 正态分布的密度函数的特点是：关于μ对称，并在μ处取最大值，在正（负）无穷远处取值为0，在μ±σ处有拐点，形状呈现中间高两边低，图像是一条位于x轴上方的钟形曲线。 卡方分布若n个相互独立的随机变量，均服从标准正态分布N（也称独立同分布于标准正态分布），则这n个服从标准正态分布的随机变量的平方和构成一新的随机变量，其分布规律称为分布。 自由度：通俗讲，样本中独立或能自由变化的自变量的个数，称为自由度 卡方分布特点：卡方值都是正值，呈正偏态（右偏态），随着参数 n 的增大；卡方分布趋近于正态分布；随着自由度n的增大，卡方分布向正无穷方向延伸（因为均值n越来越大），分布曲线也越来越低阔（因为方差2n越来越大）。 F分布设X、Y为两个独立的随机变量，X服从自由度为n的卡方分布，Y服从自由度为m的卡方分布，这两个独立的卡方分布除以各自的自由度以后的比率服从F分布，即两个服从卡方分布的随机变量的比构成 F分布的特点：是一种非对称分布；它有两个自由度，即n-1（分子自由度）和m-1（分母自由度），且不同的自由度决定了F分布的形状。 T分布假设X服从标准正态分布N（0,1），Y服从卡方 （n）分布，那么Z=X/sqrt(Y/n)的分布称为自由度为n的t分布，即正太分布的一个随机变量除于一个服从卡方分布的变量就是T分布 T分布的特点：以0为中心，左右对称的单峰分布；t分布是一簇曲线，其形态变化与n（确切地说与自由度ν）大小有关。自由度ν越小，t分布曲线越低平；自由度ν越大，t分布曲线越接近标准正态分布（u分布）曲线。 数据分类 定类（类别）：根据事物离散、无差别属性进行的分类（例如：性别、名族） 定序（顺序）：可以界定数据的大小，但不能测定差值（例如：收入的低中高） 定距（间隔）：可以界定数据大小的同时，可测定差值，但无绝对零点（乘除无意义，例如：摄氏温度） 定比（比率）：可以界定数据大小，可测定差值，有绝对零点 单因子分析 异常值分析：连续异常值、离散异常值、知识异常值 对比分析：绝对数比较、相对数比较（结构、比例、比较、动态、强度）； ​ 时间维度、空间维度、经验于计划 结构分析：静态结构、动态结构 分布分析：直接获得概率分布、是否是正态分布、极大似然 多因子分析 假设检验 建设原假设H0（包括等号），H0的反命题为H1，也叫备择假设 选择检验统计量 根据显著水平（一般为0.05），确定拒绝域 计算p值或者样本统计值，做出判断（一般取双边检验p值） 正态检验 – scipy.stats.normaltest（偏度和峰度检验方法） 卡方检验（常用于两个因素之间有没有比较强的联系） – scipy.stats.chi2_contingency T分布检验（常用于检验两组样本分布是否一致，例如临床医疗检验药物效果）– scipy.stats.ttest_ind F检验（常用在方差分析） – scipy.stats.f_oneway 相关系数：正相关、负相关、不相关（相关系数越大，越接近1，二者变化趋势越正向同步；相关系数越小，越接近-1，反向同步；相关系数趋近于0可以认为二者是没有关系的） pearson spearman（只和名次差有关，跟具体的数值关系不大） 线性回归：（最小二乘法）因变量和自变量的关系是线性的 决定系数越接近1，回归效果越好；越接近0，回归效果越差 残差不相关（DW检验）DW值范围0~4，值为2为残差不相关，接近于4代表残差正相关，接近于0代表残差负相关；好的回归残差应该是不相关的 主成分分析（PCA） – 降维 求 特征协方差矩阵 求协方差的特征值和特征向量 将特征值按照从大到小的顺序排序，选择其中最大的K个 将样本点投影到选区的特征向量上 奇异值分解（SVD） LDA降维 – 线性判别式分析 核心思想：投影变换后同一标注内距离尽可能小；不同标注间距离尽可能大 特征工程 简而言之，特征工程就是一个把原始数据转变成特征的过程，这些特征可以很好的描述这些数据，并且利用它们建立的模型在未知数据上的表现性能可以达到最优（或者接近最佳性能）。从数学的角度来看，特征工程就是人工地去设计输入变量X。 目的 特征工程的目的就是获取更好的训练数据 特征越好，灵活性越强 特征越好，构建的模型越简单 特征越好，模型的性能越出色 步骤 特征使用：数据选择 – 可用性 特征获取：特征来源 – 特征的规整与存储 特征处理：数据清洗 – 特征预处理 特征监控：现有特征 – 新特征 关于数据清洗 数据清洗 数据样本抽样 样本要具备代表性 样本比例要平衡一级样本不平衡是如何处理 尽量考虑使用全量数据 异常值（空值）处理 识别异常值和重复值 – isnull() / duplicated() 直接丢弃（包括重复数据） – drop() / dropna() / drop_duplicated() 当是否有异常当作一个新的属性，替代原值 – fillna() 集中值指代 – fillna() 边界值指代 – fillna() 插值 – interpolate() – Series 特征预处理 特征预处理 标注（标记、标签、label） 特征选择 – 剔除与标注不相关或者冗余的特征 过滤思想 数据类型 可用方法 连续 — 连续 相关系数、假设检验 连续 — 离散（二值） 相关系数、连续二值化（最小Gini切分，最大熵增益切分） 连续 — 离散（非二值） 相关系数（定序） 离散（二值）— 离散（二值） 相关系数、熵相关、F分值 离散 — 离散（非二值） 熵相关，Gini，相关系数（定序） 包裹思想 – 遍历特征子集 RFE算法：1、列出特征集合；2、构造简单模型，根据系数去掉弱特征；3、余下特征重复过程，直到评价指标下降较大或者低于阈值，停止 嵌入思想 – 根据一个简单模型来分析特征的重要性（正则化/正规化） 特征变换 对指化 – 先进行对指化，再进行归一化 – 函数Softmax 指数化 – 将数值进行指数化 对数化 – 数据缩放到较小的尺度内（例如：收入、声音分贝、地震震级） 离散化 – 将连续变量分成几段（tips：先排序） 等距 等宽 归一化 (x - xmin) / (xmax - xmin) 标准化 数值化 定类数据 – 标签化 （LabelEncode） 定序数据 – 独热 （One-HotEncode ） 正规化 （1、用在每个对象的各个特征的表示，如特征矩阵的行；2、模型的参数，如回归模型） L1 正规化 ：xi / x的绝对值的和 L2 正规化（欧式距离） ：xi / x的平方和的开方 特征降维 PCA 奇异值 LDA 特征衍生 加减乘除 求导与高阶求导 人工归纳 写在末尾本文仅为数据分析学习过程中遇到的一些名词的解析，记录下来，供随时翻阅，达到查漏补缺的作用，未完待续～","link":"/2020/08/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%AD%E9%83%A8%E5%88%86%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/"},{"title":"快速了解数据分布的常用方法","text":"摘要介绍几种数据分析中对于数据分布的常用方式，包括极差、频率以及分组的使用实例～ 前言分布分析是数据特征分析中极为常用的一种方法。在数据质量得到保证的前提下，通过绘制图表、计算某些统计量等手段对数据的分布特征和贡献度进行分析，分布分析能够揭示数据的分布特征和分布类型，对于定量数据，可以做出频率分布表、绘制频率分布直方图显示分布特征；对于定性数据，可用饼图和条形图显示分布情况。 分布分析 分布分析 → 研究数据的分布特征和分布类型，分定量数据、定性数据区分基本统计量 一般通过以下三种方式来进行分布分析： 极差 / 频率分布情况 / 分组组距及组数 下面通过一个案例对分布分析进行实操应用： 123456# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 1234567891011121314# 读取数据data = pd.read_csv('E:/深圳罗湖二手房信息.csv',engine = 'python')# 绘制散点图plt.figure(figsize = (12,8))plt.scatter(data['经度'],data['纬度'], s = data['房屋单价']/400, c = data['参考总价'], cmap = 'Reds', alpha = 0.6)plt.grid()data.head() 房屋编码 小区 朝向 房屋单价 参考首付 参考总价 经度 纬度 0 605093949 大望新平村 南北 5434 15.0 50.0 114.180964 22.603698 1 605768856 通宝楼 南北 3472 7.5 25.0 114.179298 22.566910 2 606815561 罗湖区罗芳村 南北 5842 15.6 52.0 114.158869 22.547223 3 605147285 兴华苑 南北 3829 10.8 36.0 114.158040 22.554343 4 606030866 京基东方都会 西南 47222 51.0 170.0 114.149243 22.554370 图表解读：点越大代表房屋的单价越高，颜色越深代表总价越高 极差 本案例中，可通过极差中看到销售的稳定程度 1234567891011121314# 极差def d_range(df,*cols): lst = [] for col in cols: crange = df[col].max() - df[col].min() lst.append(crange) return(lst)key1 = '参考总价'key2 = '参考首付'dr = d_range(data,key1,key2)print('%s极差为：%.2f \\n%s极差为：%.2f' % (key1,dr[0],key2,dr[1])) 12参考总价极差为：175.00 参考首付极差为：52.50 分组1234567891011# 频率分布情况data[key1].hist(bins = 8,edgecolor = 'k')# 分组区间gcut = pd.cut(data[key1],10,right=False)gcut_count = gcut.value_counts(sort = False)data['%s分组区间' % key1] = gcut.valuesdata.head() 图表解读：可以看出主要集中在160万以上，60万以下。 频率分布的划分方式：直方图可以快速的看到它的排列情况，把它拆分：分组划分 房屋编码 小区 朝向 房屋单价 参考首付 参考总价 经度 纬度 参考总价分组区间 0 605093949 大望新平村 南北 5434 15.0 50.0 114.180964 22.603698 [42.5, 60.0) 1 605768856 通宝楼 南北 3472 7.5 25.0 114.179298 22.566910 [25.0, 42.5) 2 606815561 罗湖区罗芳村 南北 5842 15.6 52.0 114.158869 22.547223 [42.5, 60.0) 3 605147285 兴华苑 南北 3829 10.8 36.0 114.158040 22.554343 [25.0, 42.5) 4 606030866 京基东方都会 西南 47222 51.0 170.0 114.149243 22.554370 [165.0, 182.5) 频率统计1234567891011121314151617181920212223242526# 区间出现频率r_zj = pd.DataFrame(gcut_count)r_zj.rename(columns = {gcut_count.name:'参考总价频数'},inplace = True)# 计算频率r_zj['频率'] = r_zj['参考总价频数'] / r_zj['参考总价频数'].sum()# 计算累计频率r_zj['累计频率'] = r_zj['频率'].cumsum()# 以百分比显示频率r_zj['百分比'] = r_zj['频率'].apply(lambda x:'%.2f%%' % (x*100))# 以百分比显示累计频率r_zj['累计频率百分比'] = r_zj['累计频率'].apply(lambda x:'%.2f%%' % (x*100))r_zj.style.bar(subset = ['频率','累计频率']) # 表格显示条形图 绘制直方图12345678910111213141516# 直方图r_zj['频率'].plot(kind = 'bar', figsize = (16,6), grid = True, color = 'k', alpha = 0.6)# 添加标签x = len(r_zj)y = r_zj['频率']m = r_zj['参考总价频数']for i,j,k in zip(range(x),y,m): plt.text(i-0.1,j+0.01,'%i' % k, color = 'k') 字段定性123456789101112# 频率分布：定性字段cx_g = data['朝向'].value_counts(sort = True)r_cx = pd.DataFrame(cx_g)r_cx.rename(columns = {cx_g.name:'朝向频数'},inplace = True)r_cx['频率'] = r_cx['朝向频数'] / r_cx['朝向频数'].sum()r_cx['累计频率'] = r_cx['频率'].cumsum()r_cx['百分比'] = r_cx['频率'].apply(lambda x:'%.2f%%' % (x*100))r_cx['累计频率百分比'] = r_cx['累计频率'].apply(lambda x:'%.2f%%' % (x*100))r_cx.style.bar(subset = ['频率','累计频率'],color = '#d35f5f',width = 100) 可视化1234567891011121314151617181920# 可视化：绘制频率直方图、饼图plt.figure(num = 1,figsize = (16,6))r_cx['频率'].plot(kind = 'bar', width = 0.8, rot = 0, color = 'k', grid = True, alpha = 0.6)plt.title('朝向分布频率直方图')plt.figure(num = 2)plt.pie(r_cx['朝向频数'], labels = r_cx.index, autopct= '%.2f%%', shadow = True)plt.axis('equal')plt.rc('font',family = 'simhei',size = 15) # 标签显示 总结以上是数据特征分析中最基础也是最常用的分布分析方法，用于研究数据的分布特征和分布类型，有错误之处，请指正，感谢阅读～","link":"/2020/09/22/%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81%E4%B9%8B%E5%88%86%E5%B8%83%E5%88%86%E6%9E%90/"},{"title":"什么是正态分布？如何进行正态性检验？","text":"摘要什么是正态分布，以及检验数据样本的正态性的方式有哪些？ 前言本文记录学习正态分布以及数据特征的正态性检验。 正态分布正态分布，又名高斯分布，是一个非常常见的连续概率分布。 定义正态分布是具有两个参数μ和σ2的连续型随机变量的分布。即若一个随机变量X服从一个数学期望为μ、方差为σ^2^的正态分布，记为N( μ , σ^2^ )。 参数μ是服从正态分布的随机变量的均值，参数σ^2^是此随机变量的方差。 其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度，当μ = 0，σ = 1时的正态分布称之为标准正态分布。 特点 集中性：正态曲线的高峰位于正中央，即均数所在的位置 对称性：正态曲线以均数为中心，左右对称，曲线两端永不于横轴相交 均匀变动性：正态曲线由均数所在处开始，分别向左右两侧逐渐均匀下降 服从正态分布的随机变量的概率规律为取 μ邻近的值的概率大 ，而取离μ越远的值的概率越小； μ决定分布的中心位置 σ越大，曲线越矮胖，总体分布越分散，反之曲线越瘦高，总体分布越集中 正态性检验定义 利用观测数据判断总体是否服从正态分布的检验称为正态性检验，是统计判决中重要的一种特殊的拟合优度假设检验 最为基础和常用的正态性检验方法有： 直方图初判 QQ图判断 K-S检验 直方图检验123456# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 1234567891011121314# 直方图初判s = pd.DataFrame(np.random.randn(1000)+10,columns=['value'])print(s.head())fig = plt.figure(figsize = (16,9))ax1 = fig.add_subplot(2,1,1)ax1.scatter(s.index,s.values)plt.grid()ax2 = fig.add_subplot(2,1,2)s.hist(bins = 20,alpha = 0.7,ax = ax2,edgecolor = 'k')s.plot(kind = 'kde',secondary_y = True,ax = ax2)plt.grid() 123456 value0 10.0319981 8.9389712 9.5218493 10.7521104 10.601193 QQ图判断123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# QQ图判断 - 通过把测试样本数据的分位数与已知分布相比较，从而来检验数据的分布情况# QQ图是一种散点图，对应于正态分布的QQ图，就是由标准正态分布的分位数为横坐标，样本值为纵坐标的散点图# 参考直线：四分之一分位点和四分之三分位点这两点确定，看散点是否落在这条线的附近# 绘制思路# 1、在做好数据清洗后，对数据进行排序# 2、排序后，计算出每个数据对应的百分位p{i}，即第i个数据x(i)为p(i)分位数，其中p(i)=(i-0.5)/n# 3、绘制直方图 + QQ图 ， 直方图做参考# 绘制散点图，横坐标是它的分位，就是分布的位置，做下排序，看是否很多的点在某条直线上，这条直线一般是拿它的一分位和三分位做一下相减，s = pd.DataFrame(np.random.randn(1000)+10,columns = ['value'])print(s.head())mean = s['value'].mean()std = s['value'].std()print('均值为：%.2f,标准差为：%.2f' % (mean,std))print('------------')# 计算均值,标准差s.sort_values(by = 'value',inplace = True)# 重新排序s_r = s.reset_index(drop = False)# 计算百分位数p(i)# 计算q值s_r['p'] = (s_r.index - 0.5) / len(s_r)# 每个值标准化后的结果s_r['q'] = (s_r['value'] - mean) / stdst = s['value'].describe()# 1/4位点x1,y1 = 0.25,st['25%']# 3/4位点x2,y2 = 0.75,st['75%']# 绘制数据分布图 -- 散点图fig = plt.figure(figsize = (16,12))ax1 = fig.add_subplot(3,1,1)ax1.scatter(s.index,s.values)plt.grid()# 绘制直方图ax2 = fig.add_subplot(3,1,2)s.hist(bins = 30,alpha = 0.7,ax = ax2)s.plot(kind = 'kde',secondary_y = True,ax = ax2)plt.grid()# 绘制QQ图，直线为四分之一位数、四分之三位数的连线，基本符合正态分布ax3 = fig.add_subplot(3,1,3)ax3.plot(s_r['p'],s_r['value'],'k.',alpha = 0.1)ax3.plot([x1,x2],[y1,y2],'-r')plt.grid() 12345678 value0 10.3152181 11.1689542 11.4689533 10.0997624 8.701746均值为：10.00,标准差为：1.01------------ K-S检验Kolmogorov-Smirnov是比较一个频率分布f(x)与一个理论分布g(x)或者两个观测值分布的检验方法。 以样本数据的累计频数分布与特定的理论分布比较（比如正态分布），如果两者差距小，则推论样本分布取自某特定分布 假设检验问题： H0：样本的总体分布 服从某特定分布 H1：样本的总体分布 不服从某特定分布 Fn(x)：样本的累计分布函数 F0(x)：理论分布的分布函数 D：F0(x) 与 Fn(x) 差值的绝对值最大值 即 D= max IFn(x) - F0(x)I D &gt; D( n , α )相比较：p &gt; 0.05则接受H0，p &lt; 0.05则接受H1. 1234567891011121314151617181920212223242526# K - S 检验 → Kolmogorov-Smirnov是比较频率分布f(x)与理论分布g(x)或者两个观测值分布的检验方法data = [87,77,92,68,80,78,85,77,81,80,80,77,92,86, 76,80,81,75,77,72,81,72,83,86,80,68,77,87, 76,77,78,92,75,80,78]df = pd.DataFrame(data,columns = ['value'])jz = df['value'].mean()bzc = df['value'].std()print('样本均值为：%.2f,样本标准差为：%.2f' % (jz,bzc))print('----------------')s = df['value'].value_counts().sort_index()df_s = pd.DataFrame({'血糖浓度':s.index, '次数':s.values})df_s['累计次数'] = df_s['次数'].cumsum()df_s['累计频率'] = df_s['累计次数'] / len(data)df_s['标准化取值'] = (df_s['血糖浓度'] - jz) / bzcdf_s['理论分布'] = [0.0244,0.0968,0.2148,0.2676,0.3228,0.3859,0.5160,0.5793,0.7054,0.8106, 0.8531,0.8888,0.9803]df_s['D'] = np.abs(df_s['累计频率'] - df_s['理论分布'])dmax = df_s['D'].max()print('实际观测D值为：%.4f' % dmax)df_s 1234样本均值为：79.74,样本标准差为：5.94----------------实际观测D值为：0.1636 把一个非标准正态分布变成一个标准正态分布—–&gt;把非标准正态分布的值变成X = (x-u) /方差—–&gt;可以找到理论值。在将这个标准化取值去跟正态分布表去找对应的值。 因为标准化取值的值它本 身就符合正态分布；系统分布与标准分布相减，如果这个函数满足标准正态分布，它的值就应该满足这个表。比如说标准化取值2.064315，其对应的查正态分布表值为0.9803，它的理论分布值是0.9803； 标准化取值-1.9777，去掉负号，查正态分布表为0.9756，正的是0.9756，负的就是1-0.9756=0.0244.可以看到与理论分布值是相对应的。 理论分布就相当于是g（x）就是F0（x），F（n）就是原来的F（n）累计频率。累计频率 - 理论分布 = D 123456#绘制折线图df_s['累计频率'].plot(style = '--k.')df_s['理论分布'].plot(style = '--r.')plt.legend(loc = 'upper left')plt.grid() 结论：实际观测D值为：0.1597 对应的0.1597放到显著性对照表，我们的样本数据一共35个，在50以内，按0.05的值去算的话，0.1587介于0.158和0.190之间，它所对应的P值是0.2和0.4，这个P值是大于0.05的。 拿到这个D值去那个表里边查，如果大于0.05就说明满足正态分布。 K-S算法1234567891011121314151617#直接用算法做KS检验from scipy import statsdata = [87,77,92,68,80,78,85,77,81,80,80,77,92,86, 76,80,81,75,77,72,81,72,83,86,80,68,77,87, 76,77,78,92,75,80,78]df = pd.DataFrame(data,columns = ['value'])jz = df['value'].mean()bzc = df['value'].std()stats.kstest(df['value'],'norm',(jz,bzc))# .kstest 方法：ks检验，参数分别是：待检验数据，检验方法，均值与标准差# 结果返回两个值：statistic → D值；pvalue → P值# P值大于0.05，满足正态分布 1KstestResult(statistic=0.1590868892818147, pvalue=0.3061435516448461) 总结以上是数据特征分析中的正态分布与正态性检验的全部内容，如有错误，恳请指正，感谢阅读～","link":"/2020/10/19/%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81%E4%B9%8B%E6%AD%A3%E6%80%81%E6%80%A7%E6%A3%80%E9%AA%8C/"},{"title":"每日一题 - 寻找第N高的数据","text":"摘要MySQL练习题，如何查找第N高的数据 MySQL练习题 [题目] exercise2 表中，记录了学生选修课程的名称以及成绩 找出语文课中成绩第二高的学生成绩，如果不存在第二高成绩的学生，那么查询应返回 null 。 12345678910111213141516171819202122232425262728293031323334353637# 建表create table exercise2( id int not null comment '学号', course varchar(64) not null comment '课程', score int comment '成绩');# 查看desc exercise2;+--------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+--------+-------------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || course | varchar(64) | NO | | NULL | || score | int(11) | YES | | NULL | |+--------+-------------+------+-----+---------+-------+# 插入数据insert into exercise2(id,course,score)values(1,&quot;语文&quot;,90),(1,&quot;数学&quot;,65),(2,&quot;语文&quot;,68),(2,&quot;数学&quot;,96),(3,&quot;数学&quot;,55);# 查看数据select * from exercise2;+----+--------+-------+| id | course | score |+----+--------+-------+| 1 | 语文 | 90 || 1 | 数学 | 65 || 2 | 语文 | 68 || 2 | 数学 | 96 || 3 | 数学 | 55 |+----+--------+-------+ [解题思路] 读题：找出语文课中成绩第二高的学生成绩，如果不存在第二高成绩的学生，那么查询应返回 null 。 解题 找出所有选修了”语文”课的学生成绩 进行排序并选择第二高成绩 特殊情况：如果不存在第二高的成绩，返回null 123456789101112# select ifnull(( select distinct score from exercise2 where course = &quot;语文&quot; order by score desc limit 1,1),null) as &quot;语文第二高成绩&quot; ;+-----------------------+| 语文第二高成绩 |+-----------------------+| 68 |+-----------------------+ 123456789101112131415161718192021222324252627# 验证特殊情况insert into exercise2(id,course,score) values (1,&quot;英语&quot;,77),(2,&quot;英语&quot;,77);+----+--------+-------+| id | course | score |+----+--------+-------+| 1 | 语文 | 90 || 1 | 数学 | 65 || 2 | 语文 | 68 || 2 | 数学 | 96 || 3 | 数学 | 55 || 1 | 英语 | 77 || 2 | 英语 | 77 |+----+--------+-------+select ifnull(( select distinct score from exercise2 where course = &quot;英语&quot; order by score desc limit 1,1),null) as &quot;英语第二高成绩&quot; ;+-----------------------+| 英语第二高成绩 |+-----------------------+| NULL |+-----------------------+ 特别声明：以上内容均来自网络，侵删，望留言告知，谢谢～ 参考文章:","link":"/2021/05/29/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E5%AF%BB%E6%89%BE%E7%AC%ACN%E9%AB%98%E7%9A%84%E6%95%B0%E6%8D%AE/"},{"title":"每日一题 - 查找重复值","text":"摘要MySQL练习题，有关表中重复值查找 MySQL练习题 [题目] 编写一个SQL查询，查找学生表中所有重复的学生名。 1234567891011121314151617181920212223242526272829303132# 建表create table exerciseOne( id int not null auto_increment, name varchar(64) not null, primary key(id));desc exerciseOne;+-------+-------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(64) | NO | | NULL | |+-------+-------------+------+-----+---------+----------------+# 插入数据insert into exerciseOne(id,name) values (001,'zhangsan'),(002,'ali'),(003,'luox'),(004,'luox'),(005,'ali');# 数据查看select * from exerciseOne;+----+----------+| id | name |+----+----------+| 1 | zhangsan || 2 | ali || 3 | luox || 4 | luox || 5 | ali |+----+----------+ [解题思路] 读题：查找学生表中所有重复的学生名 解题：以学生名分组，筛选出各个分组计数大于1的学生名 12345678select name from exerciseOne group by name having count(name) &gt; 1;+------+| name |+------+| ali || luox |+------+ 特别声明：以上内容均来自网络，侵删，望留言告知，谢谢～ 参考文章:","link":"/2021/05/28/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98-%E6%9F%A5%E6%89%BE%E9%87%8D%E5%A4%8D%E5%80%BC/"},{"title":"数据的相关性分析 &amp; 统计分析","text":"摘要怎样判别数据中的变量是否存在相关性？得到数据样本又如何进行整理归档？ 前言前面两篇文章列举了数据特征分析中的三种常见的分析方法：分布分析、对比分析和帕累托分析。接下来介绍另外两种分析方法。 相关性分析在数据特征分析中，研究两个或两个以上随机变量之间相互依存关系的方向和密切程度的方法，就称为相关性分析。而相关性的元素之间需要存在一定的联系或者概率才可以进行相关性分析，所以需要先确定两者之间是否存在相关性。 一般有以下几种方法： 图示处判 两个变量可以通过线性相关进行分析： 1、k&gt;0：正相关，随着另一个变量的增大而增大； 2、k&lt;0：负相关，随着另一个变量的增大而减小 多个变量可以通过散点图矩阵初判多变量之间的关系 123456# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 12345678910111213141516171819# 通过绘制散点图初步判断两个变量之间的线性相关性data1 = pd.Series(np.random.rand(50)*100).sort_values()data2 = pd.Series(np.random.rand(50)*50).sort_values()data3 = pd.Series(np.random.rand(50)*500).sort_values(ascending = False)# 正线性相关fig = plt.figure(figsize = (16,9))ax1 = fig.add_subplot(1,2,1)ax1.scatter(data1,data2)plt.grid()# 负线性相关ax2 = fig.add_subplot(1,2,2)ax2.scatter(data1,data3)plt.grid() 12345678910# 通过绘制散点图初步判断多变量间关系data = pd.DataFrame(np.random.randn(200,4)*100,columns = list('abcd'))pd.plotting.scatter_matrix(data,figsize = (16,9), c = 'k', marker = '*', diagonal= 'hist', alpha=0.7, range_padding=0.1)data.head() 皮尔逊相关系数皮尔逊相关系数，又称皮尔逊积矩相关系数是用于度量两个变量X和Y之间的相关（线性相关） 其值介于-1与1之间,o代表无相关性，负值为负相关，正值为正相关 0 &lt; IrI &lt; 1 ：表示存在不同程度线性相关 IrI &lt;= 0.3 ： 不存在线性相关 0.3 &lt; IrI &lt;= 0.5 ： 低度线性相关 0.5 &lt; IrI &lt;= 0.8 ： 显著线性相关 IrI &gt; 0.8 ： 高度线性相关 tips：前提是数据必须满足正态分布 1234567891011121314151617181920212223242526# 皮尔逊相关系数推导from scipy import statsdata1 = pd.Series(np.random.rand(100)*100).sort_values()data2 = pd.Series(np.random.rand(100)*50).sort_values()data = pd.DataFrame({'A':data1.values,'B':data2.values})print(data.head())print('------------------------')jz1,jz2 = data['A'].mean(),data['B'].mean()bzc1,bzc2 = data['A'].std(),data['B'].std()print('A正态性检验：\\n',stats.kstest(data['A'],'norm',(jz1,bzc1)))print('B正态性检验：\\n',stats.kstest(data['B'],'norm',(jz2,bzc2)))print('------------------------')# 正态性检验 - pvalue &gt; 0.05data['(x-jz1)*(y-jz2)'] = (data['A'] - jz1) *(data['B'] - jz2)data['(x-jz1)**2'] = (data['A'] - jz1) ** 2data['(y-jz2)**2'] = (data['B'] - jz2) ** 2print(data.head())print('------------------------')r = data['(x-jz1)*(y-jz2)'].sum() / (np.sqrt(data['(x-jz1)**2'].sum() * data['(y-jz2)**2'].sum()))print('Pearson相关系数为：%.4f' % r) 1234567891011121314151617181920 A B0 1.857207 0.4870411 6.543581 0.4988902 7.895966 0.6502853 7.992037 0.8375124 9.657230 1.412577------------------------A正态性检验： KstestResult(statistic=0.06887931747546477, pvalue=0.7404141658459595)B正态性检验： KstestResult(statistic=0.11163895057364082, pvalue=0.1532773076744851)------------------------ A B (x-jz1)*(y-jz2) (x-jz1)**2 (y-jz2)**20 1.857207 0.487041 1219.195525 2588.810639 574.1778511 6.543581 0.498890 1106.353230 2133.883984 573.6101302 7.895966 0.650285 1067.174612 2010.768847 566.3811903 7.992037 0.837512 1056.510686 2002.162152 557.5047104 9.657230 1.412577 992.418885 1855.915085 530.679045------------------------Pearson相关系数为：0.9890 Pearson算法123456789101112# Pearson相关系数 - 算法data1 = pd.Series(np.random.rand(100)*100).sort_values()data2 = pd.Series(np.random.rand(100)*50).sort_values()data = pd.DataFrame({'A':data1.values,'B':data2.values})print(data.head())print('------------------------')data.corr()# pandas相关性方法：data.corr(method='pearson', min_periods=1) → 直接给出字段的相关系数矩阵# method默认为'pearson' 1234567 A B0 2.935213 0.0976181 3.352383 0.7747132 5.230225 0.8463523 5.456244 1.0101884 6.479587 1.077713------------------------ 斯皮尔曼相关系数当数据源不服从正态分布的变量、分类的关联性时，可采用斯皮尔曼相关系数，也称为等级相关系数。计算逻辑：对两个变量成对的取值按照从小到大顺序编秩，Rx代表Xi的秩次，Ry代表Yi的秩次如果两个值大小一样，则秩次为(index1 + index2) / 2 di = Rx - Ry Sperman系数和Pearson系数在效率上等价 0 &lt; IrI &lt; 1 表示存在不同程度线性相关 IrI &lt;= 0.3 → 不存在线性相关 0.3 &lt; IrI &lt;= 0.5 → 低度线性相关 0.5 &lt; IrI &lt;= 0.8 → 显著线性相关 IrI &gt; 0.8 → 高度线性相关 1234567891011121314151617181920212223# Sperman秩相关系数 - 推导data = pd.DataFrame({'智商':[106,86,100,101,99,103,97,113,112,115], '每周看电视小时数':[7,0,27,50,12,29,20,28,6,17]})print(data)print('----------------------')data.sort_values('智商',inplace = True)data['range1'] = np.arange(1,len(data)+1)data.sort_values('每周看电视小时数',inplace = True)data['range2'] = np.arange(1,len(data)+1)print(data)print('-----------------------')data['d'] = data['range1'] - data['range2']data['d2'] = data['d'] ** 2print(data)print('-----------------------')n = len(data)rs = 1 - 6 * (data['d2'].sum()) / (n * (n**2 - 1))print('Sperman相关系数为：%.4f' % rs) 1234567891011121314151617181920212223242526272829303132333435363738 智商 每周看电视小时数0 106 71 86 02 100 273 101 504 99 125 103 296 97 207 113 288 112 69 115 17---------------------- 智商 每周看电视小时数 range1 range21 86 0 1 18 112 6 8 20 106 7 7 34 99 12 3 49 115 17 10 56 97 20 2 62 100 27 4 77 113 28 9 85 103 29 6 93 101 50 5 10----------------------- 智商 每周看电视小时数 range1 range2 d d21 86 0 1 1 0 08 112 6 8 2 6 360 106 7 7 3 4 164 99 12 3 4 -1 19 115 17 10 5 5 256 97 20 2 6 -4 162 100 27 4 7 -3 97 113 28 9 8 1 15 103 29 6 9 -3 93 101 50 5 10 -5 25-----------------------Sperman相关系数为：0.1636 Sperman算法12345678# Sperman秩相关系数 - 算法data = pd.DataFrame({'智商':[106,86,100,101,99,103,97,113,112,115], '每周看电视小时数':[7,0,27,50,12,29,20,28,6,17]})print(data)print('----------------------')data.corr(method = 'spearman') 12345678910111213 智商 每周看电视小时数0 106 71 86 02 100 273 101 504 99 125 103 296 97 207 113 288 112 69 115 17---------------------- 统计分析 统计分析，指对收集到的有关数据资料进行整理归类并进行解释的过程 统计指标对定量数据进行统计描述，常从集中趋势和离中趋势两个方面进行分析 集中趋势算数平均数12345678910111213141516171819# 集中趋势度量：指一组数据向某一中心靠拢的倾向，核心在于寻找数据的代表值或中心值，即统计平均数# 算数平均数data = pd.DataFrame({'value':np.random.randint(100,120,100), 'f':np.random.rand(100)})data['f'] = data['f'] / data['f'].sum()data.head()print('-------------')# 简单算数平均值 = 总和 / 样本数量 (不涉及权重)mean = data['value'].mean()print('简单算数平均值为：%.2f' % mean)#加权算数平均值 = (x1f1 + x2f2 + ... + xnfn) / (f1 + f2 + ... + fn) mean_w = (data['value'] * data['f']).sum() / data['f'].sum()print('加权算数平均值为：%.2f' % mean_w) 123-------------简单算数平均值为：109.20加权算数平均值为：109.26 位置平均数123456789101112131415161718192021222324252627# 位置平均数data = pd.DataFrame({'value':np.random.randint(100,130,100), 'f':np.random.rand(100)})data['f'] = data['f'] / data['f'].sum()print(data.head())print('-----------------')m = data['value'].mode().tolist()print('众数为' % m)med = data['value'].median()print('中位数为%i' % med)plt.rc('font',family = 'simhei',size = 15)plt.figure(figsize = (16,6))data['value'].plot(kind = 'kde',style = '--k',grid = True)plt.axvline(mean,color = 'r',linestyle = '--',alpha = 0.6)plt.text(mean + 5,0.005,'简单算数平均值为：%.2f' % mean,color = 'r')plt.axvline(mean_w,color = 'b',linestyle = '--',alpha = 0.6)plt.text(mean + 5,0.01,'加权算数平均值为：%.2f' % mean_w,color = 'b')plt.axvline(med,color = 'g',linestyle = '--',alpha = 0.6)plt.text(med + 5,0.015,'简单算数平均值为：%.2f' % med,color = 'g') 123456789 value f0 105 0.0025971 118 0.0005802 122 0.0044483 122 0.0058034 104 0.000088-----------------众数为中位数为116 1Text(121.0, 0.015, '简单算数平均值为：116.00') 离中趋势极差与分位差1234567891011121314151617181920212223242526# 离中趋势度量 - 指一组数据中各数据以不同程度的距离偏离中心的趋势# 极差、分位差data = pd.DataFrame(np.random.rand(30,2)*1000, columns = ['A_sale','B_sale'], index = pd.period_range('20180801','20180830'))print(data.head())print('---------------')a_r = data['A_sale'].max() - data['A_sale'].min()b_r = data['B_sale'].max() - data['B_sale'].min()print('A销售额的极差为：%.2f,B销售额的极差为：%.2f' % (a_r,b_r))print('---------------')sta = data['A_sale'].describe()stb = data['B_sale'].describe()a_iqr = sta.loc['75%'] - sta.loc['25%']b_iqr = stb.loc['75%'] - stb.loc['25%']print('A销售额的分位差为：%.2f,B销售额的分位差为：%.2f' % (a_iqr,b_iqr))print('---------------')color = dict(boxes = 'Green',whiskers = 'Orange',medians = 'Blue', caps = 'Gray')data.plot.box(vert = False,grid = True,color = color,figsize = (16,8))plt.xlim(0,1000) 1234567891011 A_sale B_sale2018-08-01 284.722111 866.7028502018-08-02 443.579648 448.1901612018-08-03 365.901111 680.0839812018-08-04 87.793742 984.7903482018-08-05 544.437922 90.517866---------------A销售额的极差为：954.10,B销售额的极差为：953.65---------------A销售额的分位差为：455.72,B销售额的分位差为：637.73--------------- 方差与标准差1234567891011121314151617181920212223242526# 方差、标准差a_std = sta.loc['std']b_std = stb.loc['std']a_var = data['A_sale'].var()b_var = data['B_sale'].var()print('A销售额的标准差为：%.2f,B销售额的标准差为：%.2f' % (a_std,b_std))print('A销售额的方差为：%.2f,B销售额的方差为：%.2f' % (a_var,b_var))# 方差 → 各组中数值与算数平均数离差平方的算数平均数# 标准差 → 方差的平方根# 标准差是最常用的离中趋势指标 → 标准差越大，离中趋势越明显fig = plt.figure(figsize = (16,6))ax1 = fig.add_subplot(1,2,1)data['A_sale'].plot(kind = 'kde',style = 'k--',grid = True,title = 'A密度曲线')plt.axvline(sta.loc['50%'],color = 'r',linestyle = '--', alpha = 0.6)plt.axvline(sta.loc['50%']-a_std,color = 'b',linestyle = '--', alpha = 0.6)plt.axvline(sta.loc['50%']+a_std,color = 'b',linestyle = '--', alpha = 0.6)ax2 = fig.add_subplot(1,2,2)data['B_sale'].plot(kind = 'kde',style = 'k--',grid = True,title = 'B密度曲线')plt.axvline(stb.loc['50%'],color = 'r',linestyle = '--', alpha = 0.6)plt.axvline(stb.loc['50%']-b_std,color = 'b',linestyle = '--', alpha = 0.6)plt.axvline(stb.loc['50%']+b_std,color = 'b',linestyle = '--', alpha = 0.6) 12A销售额的标准差为：270.43,B销售额的标准差为：315.20A销售额的方差为：73133.58,B销售额的方差为：99350.72 总结数据特征分析的几个基本分析思维已学习完，分别有：分布分析、对比分析、帕累托分析、相关性分析以及统计分析。这五种分析思路是对数据特征进行分析的基础，可以引导自己快速将数据整理归类，记录下来以供随时翻阅，感谢阅读～","link":"/2020/10/22/%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/"},{"title":"什么是监督学习和非监督学习？","text":"摘要监督学习与非监督学习的基本认识，以及常用的几种基础算法的介绍～ 前言本文将记录下学习数据分析中几个最为常见的基本算法，先来了解一下其算法概念 监督学习 监督学习，是一个机器学习中的方法。通过已有的样本数据的样本值x和结果值y去训练得到一个最优模型,再利用这个模型将所有的输入映射为相应的输出。 根据输出数据又分为回归问题和分类问题。回归问题通常输出是一个连续的数值,分类问题的输出是几个特定的数值。 回归问题 概念：在统计学中,回归分析指的是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法 按照自变量和因变量之间的关系类型,可分为线性回归分析和非线性回归分析，线性回归是监督学习中最为常用，也是最为重要的一个方法 分类问题 监督学习中针对分类问题最常用的算法是：最邻近分类算法，简称KNN。 核心逻辑：在距离空间里，如果一个样本的最接近的k个邻居里，绝大多数属于某个类别，则该样本也属于这个类别 非监督学习 无监督学习，是人工智能网络的一种算法。其目的是去对原始资料进行分类，以便了解资料内部结构。 简言之，就是根据未知类别的训练样本解决模式识别中的各种问题。一般来说非监督学习的训练样本全是特征量x,无结果值y；非监督学习更多时候做聚类或降维 PCA主成分分析 PCA主成分分析是最广泛的无监督算法，也是最基础的降维算法 通过线性变换将原始数据变换为一组各维度线性无关的表示,用于提取数据的主要特征量 → 高维数据降维 K—means聚类 最常用的机器学习聚类算法,且为典型的基于距离的聚类算法 K均值：基于原型的、划分的距离技术，它试图发现用户指定个数(K)的簇，以欧式距离作为相似度测度。 随机算法蒙特卡罗算法 蒙特卡罗算法，又称随机抽样或统计实验方法,是以概率和统计理论方法为基础的一种计算方法 使用随机数(或更常见的伪随机数)来解决很多计算问题,将所求解的问题同一定的概率模型向联系,用电子计算机实现统计模拟或抽样,以获得问题的近似解 蒙特卡罗算法特点：采样越多，越近似最优解。 以下是几种常见算法的基本应用 线性回归线性回归使用最佳的拟合直线在因变量(Y)和一个或多个自变量(X)之间建立一种关系。 简单线性回归(一元线性回归)表达式： Y = a + b * X多元线性回归表达式：Y = a + b1 * X + b2 * X , 可根据给定的预测变量S来预测目标变量的值 tips：核心在于拟合直线与样本值的误差项满足均值为0,方差为某个特定值的正态分布 一元线性回归123456# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 12345678910111213141516171819202122232425262728293031from sklearn.linear_model import LinearRegression# 构建数据rng = np.random.RandomState(1)xtrain = 10 + rng.rand(30)ytrain = 8 + 4 * xtrain + rng.rand(30)fig = plt.figure(figsize = (17,9))ax1 = fig.add_subplot(1,2,1)plt.scatter(xtrain,ytrain,marker = '.',color = 'k')plt.grid()plt.title('样本数据散点图')model = LinearRegression()model.fit(xtrain[:,np.newaxis],ytrain)# 斜率print(model.coef_)# 截距print(model.intercept_)xtest = np.linspace(10,11,1000)ytest = model.predict(xtest[:,np.newaxis])ax2 = fig.add_subplot(1,2,2)plt.scatter(xtrain,ytrain,marker = '.',color = 'k')plt.plot(xtest,ytest,color = 'k')plt.grid()plt.title('线性回归拟合') 123# 输出[4.04484138]7.9992457345747 123456789101112131415161718192021# 误差rng = np.random.RandomState(8)xtrain = 10 * rng.rand(15)ytrain = 8 + 4 * xtrain + rng.rand(15) * 30model.fit(xtrain[:,np.newaxis],ytrain)xtest = np.linspace(0,10,1000)ytest = model.predict(xtest[:,np.newaxis])plt.figure(figsize = (17,9)) # 拟合直线 plt.plot(xtest,ytest,color = 'r',linestyle = '--')plt.scatter(xtrain,ytrain,marker = '.',color = 'k')ytest2 = model.predict(xtrain[:,np.newaxis])plt.scatter(xtrain,ytest2,marker = 'x',color = 'g')plt.plot([xtrain,xtrain],[ytrain,ytest2],color = 'gray')plt.grid()plt.xlim(0,10)plt.title('误差') 1Text(0.5, 1.0, '误差') 多元线性回归123456789101112131415161718192021222324# 构建数据rng = np.random.RandomState(3)xtrain = 10 * rng.rand(150,4)# ytrain = 20 + np.dot(xtrain,[1.5,2,-4,3])ytrain = 20 + np.dot(xtrain,[1.5,2,-4,3]) + rng.rand(150)df = pd.DataFrame(xtrain,columns = ['b1','b2','b3','b4'])df['y'] = ytrainpd.scatter_matrix(df[['b1','b2','b3','b4']],figsize = (17,9), diagonal= 'kde', alpha=0.5,range_padding=0.1)print(df.head())# 查看4个自变量是否有线性相关model = LinearRegression()model.fit(df[['b1','b2','b3','b4']],df['y'])print('斜率为：',model.coef_)print('截距为：%.4f' % model.intercept_)print('线性回归函数为： \\n y = %.1fx1 + %.1fx2 + %.1fx3 + %.1fx4 + %.1f' % (model.coef_[0],model.coef_[1],model.coef_[2],model.coef_[3],model.intercept_)) 12345678910 b1 b2 b3 b4 y0 5.507979 7.081478 2.909047 5.108276 46.3646561 8.929470 8.962931 1.255853 2.072429 52.7517662 0.514672 4.408098 0.298762 4.568332 42.3659203 6.491440 2.784873 6.762549 5.908628 26.2457854 0.239819 5.588541 2.592524 4.151012 34.592464斜率为： [ 1.5012069 2.00117753 -4.00340344 3.00076621]截距为：20.4865线性回归函数为： y = 1.5x1 + 2.0x2 + -4.0x3 + 3.0x4 + 20.5 线性回归模型评估一般通过以下几个参数验证回归模型: SSE → (和方差、误差平方和)：拟合数据和原始数据对应点的误差的平方和 MSE → (均方差、方差)：预测数据和原始数据对应点误差的平方和的均值 RMSE → (均方差、标准差)：回归系统的拟合标准差,就是MSE的平方根 R-square(确定系数)：SSR与SST的比值 SSR：预测数据与原始数据均值之差的平方和 SST：原始数据与均值之差的平方和 → SST = SSE + SSR tips：SSE越接近0,说明模型选择和拟合越好,数据预测也越成功 ; MSE越小越好R-square(确定系数)的取值范围[0,1],越接近1，表明方程的变量对Y的解释能力越强,这个模型对数据拟合的越好 123456789101112131415161718192021222324from sklearn import metricsrng = np.random.RandomState(1)xtrain = 10 * rng.rand(30)ytrain = 8 + 4 * xtrain + rng.rand(30) * 3model = LinearRegression()model.fit(xtrain[:,np.newaxis],ytrain)ytest = model.predict(xtrain[:,np.newaxis])mse = metrics.mean_squared_error(ytrain,ytest)rmse = np.sqrt(mse)# ssr = ((ytest - ytrain.mean())**2).sum()# sst = ((ytrain - ytrain.mean())**2).sum()# r2 = ssr / sstr2 = model.score(xtrain[:,np.newaxis],ytrain)print('均方差MSE为：%.5f' % mse)print('均方根RMSE为：%.5f' % rmse)print('确定系数R-square为：%.5f' % r2) 123均方差MSE为：0.78471均方根RMSE为：0.88584确定系数R-square为：0.99465 KNN-最邻近分类在距离空间里，如果一个样本的最接近的k个邻居里，绝大多数属于某个类别，则该样本也属于这个类别 以下是几个KNN-最邻近分类的案例 1234567891011121314151617181920212223242526272829303132# 导入KNN分类模块from sklearn import neighbors# 构建一组电影数据data = pd.DataFrame({'name':['北京遇上西雅图','喜欢你','疯狂动物城','战狼2','力王','敢死队'], 'fight':[3,2,1,101,99,98], 'kiss':[104,100,81,10,5,2], 'type':['Romance','Romance','Romance','Action','Action','Action']})knn = neighbors.KNeighborsClassifier()knn.fit(data[['fight','kiss']],data['type'])# cat = np.array([[18,90]])print('预测电影类型为：' , knn.predict([[18,90]]))plt.figure(figsize = (17,9))plt.scatter(data[data['type'] == 'Romance']['fight'], data[data['type'] == 'Romance']['kiss'], color = 'r',marker = 'x',label = 'Romance')plt.scatter(data[data['type'] == 'Action']['fight'], data[data['type'] == 'Action']['kiss'], color = 'g',marker = 'x',label = 'Action')plt.grid()plt.legend()plt.scatter(18,90,color = 'r',marker = 'o',label = 'Romance')plt.ylabel('kiss')plt.xlabel('fight')plt.text(18,90,'&lt;你的名字&gt;',color = 'y',fontsize = 18) 1预测电影类型为： ['Romance'] 1Text(18, 90, '&lt;你的名字&gt;') 123456789101112131415161718192021222324# 随机值模拟data2 = pd.DataFrame(np.random.randn(200,2)*50,columns=['fight','kiss'])data2['typetest'] = knn.predict(data2)data2.head()plt.figure(figsize = (17,9))plt.scatter(data[data['type'] == 'Romance']['fight'], data[data['type'] == 'Romance']['kiss'], color = 'r',marker = 'x',label = 'Romance')plt.scatter(data[data['type'] == 'Action']['fight'], data[data['type'] == 'Action']['kiss'], color = 'g',marker = 'x',label = 'Action')plt.grid()plt.legend()plt.scatter(data2[data2['typetest'] == 'Romance']['fight'], data2[data2['typetest'] == 'Romance']['kiss'], color = 'r',marker = 'o',label = 'Romance')plt.scatter(data2[data2['typetest'] == 'Action']['fight'], data2[data2['typetest'] == 'Action']['kiss'], color = 'g',marker = 'o',label = 'Action')plt.grid()plt.legend() 12345678910111213141516171819202122232425262728# 多参数分类from sklearn import datasetsiris = datasets.load_iris()print(iris.keys())print('数据长度为：%i条' % len(iris['data']))print(iris.feature_names)# 特征分类：萼片长度、萼片宽度、花瓣长度、花瓣宽度print(iris.target_names)# print(iris.target)data = pd.DataFrame(iris.data,columns= iris.feature_names)data['target'] = iris.targetty = pd.DataFrame({'target':[0,1,2], 'target_names':iris.target_names})df = pd.merge(data,ty,on = 'target')knn = neighbors.KNeighborsClassifier()knn.fit(iris.data,iris.target)# 多参数进行分类pre_data = knn.predict([[0.2,0.1,0.3,0.4]])print(pre_data)df.head() 1234dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])数据长度为：150条['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']['setosa' 'versicolor' 'virginica'] PCA主成分分析 通过线性变换将原始数据变换为一组各维度线性无关的表示,用于提取数据的主要特征分类 123456789101112131415161718# 定义随机种子rng = np.random.RandomState(8)# 构建数据data = np.dot(rng.rand(2,2),rng.randn(2,200)).Tdf = pd.DataFrame({'X1':data[:,0], 'X2':data[:,1]})print(df.head())print(df.shape)plt.figure(figsize = (16,6))plt.scatter(df['X1'],df['X2'],alpha=0.7,marker = '.')plt.axis('auto')# plt.xlim(-1,2)plt.grid() 1234567 X1 X20 -1.174787 -1.4041311 -1.374449 -1.2946602 -2.316007 -2.1661093 0.947847 1.4604804 1.762375 1.640622(200, 2) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 二维数据降维 - 构建模型,分析主成分&quot;&quot;&quot;PCA参数：('n_components=None', 'copy=True', 'whiten=False', &quot;svd_solver='auto'&quot;,'tol=0.0', &quot;iterated_power='auto'&quot;, 'random_state=None')&quot;&quot;&quot;from sklearn.decomposition import PCApca = PCA(n_components= 1)pca.fit(df)# pca.fit(X, y=None) → 调用fit方法的对象本身,如pca.fit(X),表示用X对pca这个对象进行训练# n_components :PCA算法中所要保留的主成分个数n,也即保留下来的特征个数n# copy :默认为True, → 表示是否在运行算法时,将原始训练数据复制一份# 特征值print(pca.explained_variance_)# 特征向量 - 具有最大方差的成分print(pca.components_)# 特征值个数 - 返回所保留的成分个数nprint(pca.n_components)# 2.797 * (0.779 * x1 + 0.627 * x2)# 数据转换x_pca = pca.transform(df)# 将降维后的数据转换成原始数据x_new = pca.inverse_transform(x_pca)print('original shape：',df.shape)print('transformd shape：',x_pca.shape)print(x_pca[:5])print('-------------------------------')plt.figure(figsize = (17,6))plt.scatter(df['X1'],df['X2'],alpha=0.7,marker = '.')plt.scatter(x_new[:,0],x_new[:,1],alpha=0.8,marker = '.',color = 'r')plt.axis('equal')plt.grid() 1234567891011[2.79699086][[-0.7788006 -0.62727158]]1original shape： (200, 2)transformd shape： (200, 1)[[ 1.77885258] [ 1.8656813 ] [ 3.14560277] [-1.67114513] [-2.41849842]]------------------------------- 12345678910111213141516171819202122232425262728293031323334353637# 多维数据降维from sklearn.datasets import load_digitsdigits = load_digits()print(digits.keys())print('数据长度为：%i条' % len(digits['data']))print('数据形状为：%i条' , digits.data.shape)print(digits.data[:2])print('---------------------------------------')pca = PCA(n_components= 2)pca.fit(digits.data)prj = pca.transform(digits.data)print('original shape：',digits.data.shape)print('transformd shape：',prj.shape)# prj = pca.fit_transform(digits.data)print(prj[:5])print('---------------------------------------')# 查看特征值print(pca.explained_variance_)print('--------------------------------------')# print(pca.components_) #特征向量 - 具有最大方差的成分# print(pca.n_components) #特征值个数 - 返回所保留的成分个数nplt.figure(figsize = (13,6))plt.scatter(prj[:,0],prj[:,1], c = digits.target,edgecolor = 'none',alpha = 0.6, cmap = 'Reds',s = 5)plt.axis('equal')plt.grid()plt.colorbar() 12345678910111213141516dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])数据长度为：1797条数据形状为：%i条 (1797, 64)[[ 0. 0. 5. 13. 9. 1. 0. 0. 0. 0. 13. 15. 10. 15. 5. 0. 0. 3. 15. 2. 0. 11. 8. 0. 0. 4. 12. 0. 0. 8. 8. 0. 0. 5. 8. 0. 0. 9. 8. 0. 0. 4. 11. 0. 1. 12. 7. 0. 0. 2. 14. 5. 10. 12. 0. 0. 0. 0. 6. 13. 10. 0. 0. 0.] [ 0. 0. 0. 12. 13. 5. 0. 0. 0. 0. 0. 11. 16. 9. 0. 0. 0. 0. 3. 15. 16. 6. 0. 0. 0. 7. 15. 16. 16. 2. 0. 0. 0. 0. 1. 16. 16. 3. 0. 0. 0. 0. 1. 16. 16. 6. 0. 0. 0. 0. 1. 16. 16. 6. 0. 0. 0. 0. 0. 11. 16. 10. 0. 0.]]---------------------------------------original shape： (1797, 64)transformd shape： (1797, 2)---------------------------------------[179.0069301 163.71774688] 123456789101112131415161718192021# 主成分筛选pca = PCA(n_components= 10)pca.fit(digits.data)prj = pca.transform(digits.data)print('original shape：',digits.data.shape)print('transformd shape：',prj.shape)# prj = pca.fit_transform(digits.data)prj[:5]print('---------------------------------------')s = pca.explained_variance_c_s = pd.DataFrame({'b':s, 'b_sum':s.cumsum() / s.sum()})c_s['b_sum'].plot(style = '--ko',figsize = (16,6))plt.axhline(0.85,color = 'r',linestyle = '--',alpha = 0.6)plt.text(6,c_s['b_sum'].iloc[6] - 0.08,'第七个成分累计贡献率超过85%',color = 'r')plt.grid() 123original shape： (1797, 64)transformd shape： (1797, 10)--------------------------------------- K-means聚类聚类分析：是一种将研究对象分为相对同质的群组的统计分析技术 将观测对象的群体按照相似性和相异性进行不同群组的划分,划分后每个群组内部各对象相似度很高, 而不同群组之间的对象彼此相异度很高 聚类分析后会产生一组集合,主要用于降维 K均值算法实现逻辑：K均值算法需要输入待聚类的数据和欲聚类的簇数K1、随机生成k个初始点作为质心2、将数据集中的数据按照距离质心的远近分到各个簇中3、将各个簇中的数据求平均值,作为新的质心,重复上一步,直到所有的簇不再改变 12345678910111213141516171819202122232425262728293031323334353637383940414243# make_blobs 聚类数据生成器from sklearn.datasets.samples_generator import make_blobsx,y_ture = make_blobs(n_samples= 300, centers= 4, cluster_std= 0.5, random_state= 0)&quot;&quot;&quot;参数解析：n_samples : 待生成的样本总数centers : 类别数cluster_std : 每个类别的方差,如多类数据不同方差,可设置为区间类似[1.0,3.0]这里针对2类数据random_state : 随机数种子x → 生成数据值 , y → 生成数据对应的类别标签&quot;&quot;&quot;print(x[:5])print(y_ture[:5])print('-------------------------------------')# plt.figure(figsize = (16,6))# plt.scatter(x[:,0],x[:,1],s = 10 , alpha = 0.6)# plt.grid()from sklearn.cluster import KMeanskmeans = KMeans(n_clusters= 4)kmeans.fit(x)y_kmeans = kmeans.predict(x)centroids = kmeans.cluster_centers_plt.figure(figsize = (16,6))plt.scatter(x[:,0],x[:,1],c = y_kmeans,cmap = 'Dark2',s = 50,alpha = 0.5,marker = 'x')plt.scatter(centroids[:,0],centroids[:,1],c = [0,1,2,3],cmap = 'Dark2',s = 70,marker = 'o')plt.title('K-means 300 points\\n')plt.xlabel('value1')plt.ylabel('value2')plt.grid() 1234567[[ 1.03992529 1.92991009] [-1.38609104 7.48059603] [ 1.12538917 4.96698028] [-1.05688956 7.81833888] [ 1.4020041 1.726729 ]][1 3 0 3 1]------------------------------------- 写在最后以上是在数据分析过程中常用到的几种基本算法，可以快速的定位数据分析的方向和数据类型。其中线性回归和主成分分析会更常见一下，这份笔记也是为了随时翻阅而记录，欢迎指正，感谢阅读～","link":"/2020/10/29/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"title":"随机算法之蒙特卡罗","text":"摘要随机算法的基础认知以及简单推导应用～ 随机算法蒙特卡罗算法 蒙特卡罗算法，又称随机抽样或统计实验方法,是以概率和统计理论方法为基础的一种计算方法 使用随机数(或更常见的伪随机数)来解决很多计算问题,将所求解的问题同一定的概率模型向联系,用电子计算机实现统计模拟或抽样,以获得问题的近似解 以一个概率模型为基础,按照这个模型所描绘段得过程,通过模拟实验的结果,作为问题的近似解1、构造或描述概率过程2、实现从已知概率分布抽样3、建立各种估计量 优点：简单快速特点：随机采样上计算得到近似结果,随着采样的增多,得到的结果是正确结果的概率逐渐加大 π的计算123456#导入模块import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline 1234567891011121314151617181920212223242526272829303132333435363738394041# 设置投点次数n = 10000# 设置半径r = 1.0# 圆心a,b = (0.0,0.0)#区域边界xmin,xmax = a-r,a+rymin,ymax = b-r,b+r# 在正方形区域内随机投点# numpy.random.uniform(low,high,size) → 从一个均匀分布[low,high)中随机采样，均匀分布x = np.random.uniform(xmin,xmax,n)y = np.random.uniform(ymin,ymax,n)fig = plt.figure(figsize = (6,6))axes = fig.add_subplot(1,1,1)plt.plot(x,y,'ro',markersize = 1)plt.axis('equal')plt.xlim(-1,1)plt.ylim(-1,1)d = np.sqrt((x - a)**2 + (y - b)**2)res = sum(np.where(d&lt;r,1,0))pi = 4 * res / nprint(pi)# 导入绘制圆形 Circle (椭圆Ellipse)from matplotlib.patches import Circlecircle = Circle(xy = (a,b),radius = r,alpha = 0.5,color = 'gray')axes.add_patch(circle)plt.grid(True,linestyle = '--',linewidth = '0.8')plt.show() 13.1256 积分计算123456789101112131415161718192021222324252627282930313233343536# 设置投点次数n = 10000# 矩形区域边界x_min,x_max = 0.0,1.0y_min,y_max = 0.0,1.0#在矩形区域内随机投点x = np.random.uniform(x_min,x_max,n)y = np.random.uniform(y_min,y_max,n)#统计落在函数图像下方的点的数目def f(x): return x**2res = sum(np.where(y &lt; f(x),1,0))#计算定积分的近似值integral = res / nprint('integral:',integral)fig = plt.figure(figsize = (6,6))axes = fig.add_subplot(111)axes.plot(x,y,'ro',markersize = 1)plt.axis('equal')plt.xlim(0,1)plt.ylim(0,1)xi = np.linspace(0,1,100)yi = xi ** 2plt.plot(xi,yi,'--k')plt.fill_between(xi,yi,0,color = 'gray',alpha = 0.5,label = 'area')plt.grid() 1integral: 0.3367 实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 厕所排队问题# 1、两场电影结束时间相隔较长,互不影响# 2、每场电影结束之后会有20个人想上厕所# 3、这20个人会在0到10分钟内全部到达厕所# 4、每个人上厕所时间在1-3分钟之间# 首先模拟最简单的情况,也就是厕所只有一个位置,不考虑俩人共用的情况则没人必须等上一人完毕方可进行# 参数：到达时间 / 等待时间 / 开始上厕所时间 / 结束时间arrivingtime = np.random.uniform(0,10,size = 20)arrivingtime.sort()workingtime = np.random.uniform(1,3,size = 20)# np.random.uniform 随机数：均匀分布的样本值print(arrivingtime)print('--------------------------')startingtime = [0 for i in range(20)]finishtime = [0 for i in range(20)]waitingtime = [0 for i in range(20)]emptytime = [0 for i in range(20)]startingtime[0] = arrivingtime[0]finishtime[0] = startingtime[0] + workingtime[0]waitingtime[0] = startingtime[0] - arrivingtime[0]print(startingtime[0],workingtime[0],finishtime[0],waitingtime[0])print('---------------------------')# 判断：如果下一个人在上一个人完成之前到达,则 开始时间 = 上一个人的结束时间# 否则：开始时间 = 到达时间,且存在空闲时间 = 到达时间 - 上一个人的完成时间for i in range(1,len(arrivingtime)): if finishtime[i-1] &gt; arrivingtime[i]: startingtime[i] = finishtime[i-1] else: startingtime[i] = arrivingtime[i] emptytime[i] = arrivingtime[i] - finishtime[i-1] finishtime[i] = startingtime[i] + workingtime[i-1] waitingtime[i] = startingtime[i] - arrivingtime[i] print('第%d个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间\\n' % i, arrivingtime[i], startingtime[i], workingtime[i], finishtime[i], waitingtime[i], '\\n')print('arerage waiting time is %f' % np.mean(waitingtime))print('-----------------------------') fig = plt.figure(figsize = (16,7))plt.plot(waitingtime,'-go')plt.grid(True,linestyle = '--',color = 'gray',linewidth = '0.8')plt.title('蒙特卡罗模拟 - 厕所排队问题')plt.show() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[0.16587897 0.62204463 1.12297214 1.61646409 2.2734175 3.58188608 4.06180627 4.42582219 4.92397497 5.00023146 6.3967079 6.82771899 6.96481151 7.21465387 7.34371744 7.51496538 8.68568567 8.94934034 9.61589877 9.7940074 ]--------------------------0.16587897242361538 1.9163595239913254 2.0822384964149405 0.0---------------------------第1个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 0.6220446336305152 2.0822384964149405 2.188198872364582 3.998598020406266 1.4601938627844253 第2个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 1.122972137882483 3.998598020406266 2.764457045410645 6.1867968927708485 2.875625882523783 第3个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 1.6164640948717424 6.1867968927708485 1.8619334107218273 8.951253938181495 4.570332797899106 第4个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 2.27341749721095 8.951253938181495 1.9558472451168596 10.813187348903321 6.677836440970545 第5个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 3.5818860753684123 10.813187348903321 2.4715009771412073 12.76903459402018 7.231301273534909 第6个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 4.061806266415359 12.76903459402018 1.0395021167184366 15.240535571161388 8.70722832760482 第7个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 4.425822187022732 15.240535571161388 2.9938063833706754 16.280037687879826 10.814713384138656 第8个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 4.923974971103213 16.280037687879826 1.5688516924561942 19.2738440712505 11.356062716776613 第9个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 5.0002314565754515 19.2738440712505 1.6350976155013934 20.842695763706693 14.273612614675049 第10个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 6.396707898323078 20.842695763706693 2.78313954407255 22.477793379208087 14.445987865383614 第11个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 6.827718993543048 22.477793379208087 1.4873508450792379 25.260932923280638 15.65007438566504 第12个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 6.96481151330325 25.260932923280638 2.7352000597223616 26.748283768359876 18.29612140997739 第13个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 7.21465386868163 26.748283768359876 2.1947890646530044 29.48348382808224 19.533629899678246 第14个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 7.3437174368142575 29.48348382808224 1.2871138118247274 31.678272892735244 22.139766391267983 第15个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 7.514965384306672 31.678272892735244 1.2507700564206545 32.96538670455997 24.163307508428574 第16个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 8.685685670740416 32.96538670455997 1.9506767657028061 34.216156760980624 24.279701033819553 第17个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 8.949340336055485 34.216156760980624 1.8000774818307035 36.16683352668343 25.26681642492514 第18个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 9.615898768155551 36.16683352668343 1.1238441426072674 37.96691100851414 26.55093475852788 第19个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 9.794007397518186 37.96691100851414 1.6867760138470005 39.09075515112141 28.172903610995952 arerage waiting time is 14.323308----------------------------- 写在结尾以上是对随机算法中的蒙特卡罗算法的一些初步认知。在数据分析中，这种随机算法应用的十分广泛，也是初学者必知必会，有任何不对的地方，恳请指正～感谢阅读！","link":"/2020/11/13/%E9%9A%8F%E6%9C%BA%E7%AE%97%E6%B3%95-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97/"}],"tags":[{"name":"Python库","slug":"Python库","link":"/tags/Python%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"python数据类型","slug":"python数据类型","link":"/tags/python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"name":"可视化图表","slug":"可视化图表","link":"/tags/%E5%8F%AF%E8%A7%86%E5%8C%96%E5%9B%BE%E8%A1%A8/"},{"name":"对比分析","slug":"对比分析","link":"/tags/%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/"},{"name":"二八定律","slug":"二八定律","link":"/tags/%E4%BA%8C%E5%85%AB%E5%AE%9A%E5%BE%8B/"},{"name":"名词解析","slug":"名词解析","link":"/tags/%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/"},{"name":"数据分布","slug":"数据分布","link":"/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/"},{"name":"正态性检验","slug":"正态性检验","link":"/tags/%E6%AD%A3%E6%80%81%E6%80%A7%E6%A3%80%E9%AA%8C/"},{"name":"MySQL练习题","slug":"MySQL练习题","link":"/tags/MySQL%E7%BB%83%E4%B9%A0%E9%A2%98/"},{"name":"相关性分析","slug":"相关性分析","link":"/tags/%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90/"},{"name":"统计分析","slug":"统计分析","link":"/tags/%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/"},{"name":"算法基础","slug":"算法基础","link":"/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"}],"categories":[{"name":"python","slug":"python","link":"/categories/python/"},{"name":"SQL","slug":"SQL","link":"/categories/SQL/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"Seaborn","slug":"python/Seaborn","link":"/categories/python/Seaborn/"},{"name":"Matplotlib","slug":"python/Matplotlib","link":"/categories/python/Matplotlib/"},{"name":"data_type","slug":"python/data-type","link":"/categories/python/data-type/"},{"name":"MySQL","slug":"SQL/MySQL","link":"/categories/SQL/MySQL/"},{"name":"数据分析","slug":"数据分析","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"post","slug":"blog/post","link":"/categories/blog/post/"},{"name":"可视化","slug":"数据分析/可视化","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"数据特征","slug":"数据分析/数据特征","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81/"},{"name":"名词解析","slug":"数据分析/名词解析","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/"},{"name":"算法","slug":"数据分析/算法","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E7%AE%97%E6%B3%95/"}]}