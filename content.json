{"pages":[{"title":"","text":"About Me分享出自《被讨厌的勇气》一书中的一段话： 关于自己的人生你能做到的就只有 “ 选择自己认为最好的道路 “ 。另一方面，别人如何评价你的选择，那是别人的课题，你根本无法左右。基本上，一切人际关系矛盾都起因于对别人的课题妄加干涉或者自己的课题被别人妄加干涉。只要能够进行课题分离，人际关系就会发生巨大改变。而辨别究竟是谁的课题的方法非常简单，只需要考虑一下”某种选择所带来的结果最终要由谁来承担？” So，但行好事，莫问前程～共勉！！ ->>>>>>>>>>>>>>>>>>>> 个人信息 90后一枚，正努力学习数据分析相关技能 信息管理与信息系统专业 爱好：运动、数独、炉石传说 本站信息 博客主要记录关于数据分析的技能学习的笔记类文章，好记性不如烂笔头嘛～ 另外会记录一下面试练习题以及一些自己遇到的坑(已解决或者未解决的),由于表达能力有限，有疑问处欢迎留言。 本站文章若无特别说明，均为原创，转载请注明来源。 关于博客 网站采用的hexo-theme-amazing主题 追求尽可能的简洁，清晰，易用。 在hexo-theme-amazing主题之上进行了部分修改。 感谢以下开源项目贡献者(不完全统计，如有遗漏欢迎留言)： Hexo Author:Hexo contributors Icarus Author:ppoffice amazing Author:removeif gitalk Author:Gitalk contributors","link":"/about/index.html"},{"title":"","text":"来而不往非礼也畅所欲言，有留必应","link":"/message/index.html"}],"posts":[{"title":"A&#x2F;B 测试的概念认识","text":"摘要深入了解什么是数据指标？以及数据指标的作用和分类～ A/B 测试中的测试说的是什么 测试的重要性 测试是数据分析中的最常用的一种手段 早期选定的目标往往是尝试性的，但并不是一成不变的。因为早期设定的目标和关键数据指标一般都是基于假设，这就导致与用户或者现实中的真实行为之间有着差别，或细微，或显著。所以，及时调整目标和关键数据指标都是可行的，也是必要的，而调整所需要的依据(数据支撑)所用的方式一般都通过测试来得到。 测试的概念 通常来说，测试就是通过市场细分、同期群分析或A/B测试来比较两个样本的不同。 常用测试手段 市场细分 概念：细分市场就是一群拥有某种共同特征的人。 举例： 了解产品对哪类家庭更具吸引力就是一种市场细分，可以反馈出一开始设定的划分活跃/非活跃用户的准绳实际上并不能很好的反映实际用户的参与度。 去餐馆前是否先预约也是一种市场细分，而根据后续的消费、评价等各个数据指标进行分析得出区别，从而进行不同的维护联系 适用场景 可应用于网站、或者其他行业、各种形式的营销 同期群分析 概念：比较相似群体随时间的变化。能够观察处于生命周期不同阶段客户的行为模式，而非一刀切所有用户。 举例： 在产品上线第一个月就”吃螃蟹”的用户与四个月后才加入的用户肯定有不同的上手体验，寻求这对用户流失率有什么影响，就是一种常用的同期群分析的使用场景 适用场景 营收、客户流失率、口碑的病毒式传播、客户支持成本等数据指标 同期群分析实例场景设置：一家网店，每个月能获取1000个新客户，他们每个人都会买一些东西。下表显示了前五个月中平均每位客户带来的营收。 表2-1：5个月平均每位客户营收 1月 2月 3月 4月 5月 客户总数 1000 2000 3000 4000 5000 平均每位客户营收 5美元 4.5美元 4.33美元 4.25美元 4.5美元 由表2-1传达的信息：营收在经历小幅下降后又回升，而评价每位客户带来的营收值几近均一；而网店的生意到底是好是坏？无法回答，原因是没有对比新客户和老客户的表现；而且最主要的是表2-1中将新客户和5个月前注册的老客户的数据是混淆在一起的。 使用相同的数据，根据客户首次光顾的时间按月份进行分段。 表2-2：按照客户首次光顾月份比较营收 1月 2月 3月 4月 5月 新客户数 1000 1000 1000 1000 1000 总客户数 1000 2000 3000 4000 5000 第一个月 5美元 3美元 2美元 1美元 0.5美元 第二个月 6美元 4美元 2美元 1美元 第三个月 7美元 6美元 5美元 第四个月 8美元 7美元 第五个月 9美元 从表2-2中，在第五个月光顾的网店客户，其首月平均消费为9美元，相比较第一个月光顾的客户消费额翻了近1倍，这是一个巨大的增长，而不是像表2-1中显示的几近均一。 根据用户在网店上的**”店龄”**来划分数据，则显示了另一个重要的数据指标。 表2-3：营收数据的同期群分析 使用月份 1 2 3 4 5 1月 5美元 3美元 2美元 1美元 0.5美元 2月 6美元 4美元 2没有 1美元 3月 7美元 6美元 5美元 4月 8美元 7美元 5月 9美元 平均值 7美元 5美元 3美元 1美元 0.5美元 如表2-3所示，1月份的同期群首月消费了5美元，然后逐月递减，到第五个月仅消费0.5美元；往后月份也有所递减，可以说前几个月中用户变现的疲软表现已经对网店营收指标的总体状况造成损害，但后续随着网站的发展，新客户的首月消费有显著的增长，所以呈现出来表2-1的平滑现象。但数据显示，网店其实在茁壮成长，随之而来引起重视的数据指标将是：在首月注册消费后，客户消费的递减量。 小结通过这个实例，具体的展现了同期群分析的实用性。通过同期群分析能够观察处于生命周期不同阶段客户的行为模式，而非忽略个体的自然生命周期，对所有客户一次性切分；而通过比较不同的同期群，可以获知：从总体上看，关键指标的表现是否越来越好。 A/B和多变量测试 A/B测试 概念：假设其他条件保持不变，仅考虑某一属性对用户的影响，就是所谓的A/B测试、 举例： 对半数用户展示一个绿色链接，对另一半用户展示蓝色链接；观察哪种颜色的链接点击率更高 触发访客行为：立即试用产品链接的文字设置为 “免费试用”/“免费开始” 缺点：A/B测试需要大量的数据反馈，在用户流量巨大的大型网站上进行A/B测试能短时间得到答案，而在一些流量相对较小的网站上需要一定的时间成本，同时也可能需要测试多个因素，例如网页的色调、触发用户行为的链接文字、图片效果等 多变量测试 概念：用统计学方法剥离出单个影响因子与结果中某一项指标提升的相关性。 举例： 同时改动产品的多个方面，看看哪个与结果的相关性最大 适用场景 在一些用户流量没那么大的应用场景下，相比较A/B测试的单一属性测试，多变量分析能更快得到结果和数据反馈 总结测试是数据分析的灵魂。市场细分是对按某属性分类的人群进行横向比较，而同期群分析是沿着客户群体的自然生命周期收集并比较相似群体；而A/B测试和多变量测试均是研究在同一时间段内对不同群体提供不同的体验，从而得到哪种体验更得人心。 参考文章: [书籍 - 精益数据分析]","link":"/2021/05/15/AB%20%E6%B5%8B%E8%AF%95%E4%B8%AD%E7%9A%84%E6%B5%8B%E8%AF%95%E8%AF%B4%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88/"},{"title":"大型商场销售额分析","text":"摘要根据销售额数据分析整个大型商场的销售情况，以及转化为可视化图表～ 项目描述项目名称：大型商场销售额分析及数据可视化 数据来源：该项目提供了从不同城市的10家商店中收集的1559种产品的2013年销售数据。 字段说明： Item_Identifier : 唯一产品ID Item_Weight : 产品重量 Item_Fat_Content : 产品是否低脂 Item_Visibility : 该产品占商店总产品展示区的百分比 Item_Type : 产品种类 Item_MRP : 产品最高零售价(标价) Outlet_Identifier : 商品唯一ID Outlet_Establishment_Year : 商品成立的年份 Outlet_Size : 商品的面积 Outlet_Location_Type : 商店所在城市类型 Outlet_Type : 商店的类型 项目目的：根据2013年的销售数据，通过可视化分析产品在不同维度的销售情况。 环境解释：基于jupyter notebook，可视化工具包：seaborn、matplotlib 123456789# 导入模块import pandas as pdimport numpy as npimport seaborn as snsimport warningsimport matplotlib.pyplot as pltwarnings.filterwarnings('ignore')%matplotlib inline 1234567891011# 数据导入test = pd.read_csv('/Users/nanb/Downloads/Test_u94Q5KV.txt',sep = ',')train = pd.read_csv('/Users/nanb/Downloads/Train_UWu5bXk.txt',sep = ',')train['source'] = 'train'test['source'] = 'test'# 测试数据中'Item_Outlet_Sales'缺失，为方便处理，将测试数据中该项赋值为0test['Item_Outlet_Sales'] = 0data = pd.concat([test,train],sort=False) #数据量并非很大，合并使用全部数据进行处理并建模 销售额分析123# 查看Item_Outlet_Sales项的数据分布情况sns.distplot(data['Item_Outlet_Sales']) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2431c860&gt; 1234# 查看偏度系数、峰度系数print('偏度系数Skewness: %f' % data['Item_Outlet_Sales'].skew())print('峰度系数Kurtsis: %f' % data['Item_Outlet_Sales'].kurt()) 偏度系数Skewness: 1.544684 峰度系数Kurtsis: 2.419439 图表解析： 输出结果可以看出：1、偏离正态分布；2、具有明显的正偏态 注意：原先将测试数据里的Item_Outlet_Sales项全赋值为0 数据查看123# 查看训练数据的数值变量的描述统计train.describe() 图表解析： 1、Item_Visibility的最小值为零。这没有实际意义，因为当在商店中销售产品时，可见性不能为0。2、Outlet_Establishment_Years从1985年到2009年各不相同。这种形式的值可能不合适。 相反，如果我们可以将它们转换为特定商店的年龄，它应该对销售产生更好的影响。 123# 检查各列缺失值data.apply(lambda x:sum(x.isnull())) Item_Identifier 0 Item_Weight 2439 Item_Fat_Content 0 Item_Visibility 0 Item_Type 0 Item_MRP 0 Outlet_Identifier 0 Outlet_Establishment_Year 0 Outlet_Size 4016 Outlet_Location_Type 0 Outlet_Type 0 source 0 Item_Outlet_Sales 0 dtype: int64 图表解析：Item_Weight和Outlet_Size存在缺失值 123# 查看每个变量的唯一值data.apply(lambda x:len(x.unique())) Item_Identifier 1559 Item_Weight 416 Item_Fat_Content 5 Item_Visibility 13006 Item_Type 16 Item_MRP 8052 Outlet_Identifier 10 Outlet_Establishment_Year 9 Outlet_Size 4 Outlet_Location_Type 3 Outlet_Type 4 source 2 Item_Outlet_Sales 3494 dtype: int64 图表解析：一共有1559种产品和十个商店,值得注意的是：Item_Type有16个唯一值 1234567# 查看每个字段中不同类别的频率统计categorical_columns = [x for x in data.dtypes.index if data.dtypes[x] == 'object']categorical_columns = [x for x in categorical_columns if x not in ['Item_Identifier','Outlet_Idendtifier','source']]for col in categorical_columns: print('\\nFrequency of Categories for varible %s'%col) print(data[col].value_counts()) Frequency of Categories for varible Item_Fat_Content Low Fat 8485 Regular 4824 LF 522 reg 195 low fat 178 Name: Item_Fat_Content, dtype: int64 Frequency of Categories for varible Item_Type Fruits and Vegetables 2013 Snack Foods 1989 Household 1548 Frozen Foods 1426 Dairy 1136 Baking Goods 1086 Canned 1084 Health and Hygiene 858 Meat 736 Soft Drinks 726 Breads 416 Hard Drinks 362 Others 280 Starchy Foods 269 Breakfast 186 Seafood 89 Name: Item_Type, dtype: int64 Frequency of Categories for varible Outlet_Identifier OUT027 1559 OUT013 1553 OUT046 1550 OUT049 1550 OUT035 1550 OUT045 1548 OUT018 1546 OUT017 1543 OUT010 925 OUT019 880 Name: Outlet_Identifier, dtype: int64 Frequency of Categories for varible Outlet_Size Medium 4655 Small 3980 High 1553 Name: Outlet_Size, dtype: int64 Frequency of Categories for varible Outlet_Location_Type Tier 3 5583 Tier 2 4641 Tier 1 3980 Name: Outlet_Location_Type, dtype: int64 Frequency of Categories for varible Outlet_Type Supermarket Type1 9294 Grocery Store 1805 Supermarket Type3 1559 Supermarket Type2 1546 Name: Outlet_Type, dtype: int64 图表解析：1、Item_Fat_Content:有Low Fat、LF和low fat,以及Regular和reg的数据2、Outlet_Type:type2和type3的数据过少,是否需要合并为一个类别 数据清洗 数据待处理： Item_Visibility的最小值为零。这没有实际意义，因为当在商店中销售产品时，可见性不能为0。 Outlet_Establishment_Years从1985年到2009年各不相同。这种形式的值可能不合适。相反，如果我们可以将它们转换为特定商店的年龄，有可能对销售预测产生更好的影响。 Item_Weight和Outlet_Size存在缺失值 Item_Fat_Content:有Low Fat、LF和low fat,以及Regular和reg的数据 Outlet_Type:type2和type3的数据过少,是否需要合并为一个类别 数据重定义1234567891011# 我们注意到 Item_Visibility 的最小值是0，这没有实际意义。让我们把它看作是丢失的信息，并将其与产品的平均可见性联系起来。data['Item_Visibility'].value_counts()visibility_avg = data.pivot_table(values='Item_Visibility',index='Item_Identifier')miss_bool = (data['Item_Visibility'] == 0)print('number of 0 values initially: %d' %sum(miss_bool))data.loc[miss_bool,'Item_Visibility'] = data.loc[miss_bool,'Item_Identifier'].apply(lambda x:visibility_avg.at[x,'Item_Visibility'])print('number of 0 values after modification: %d' %sum(data['Item_Visibility'] == 0)) number of 0 values initially: 879 number of 0 values after modification: 0 缺失值填充1234567# Item_Weight缺失值：可以考虑用重量平均数填充item_avg_weight = data.pivot_table(values='Item_Weight',index='Item_Identifier')miss_bool = data['Item_Weight'].isnull()print ('Orignal #missing: %d'% sum(miss_bool))data.loc[miss_bool,'Item_Weight'] = data.loc[miss_bool,'Item_Identifier'].apply(lambda x:item_avg_weight.at[x,'Item_Weight'])print ('Final #missing: %d'% sum(data['Item_Weight'].isnull())) Orignal #missing: 2439 Final #missing: 0 12345678910111213141516171819# outlet_size缺失值处理：关于商店的面积因为没有相对的参数去评估，所以这里使用scipy中的mode模块来处理，即用众数来替换空值# 导入模块from scipy.stats import modeoutlet_size_mode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=(lambda x:mode(x.astype('str')).mode[0]))print ('Mode for each Outlet_Type:')print (outlet_size_mode)# 将Item_Weight的缺失值赋值给一个布尔变量missing_values = data['Outlet_Size'].isnull() # 输入数据，并检测转换前后的缺失值print ('\\nOrignal #missing: %d'% sum(missing_values))data.loc[missing_values,'Outlet_Size'] = data.loc[missing_values,'Outlet_Type'].apply(lambda x: outlet_size_mode[x])print ('Final #missing: %d'% sum(data['Outlet_Size'].isnull())) Mode for each Outlet_Type: Outlet_Type Grocery Store Supermarket Type1 Supermarket Type2 \\ Outlet_Size nan Small Medium Outlet_Type Supermarket Type3 Outlet_Size Medium Orignal #missing: 4016 Final #missing: 0 问题发现：可以发现Outlet_Size中有nan值存在，数据量有925条，观察数据，可选择将nan值暂定处理为Medium 12data['Outlet_Size'].replace('nan','Medium',inplace = True)data['Outlet_Size'].value_counts() Small 7071 Medium 5580 High 1553 Name: Outlet_Size, dtype: int64 数据整合12345# 处理Item_Fat_Content(产品是否低脂)数据存在别名的情况#data['Item_Fat_Content'].value_counts()data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat','low fat':'Low Fat','reg':'Regular'})data['Item_Fat_Content'].value_counts() Low Fat 9185 Regular 5019 Name: Item_Fat_Content, dtype: int64 1234567# 新特征生成# 观察数据，可根据唯一产品ID将产品分为3大种类data['Item_Type_Combined'] = data['Item_Identifier'].apply(lambda x:x[0:2])data['Item_Type_Combined'] = data['Item_Type_Combined'].map({'FD':'Food','NC':'Non-Consumable','DR':'Drink'})data['Item_Type_Combined'].value_counts() Food 10201 Non-Consumable 2686 Drink 1317 Name: Item_Type_Combined, dtype: int64 1234# 因为上一步根据唯一ID将产品分为三大类，而类别中有Non-Consumable(非消耗品)，可以考虑根据这个类别将是否低脂选项进行细分data.loc[data['Item_Type_Combined'] == 'Non-Consumable','Item_Fat_Content'] = 'Non-Edible'data['Item_Fat_Content'].value_counts() Low Fat 6499 Regular 5019 Non-Edible 2686 Name: Item_Fat_Content, dtype: int64 可视化商店面积12345# 商店面积类别柱状图#sns.countplot(x = &quot;Outlet_Size&quot;, data = train)sns.countplot(x = &quot;Outlet_Size&quot;, data = data) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a25636cf8&gt; 图表解析：可看出大部分都属于中小型商店，大型商店占比较小 商品是否低脂与产品重量123# 商品是否低脂与产品重量sns.barplot(x = &quot;Item_Fat_Content&quot;, y = &quot;Item_Weight&quot; , data = data) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a25b12da0&gt; 图表解析：可以看出商品是否低脂与产品重量没什么多大联系 商店ID与销售额1234# Outlet_Identifier 与 Item_Outlet_Salesfig, ax = plt.subplots(figsize=(12,10))sns.barplot(ax=ax,x = &quot;Outlet_Identifier&quot;, y = &quot;Item_Outlet_Sales&quot; , data = data) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a25ae9550&gt; 图表解析：OUT017与OUT019的销售额最少，经营情况很差；而OUT027的销售额最高，其余商店的销售额相差无几 商品种类与销售额1234567891011# 商品种类与销售额趋势图plt.figure(figsize=(16,16))requiredColumns = data.Item_Type.unique()counter = 1for col in requiredColumns: plt.subplot(4, 4, counter) sns.kdeplot(data[data['Item_Type'] == col]['Item_Outlet_Sales'], color='r', label=col, shade=True) plt.legend(loc='upper right') plt.title(col) counter = counter + 1 图表解析：明显可以看出，Seafood的销售金额相比其他会高出不少，而Fruits and Vegetables的销售金额曲线最平滑，金额也偏低。 商店ID与销售额1234567891011# Outlet_Identifier 与 销售额plt.figure(figsize=(16,16))requiredColumns1 = data.Outlet_Identifier.unique()counter = 1for col in requiredColumns1: plt.subplot(4, 4, counter) sns.kdeplot(data[data['Outlet_Identifier'] == col]['Item_Outlet_Sales'], color='purple', label=col, shade=True) plt.legend(loc='upper right') plt.title(col) counter = counter + 1 图表解析：与上面分析结果一致，销售额最差的是OUT017与OUT019，经营情况最差；而销售额最高的是OUT027，其余商店销售额曲线相差不大 商店年份与销售额12data.groupby('Outlet_Establishment_Year')['Item_Outlet_Sales'].mean().plot.bar()plt.xticks(rotation = 360) (array([0, 1, 2, 3, 4, 5, 6, 7, 8]), &lt;a list of 9 Text xticklabel objects&gt;) 图表解析：可以看出除1998年，其他商店年份的销售额都相差不大 商店年龄与销售额123456# 定义新特征 - 将年份转变成商店年龄data['Outlet_Years'] = 2019 - data['Outlet_Establishment_Year']data['Outlet_Years'].describe()data.groupby('Outlet_Years')['Item_Outlet_Sales'].mean().plot.bar()plt.xticks(rotation = 360) (array([0, 1, 2, 3, 4, 5, 6, 7, 8]), &lt;a list of 9 Text xticklabel objects&gt;) 12345678plt.figure(figsize = (12,6))ax = sns.boxplot(x = 'Outlet_Years', y = 'Item_Outlet_Sales', data = data)ax.set_xticklabels(ax.get_xticklabels(), rotation = 45)ax.set_title('Outlet years vs Item_Outlet_Sales')ax.set_xlabel('', fontsize = 15)ax.set_ylabel('Item_Outlet_Sales', fontsize = 15)plt.show() 图表解析：商店年龄在21年的商店销售额最少 商品类别、商品是否低脂与销售额1234# 原先考虑将超市Type2和Type3变量# 检查方式是按商店类型分析平均销售额，如果有相似的销售额，那么分成两个类别也没有多大帮助data.pivot_table(values='Item_Outlet_Sales',index='Outlet_Type') 图表解析：可以看出销售额存在差异，所以我们保留两个特征 12345678910111213141516171819# Item_Type_Combined 、Item_Fat_Content 箱型图plt.figure(figsize = (10,9))# 商品类别与销售额plt.subplot(211)plt.xticks(fontsize = 15)sns.boxplot(x = 'Item_Type_Combined',y = 'Item_Outlet_Sales',data = data,palette='Set1')# 商品是否低脂与销售额plt.subplot(212)plt.xticks(fontsize = 15)sns.boxplot(x = 'Item_Fat_Content',y = 'Item_Outlet_Sales',data = data,palette='Set1')plt.subplots_adjust(wspace = 0.2,hspace = 0.4,top = 1.5)plt.show() 图表解析：Drink类商品的销售额相比另外两种还是要差上不少，而商品是否低脂与销售额基本上关系不大，与上面的分析结果一致 商店ID与产品种类123456789101112131415161718192021# Outlet_Identifier 、Item_Type 箱型图plt.figure(figsize = (14,9))plt.subplot(211)ax = sns.boxplot(x = 'Outlet_Identifier',y = 'Item_Outlet_Sales',data=data,palette='hls')ax.set_title('Outlet_Identifier VS Item_Outlet_Sales',fontsize = 15)ax.get_xticklabels('Outlet_Identifier')ax.set_xlabel('Outlet_Identifier',fontsize = 12)ax.set_ylabel('Item_Outlet_Sales',fontsize = 12)plt.subplot(212)ax = sns.boxplot(x = 'Item_Type',y = 'Item_Outlet_Sales',data=data,palette='hls')ax.set_title('Item_Type VS Item_Outlet_Sales',fontsize = 15)ax.set_xlabel('Item_Type',fontsize = 12)ax.set_ylabel('Item_Outlet_Sales',fontsize = 12)plt.subplots_adjust(hspace=0.9,top=0.9)plt.setp(ax.get_xticklabels(),rotation = 45)plt.show() 数据类型转换123456789101112# 因为由于scikit-learn只接受数值变量，所以我将所有类别的名义变量类别转换为数值类型。 -- 进行标准化# 为了保留 Outlet_Identifier ，创建新的变量 Outlet 接收 Outlet_Identifier ，并进行编码 from sklearn.preprocessing import LabelEncoderle = LabelEncoder()data['Outlet'] = le.fit_transform(data['Outlet_Identifier'])cols = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet']for c in cols: data[c] = le.fit_transform(data[c])data.head() 相关性 - 热力图12345678910# 标准化后查看训练数据的各项相关性# 热力图trainData = data[:train.shape[0]]trainData.head()plt.figure(figsize=(10,10))sns.heatmap(trainData.iloc[:, 2:].corr(), annot=True, square=True, cmap='Greens')plt.show() 独热编码1234#One Hot Coding:data = pd.get_dummies(data, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type', 'Item_Type_Combined','Outlet']) 1data.dtypes Item_Identifier object Item_Weight float64 Item_Visibility float64 Item_Type object Item_MRP float64 Outlet_Identifier object Outlet_Establishment_Year int64 source object Item_Outlet_Sales float64 Outlet_Years int64 Item_Fat_Content_0 uint8 Item_Fat_Content_1 uint8 Item_Fat_Content_2 uint8 Outlet_Location_Type_0 uint8 Outlet_Location_Type_1 uint8 Outlet_Location_Type_2 uint8 Outlet_Size_0 uint8 Outlet_Size_1 uint8 Outlet_Size_2 uint8 Outlet_Type_0 uint8 Outlet_Type_1 uint8 Outlet_Type_2 uint8 Outlet_Type_3 uint8 Item_Type_Combined_0 uint8 Item_Type_Combined_1 uint8 Item_Type_Combined_2 uint8 Outlet_0 uint8 Outlet_1 uint8 Outlet_2 uint8 Outlet_3 uint8 Outlet_4 uint8 Outlet_5 uint8 Outlet_6 uint8 Outlet_7 uint8 Outlet_8 uint8 Outlet_9 uint8 dtype: object 数据保存123456789101112131415161718# 删除一些不必要项data.drop(['Item_Type','Outlet_Establishment_Year'],axis=1,inplace=True)# 区分训练集和测试集train = data.loc[data['source']==&quot;train&quot;]test = data.loc[data['source']==&quot;test&quot;]# 训练集与测试集删除一些非必要项test.drop(['Item_Outlet_Sales','source'],axis=1,inplace=True)train.drop(['source'],axis=1,inplace=True)# 数据保存train.to_csv(&quot;train_modified.csv&quot;,index=False)test.to_csv(&quot;test_modified.csv&quot;,index=False) 未完待续 总结以上是大型商店销售额预测项目的一些分析以及可视化，原项目是为做预测模型来预测各个商店的未来的经营情况，本次只做了一下商店数据上的分析以及可视化，算法部分有心无力，待学成再补充，有任何需要修正的地方，欢迎指正，感谢阅读～","link":"/2020/12/29/Bigmart%20Sale/"},{"title":"员工离职原因分析","text":"摘要利用员工离职数据从不同维度分析员工离职的种种原因，Hr的不时之需～ 项目描述项目名称：人力资源分析 - 离职原因 数据来源：数据来源于Kaggle中的人力资源分析项目 字段说明： left：是否离职 satisfaction_level：满意度 last_evaluation：绩效评估 number_project：完成项目数 average_montly_hours：平均每月工作时间 time_spend_company：为公司服务的年限 work_accident：是否有工作事故 promotion：过去5年是否有升职 department：部门 salary：薪资水平 项目目的：通过已知的员工离职与否情况，探究哪些因素对员工的离职产生了较大的影响 环境解释：基于jupyter notebook，可视化工具包：seaborn、matplotlib 12345678910111213141516# 模块导入import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as plt%matplotlib inlineimport warningswarnings.filterwarnings('ignore')import osos.chdir(r'/Users/nanb/Documents/数据存放')sns.set_style('darkgrid')sns.set_context('paper') 1234567#导入数据data = pd.read_csv('HR_comma_sep.csv',sep = ',')# 修改下列名，以便操作data.rename(columns={'promotion_last_5years':'promotion', 'sales':'department'},inplace=True) 12345# 查看数据data.info()# 检查空值# data.apply(lambda x:sum(x.isnull())) &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 14999 entries, 0 to 14998 Data columns (total 10 columns): satisfaction_level 14999 non-null float64 last_evaluation 14999 non-null float64 number_project 14999 non-null int64 average_montly_hours 14999 non-null int64 time_spend_company 14999 non-null int64 Work_accident 14999 non-null int64 left 14999 non-null int64 promotion 14999 non-null int64 department 14999 non-null object salary 14999 non-null object dtypes: float64(2), int64(6), object(2) memory usage: 1.1+ MB 部门离职情况12345678# 不同部门的离职分析# pd.crosstab ：一种特殊的pivot_table()，专用于计算分组频率depart_left_table = pd.crosstab(index = data['department'],columns = data['left'])depart_left_table.plot(kind = 'bar',figsize = (15,8), stacked = True, fontsize = 15)plt.xticks(rotation = 30) (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), &lt;a list of 10 Text xticklabel objects&gt;) 部门名称：sales（销售）、technical（技术）、support（支持）、IT、product_mng（产品经理）、marketing（市场营销）、RandD（研发）、accounting（会计）、hr、management（管理） 图表解析：除sales、technical、support离职率较高之外，其余部门的离职率大致相似；其中管理岗位离职率较低，可能是管理者处于公司地位较高，这类型不倾向于离开 部门与薪资123456# 部门与薪资depart_salary = pd.crosstab(index = data['department'],columns = data['salary'])depart_salary.plot(kind = 'bar',figsize = (15,8), fontsize = 15)plt.xticks(rotation = 30) (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), &lt;a list of 10 Text xticklabel objects&gt;) 图表解析：销售sales的工资基本处于低中水平，可能是导致离职率较高的因素；而管理层的薪资基本都在中高水平，离职率也最低 薪资分析123456# 薪资与是否离职salary_left = pd.crosstab(index = data['salary'],columns = data['left'])salary_left.plot(kind = 'bar',figsize = (15,8),stacked = True, fontsize = 15)plt.xticks(rotation = 30) (array([0, 1, 2]), &lt;a list of 3 Text xticklabel objects&gt;) 图表解析：很直观的看出，离职的员工大多数薪资都在低到中等的水平，而很少有高薪资的员工离开公司 升职与否1234567# 过去5年是否有升职与离职率promotion_left = pd.crosstab(index = data['promotion'],columns = data['left'])promotion_left.plot(kind = 'bar',figsize = (15,8), fontsize = 15)plt.xticks(rotation = 360) (array([0, 1]), &lt;a list of 2 Text xticklabel objects&gt;) 图表解析：明显得知，离职员工中几乎都没有得到过升职，而得到升职的员工基本都没有离开 项目数量分析12345678# 项目数量与离职率number_prj_left = pd.crosstab(index = data['number_project'], columns = data['left'])number_prj_left.plot(kind = 'bar',figsize = (15,8),stacked = True, fontsize = 15)plt.xticks(rotation = 360) (array([0, 1, 2, 3, 4, 5]), &lt;a list of 6 Text xticklabel objects&gt;) 图表解析：超过一半的员工只有2个项目就离开了公司；同样有从4-7个项目统计的员工离职，并且随着项目的增多，员工离职率逐渐增高；3个项目的员工离职率最低 猜测：可能项目数量在2或者更少的员工工作力度不够，或者没有被高度重视，而离职；6个及6个以上项目的员工可能因过度劳累，而离职 服务年限分析123456# 为公司服务年限与离职率time_spend_compy_left = pd.crosstab(index = data['time_spend_company'],columns = data['left'])time_spend_compy_left.plot(kind = 'bar',figsize = (15,8), stacked = True,fontsize = 15)plt.xticks(rotation = 360) (array([0, 1, 2, 3, 4, 5, 6, 7]), &lt;a list of 8 Text xticklabel objects&gt;) 图表解析：离职员工在离职前大多数在公司已经工作3-5年；在公司工作7年及以上几乎没有人离职；在公司工作2年的员工基本上也不会选择离职 工作时长分析1234567# 探究平均每个月工作时长与离职率#hours_left = pd.crosstab(index = data['average_montly_hours'],columns = data['left'])plt.figure(figsize = (15,8))sns.kdeplot(data.loc[(data['left'] == 0),'average_montly_hours'],color = 'b',shade = True,label = 'no left')sns.kdeplot(data.loc[(data['left'] == 1),'average_montly_hours'],color = 'r',shade = True,label = 'left') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a23732d68&gt; 图表解析：就离职部分员工来说，很明显的双峰分布，说明员工每月平均工作时长低于150小时和工作时长高与250小时的员工离职率最高。 简言之，一般离开公司的员工要么工作时间少，要么就是过度工作 绩效评估分析1234567# 绩效评估与离职率# evaluation_left = pd.crosstab(index = data['last_evaluation'],columns = data['left'])plt.figure(figsize = (15,8))sns.kdeplot(data.loc[(data['left'] == 0),'last_evaluation'],color = 'b',shade = True,label = 'no left')sns.kdeplot(data.loc[(data['left'] == 1),'last_evaluation'],color = 'r',shade = True,label = 'left') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2393ecf8&gt; 图表解析 ：也是一个双峰分布：表现糟糕的和表现优秀的员工出现了离职的两个峰值。有可能是绩效评估出色的员工，公司没有相应的转换到升职和加薪上，导致了表现优秀的员工离职率变高；而绩效评估在0.6~0.8之间员工有较高的留存率 满意度分析123456# 对公司的满意度与离职率plt.figure(figsize = (15,8))sns.kdeplot(data.loc[(data['left'] == 0),'satisfaction_level'],color = 'b',shade = True,label = 'no left')sns.kdeplot(data.loc[(data['left'] == 1),'satisfaction_level'],color = 'r',shade = True,label = 'left') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2393e0b8&gt; 图表解析： 图中出现了三个峰值：满意度低于0.1的员工基本离职；满意度在0.3-0.5之间离职的员工又达到一个峰值；而在满意度为0.8左右时，离职情况又出现了一个峰值。 猜测：满意度较高的员工出现离职的情况，有可能是有更好的工作机会，而不一定是对公司不满所引发的离开 满意度与绩效评估12345# 对公司的满意度与绩效评估之间的探究df = data[data['left'] == 1]pd.plotting.scatter_matrix(df[['satisfaction_level','last_evaluation']], color = 'g',figsize = (12,8)) array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a23d02b00&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a21b92cc0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a23d6c668&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a23f25fd0&gt;]], dtype=object) 从绩效评估和满意度的散射矩阵中，大致分为三个不同的集群 1、集群1：满意度低于0.2，绩效评估大于0.75，这很直观的表明离开公司的员工在工作上受到高度评价，但是对自己的工作并不满意；所以该集群可能代表着’过度劳累’部分的员工 2、集群2：满意度在0.35~0.45之间，绩效评估在0.58以下；这部分员工在工作上受到不太好的评价，可能代表着员工本身表现不大好，而且员工对公司的满意度也不好 3、集群3：满意度在0.7~1之间，绩效评估大于0.8；这部分员工不仅在工作上受到高度评价，而且对公司的满意度也高；假如这个类别的员工离职，可能是有了更好的工作机会，包括更高的薪资、更好的发展机会或者更大的晋升空间等 特征处理123456789101112131415# MinMaxScaler：归一化；StandardScaler：标准化from sklearn.preprocessing import MinMaxScaler,StandardScaler# 编码from sklearn.preprocessing import LabelEncoder,OneHotEncoder# 正规化from sklearn.preprocessing import Normalizer# LDA降维from sklearn.discriminant_analysis import LinearDiscriminantAnalysis# PCA降维from sklearn.decomposition import PCA 123456789101112131415161718192021# 函数说明'''sl:satisfaction_level --- False:MinMaxScaler;True:StandardScalerle:last_evaluation --- False:MinMaxScaler;True:StandardScalernpr:number_project --- False:MinMaxScaler;True:StandardScaleramh:average_montly_hours --- False:MinMaxScaler;True:StandardScalertsc:time_spend_company --- False:MinMaxScaler;True:StandardScalerwa:work_accident --- False:MinMaxScaler;True:StandardScalerpl5:promotion --- False:MinMaxScaler;True:StandardScalerdp:department --- False:LabelEncoding;True:OneHotEncodingslr:salary --- False:LabelEncoding;True:OneHotEncoding''' 12345678910111213141516171819202122232425262728293031323334353637383940414243def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=False,slr=False,lower_d=False,ld_n=1): #1、得到标注 global data label = df['left'] data = data.drop('left',axis = 1) #2、数据清洗 #3、特征选择 #4、特征处理 scaler_lst = [sl,le,npr,amh,tsc,wa,pl5] column_lst = ['satisfaction_level','last_evaluation',\\ 'number_project','average_montly_hours',\\ 'time_spend_company','Work_accident','promotion'] for i in range(len(scaler_lst)): if not scaler_lst[i]: data[column_lst[i]] = \\ MinMaxScaler().fit_transform(data[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0] else: data[column_lst[i]] = \\ StandardScaler().fit_transform(data[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0] scaler_lst = [slr,dp] column_lst = ['salary','department'] for i in range(len(scaler_lst)): if not scaler_lst[i]: if column_lst[i] == 'salary': data[column_lst[i]] = [map_salary(s) for s in data['salary'].values] else: data[column_lst[i]] = LabelEncoder().fit_transform(data[column_lst[i]]) data[column_lst[i]] = MinMaxScaler().fit_transform(data[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0] else: data = pd.get_dummies(data,columns=[column_lst[i]]) if lower_d: return PCA(n_components=ld_n).fit_transform(data.values),label return data,labelsd = dict([('low',0),('medium',1),('high',2)])def map_salary(s): return d.get(s,0)def main(): print(hr_preprocessing(lower_d=True,ld_n=3)) main() (array([[-0.14989291, -0.4220161 , -0.42173539], [-0.14643287, 0.41677176, 0.1984778 ], [-0.26709532, 0.68671233, 0.18811401], ..., [-0.14803561, -0.43441955, -0.503113 ], [-0.29885501, 0.76297807, -0.24200472], [-0.15087015, -0.41003776, -0.49717793]]), 0 1 1 1 2 1 3 1 .. 14996 1 14997 1 14998 1 Name: left, Length: 3571, dtype: int64) 未完待续～ 写在最后以上是基于HR数据进行的员工离职原因案例的一部分分析，剩余预测模型的构建，也就是算法部分，目前对算法的了解尚浅，能力不足以处理，待学习后进行补充。对以上分析有疑义，欢迎指正，感谢阅读～","link":"/2020/11/29/HR_Analytics/"},{"title":"用户消费有哪些特征？用户质量又如何界定？","text":"摘要借由在线音乐零售平台CDNow的部分用户消费数据，分析用户的消费特征、用户的消费质量等～ 项目描述项目名称：CDNow在线音乐零售平台用户消费行为分析 项目背景：CDNow是一家网络公司，经营着一个在线购物网站，主要销售光盘和音乐相关产品。2000年7月，被Bertelsmann Music Group 以1.17亿美元收购；而后不久，由亚马逊承包经营。本文主要通过分析CDNow网站的用户购买明细数据来分析该网站的用户消费行为，使运营部门在营销时更具有针对性，从而节省成本，提高效率。 字段说明 用户ID：用户身份唯一标识(数据集中一个用户ID有多条消费记录) 购买日期：即消费日期 订单数：购买数量 订单金额：总消费金额 项目目的： 用户消费特征 用户整体消费分析 用户个人消费分析 用户消费周期 用户分层 RFM模型 用户活跃度分层 用户质量分析 消费频次 复购率 回购率 留存率 数据处理2.1 数据加载123456import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as ssimport warnings%matplotlib inline 1234import osos.chdir(&quot;/home/min/data&quot;)warnings.filterwarnings(&quot;ignore&quot;) 12345# 设置列名colums = ['用户ID','购买日期','订单数','订单金额']data = pd.read_csv(&quot;CDNOW.txt&quot;,names = colums,sep=&quot;\\s+&quot;)data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 用户ID 购买日期 订单数 订单金额 0 1 19970101 1 11.77 1 2 19970112 1 12.00 2 2 19970112 5 77.00 3 3 19970102 2 20.76 4 3 19970330 2 20.76 2.2 数据查看1data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 用户ID 购买日期 订单数 订单金额 count 69659.000000 6.965900e+04 69659.000000 69659.000000 mean 11470.854592 1.997228e+07 2.410040 35.893648 std 6819.904848 3.837735e+03 2.333924 36.281942 min 1.000000 1.997010e+07 1.000000 0.000000 25% 5506.000000 1.997022e+07 1.000000 14.490000 50% 11410.000000 1.997042e+07 2.000000 25.980000 75% 17273.000000 1.997111e+07 3.000000 43.700000 max 23570.000000 1.998063e+07 99.000000 1286.010000 1data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 69659 entries, 0 to 69658 Data columns (total 4 columns): 用户ID 69659 non-null int64 购买日期 69659 non-null int64 订单数 69659 non-null int64 订单金额 69659 non-null float64 dtypes: float64(1), int64(3) memory usage: 2.1 MB 2.3 数据类型转换 &amp; 添加字段1234# 转换日期类型data['购买日期'] = pd.to_datetime(data['购买日期'],format='%Y%m%d')data['月份'] = data['购买日期'].values.astype('datetime64[M]') 1data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 69659 entries, 0 to 69658 Data columns (total 5 columns): 用户ID 69659 non-null int64 购买日期 69659 non-null datetime64[ns] 订单数 69659 non-null int64 订单金额 69659 non-null float64 月份 69659 non-null datetime64[ns] dtypes: datetime64[ns](2), float64(1), int64(2) memory usage: 2.7 MB 1data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 用户ID 购买日期 订单数 订单金额 月份 0 1 1997-01-01 1 11.77 1997-01-01 1 2 1997-01-12 1 12.00 1997-01-01 2 2 1997-01-12 5 77.00 1997-01-01 3 3 1997-01-02 2 20.76 1997-01-01 4 3 1997-03-30 2 20.76 1997-03-01 123# 检查空值data.apply(lambda x:sum(x.isnull())) 用户ID 0 购买日期 0 订单数 0 订单金额 0 月份 0 dtype: int64 用户消费分析3.1 用户总体消费趋势123456789101112131415161718192021222324252627282930313233# 按照月份初步查看用户总体消费趋势分析# 风格选择plt.style.use('ggplot')plt.figure(figsize = (20,12))# 每月的中销售额plt.subplot(221)data.groupby('月份')['订单金额'].sum().plot(fontsize = 24)plt.title('总销售额',fontsize = 24)# 每月的消费次数plt.subplot(222)data.groupby('月份')['购买日期'].count().plot(fontsize = 24)plt.title('消费次数',fontsize = 24)# 每月的销量plt.subplot(223)data.groupby('月份')['订单数'].sum().plot(fontsize = 24)plt.title('总销量',fontsize = 24)# 每月的消费人数plt.subplot(224)data.groupby('月份')['用户ID'].apply(lambda x:len(x.unique())).plot(fontsize = 24)plt.title('消费人数',fontsize = 24)plt.tight_layout()plt.show() 图表解析：四个折线图趋势基本一致，呈现为前3个月销量极高，销售额暴涨，往后骤然下降，最后趋于平稳。 消费金额在前三个月达到最高峰，后期消费金额较为平稳，呈小幅度下降趋势 前三个月消费次数在10000左右，往后月份消费次数基本维持在2500 产品购买量趋势呈现为早起购买量多 后期小幅度下降趋势 每月消费人数小于每月的消费次数（订单数），但是区别不大，前三个月每月的消费人数在8000-10000之间，后续月份平均2000左右，一样是前期消费人数多，后期平稳下降趋势 出现这种状况，假设问题是出现在用户身上，早期时间段的用户有异常值，或者由于各类促销营销，由于只有消费数据，无法进一步进行判断 1234567# 数据透视查看 -- 按照月份查看用户购买金额、订单数、用户人数data.pivot_table(index='月份', values=['订单数','订单金额','用户ID'], aggfunc={'订单数':'sum', '订单金额':'sum', '用户ID':'count'}) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 用户ID 订单数 订单金额 月份 1997-01-01 8928 19416 299060.17 1997-02-01 11272 24921 379590.03 1997-03-01 11598 26159 393155.27 1997-04-01 3781 9729 142824.49 1997-05-01 2895 7275 107933.30 1997-06-01 3054 7301 108395.87 1997-07-01 2942 8131 122078.88 1997-08-01 2320 5851 88367.69 1997-09-01 2296 5729 81948.80 1997-10-01 2562 6203 89780.77 1997-11-01 2750 7812 115448.64 1997-12-01 2504 6418 95577.35 1998-01-01 2032 5278 76756.78 1998-02-01 2026 5340 77096.96 1998-03-01 2793 7431 108970.15 1998-04-01 1878 4697 66231.52 1998-05-01 1985 4903 70989.66 1998-06-01 2043 5287 76109.30 3.2 用户个体消费分析3.2.1 用户消费金额 &amp; 消费次数1234567# 用户个体消费# 根据用户ID进行分组# 1、用户消费金额 &amp; 消费次数group_user = data.groupby('用户ID').sum()group_user.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 订单数 订单金额 count 23570.000000 23570.000000 mean 7.122656 106.080426 std 16.983531 240.925195 min 1.000000 0.000000 25% 1.000000 19.970000 50% 3.000000 43.395000 75% 7.000000 106.475000 max 1033.000000 13990.930000 图表解析： 用户平均购买7张CD，而标准差为17，说明波动较大；中位值只有3，更是说明了存在小部分用户购买了大数量的CD 用户平均消费金额106，中位值是43，也说明了存在极值干扰 3.2.2 用户消费金额 &amp; 购买数量(散点图)123# 2、散点图 -- 设置去除某些极值group_user.query('订单金额 &lt; 4000').plot.scatter(x = '订单金额',y = '订单数',figsize = (20,10)) 图表解析：由于数据源是CD网站的销售数据，商品种类单一，金额和商品量呈线性，并不存在大量离群点。 123# 每笔订单消费金额data.plot.scatter(x = '订单金额',y = '订单数',figsize = (20,10)) 图表解析：从订单角度分析，订单消费金额多数在0～400，而每笔订单购买数量大多在0～40，而且订单购买数量极值极少，基本与上方用户消费角度分析趋势一致。 3.2.3 消费金额分布123# 3、用户消费金额的分布图group_user['订单金额'].plot.hist(bins = 20,figsize = (16,10)) 图表解析：用户大部分消费能力不高，基本上绝大部分用户处于很低的消费档次，图中基本看不到高消费用户。 3.2.4 用户消费次数123# 4、用户消费次数分布 -- 去除某些极值group_user.query('订单数 &lt; 92')['订单数'].hist(bins = 40,figsize = (16,10)) 图表解析：排除部分极值干扰，可看出大部分用户购买CD的数量都在3张以内，购买大数量的用户属极少数。 3.2.5 用户累计消费金额占比1234# 用户累计消费金额占比user_cumsum = group_user.sort_values('订单金额').apply(lambda x:x.cumsum() / x.sum())user_cumsum.reset_index()['订单金额'].plot(figsize = (16,10)) 图表解析：50%的用户仅贡献了15%的消费额，而排名前5000的用户就贡献了60%的消费金额。 123# user_cumsum.head(10)# user_cumsum.tail(10) 用户消费周期3.3.1 用户首购时间1234# 1、第一次购买时间orderdt_min = data.groupby('用户ID').min()['购买日期']orderdt_min.value_counts().plot(figsize = (16,10)) 图表解析：用户第一次购买基本集中在前三个月，而在2月份中下旬有一次较大的波动 3.3.2 用户最后一次购买时间1234# 最后一次购买时间orderdt_max = data.groupby('用户ID').max()['购买日期']orderdt_max.value_counts().plot(figsize = (16,10)) 图表解析： 用户最后一次购买的分布比第一次购买分布要广 大部分最后一次购买在前三个月，说明很多用户购买一次之后就不再消费 往后月份，最后一次购买数呈小幅度递增，有上升趋势 用户分层RFM模型分层4.1 构建RFM模型123456rfm = data.pivot_table(index = '用户ID', values=['订单数','订单金额','购买日期'], aggfunc={'购买日期':'max', '订单金额':'sum', '订单数':'sum'})rfm.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 订单数 订单金额 购买日期 用户ID 1 1 11.77 1997-01-01 2 6 89.00 1997-01-12 3 16 156.46 1998-05-28 4 7 100.50 1997-12-12 5 29 385.61 1998-01-03 1-(rfm['购买日期'] - rfm['购买日期'].max()).head() 用户ID 1 545 days 2 534 days 3 33 days 4 200 days 5 178 days Name: 购买日期, dtype: timedelta64[ns] 12345# 重命名 &amp; 去除单位rfm['R'] = -(rfm['购买日期'] - rfm['购买日期'].max()) / np.timedelta64(1,'D')rfm.rename(columns={'订单数':'F','订单金额':'M'},inplace=True)rfm.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } F M 购买日期 R 用户ID 1 1 11.77 1997-01-01 545.0 2 6 89.00 1997-01-12 534.0 3 16 156.46 1998-05-28 33.0 4 7 100.50 1997-12-12 200.0 5 29 385.61 1998-01-03 178.0 1rfm[['R','F','M']].apply(lambda x:x-x.mean()).head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } R F M 用户ID 1 177.778362 -6.122656 -94.310426 2 166.778362 -1.122656 -17.080426 3 -334.221638 8.877344 50.379574 4 -167.221638 -0.122656 -5.580426 5 -189.221638 21.877344 279.529574 123456789101112131415161718def rfm_func(x): level = x.apply(lambda x:'1' if x&gt;=0 else '0') label = level.R + level.F + level.M d = { '111':'重要价值客户', '011':'重要保持客户', '101':'重要挽留客户', '001':'重要发展客户', '110':'一般价值客户', '010':'一般保持客户', '100':'一般挽留客户', '000':'一般发展客户', } result = d[label] return resultrfm['label'] = rfm[['R','F','M']].apply(lambda x:x-x.mean()).apply(rfm_func,axis = 1)rfm.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } F M 购买日期 R label 用户ID 1 1 11.77 1997-01-01 545.0 一般挽留客户 2 6 89.00 1997-01-12 534.0 一般挽留客户 3 16 156.46 1998-05-28 33.0 重要保持客户 4 7 100.50 1997-12-12 200.0 一般发展客户 5 29 385.61 1998-01-03 178.0 重要保持客户 1rfm.groupby('label').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } F M R label 一般价值客户 650 7181.28 36295.0 一般保持客户 1712 19937.45 29448.0 一般发展客户 13977 196971.23 591108.0 一般挽留客户 29346 438291.81 6951815.0 重要价值客户 11121 167080.83 358363.0 重要保持客户 107789 1592039.62 517267.0 重要发展客户 2023 45785.01 56636.0 重要挽留客户 1263 33028.40 114482.0 1rfm.groupby('label').count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } F M 购买日期 R label 一般价值客户 77 77 77 77 一般保持客户 206 206 206 206 一般发展客户 3300 3300 3300 3300 一般挽留客户 14074 14074 14074 14074 重要价值客户 787 787 787 787 重要保持客户 4554 4554 4554 4554 重要发展客户 331 331 331 331 重要挽留客户 241 241 241 241 12345678910111213# 各类型用户占比user_t = rfm.groupby('label').count()plt.axis('equal')labels = ['一般价值客户','一般保持客户','一般发展客户','一般挽留客户','重要价值客户','重要保持客户','重要发展客户','重要挽留客户']plt.pie(user_t['M'], autopct='%3.1f%%', labels=labels, pctdistance=0.9, labeldistance=1.2, radius = 3, startangle=15)plt.show() 12345678910plt.figure(figsize=(16,10))for label,grouped in rfm.groupby('label'): x = grouped['F'] y = grouped['R'] plt.scatter(x,y,label = label)plt.legend(loc = 'best')plt.xlabel('Frequency')plt.ylabel('Recency')plt.show() 图表解析：可以看出，大部分用户为重要保持客户，原因可能是存在极值，也说明了RFM模型的划分应该依照业务的实际情况来进行划分层级。 活跃度分层4.2 按照活跃度分层用户123456789# 通过每月是否消费来划分用户pivoted_counts = data.pivot_table(index='用户ID', columns='月份', values='购买日期', aggfunc='count').fillna(0)pivoted_counts.columns = data['月份'].sort_values().astype('str').unique()pivoted_counts.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1997-01-01 1997-02-01 1997-03-01 1997-04-01 1997-05-01 1997-06-01 1997-07-01 1997-08-01 1997-09-01 1997-10-01 1997-11-01 1997-12-01 1998-01-01 1998-02-01 1998-03-01 1998-04-01 1998-05-01 1998-06-01 用户ID 1 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3 1.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 4 2.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 5 2.0 1.0 0.0 1.0 1.0 1.0 1.0 0.0 1.0 0.0 0.0 2.0 1.0 0.0 0.0 0.0 0.0 0.0 12data_purchase = pivoted_counts.applymap(lambda x:1 if x&gt;0 else 0)data_purchase.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1997-01-01 1997-02-01 1997-03-01 1997-04-01 1997-05-01 1997-06-01 1997-07-01 1997-08-01 1997-09-01 1997-10-01 1997-11-01 1997-12-01 1998-01-01 1998-02-01 1998-03-01 1998-04-01 1998-05-01 1998-06-01 用户ID 23566 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 23567 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 23568 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 23569 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 23570 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 12345678910111213141516171819202122def active_status(data): status = [] for i in range(18): if data[i] == 0: if len(status) &gt; 0: if status[i-1] == 'unreg': status.append('unreg') else: status.append('unactive') else: status.append('unreg') else: if len(status) == 0: status.append('new') else: if status[i-1] == 'unactive': status.append('return') elif status[i-1] == 'unreg': status.append('new') else: status.append('active') return pd.Series(status,data_purchase.columns) 12purchase_states = data_purchase.apply(active_status,axis = 1)purchase_states.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1997-01-01 1997-02-01 1997-03-01 1997-04-01 1997-05-01 1997-06-01 1997-07-01 1997-08-01 1997-09-01 1997-10-01 1997-11-01 1997-12-01 1998-01-01 1998-02-01 1998-03-01 1998-04-01 1998-05-01 1998-06-01 用户ID 23566 unreg unreg new unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive 23567 unreg unreg new unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive 23568 unreg unreg new active unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive 23569 unreg unreg new unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive 23570 unreg unreg new unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive 12345678# 去除未注册的用户 -- 设置为空值purchase_states_m = purchase_states.replace('unreg',np.NaN).apply(lambda x:pd.value_counts(x))# 转置purchase_states_m = purchase_states_m.fillna(0).Tpurchase_states_m .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } active new return unactive 1997-01-01 0.0 7846.0 0.0 0.0 1997-02-01 1157.0 8476.0 0.0 6689.0 1997-03-01 1681.0 7248.0 595.0 14046.0 1997-04-01 1773.0 0.0 1049.0 20748.0 1997-05-01 852.0 0.0 1362.0 21356.0 1997-06-01 747.0 0.0 1592.0 21231.0 1997-07-01 746.0 0.0 1434.0 21390.0 1997-08-01 604.0 0.0 1168.0 21798.0 1997-09-01 528.0 0.0 1211.0 21831.0 1997-10-01 532.0 0.0 1307.0 21731.0 1997-11-01 624.0 0.0 1404.0 21542.0 1997-12-01 632.0 0.0 1232.0 21706.0 1998-01-01 512.0 0.0 1025.0 22033.0 1998-02-01 472.0 0.0 1079.0 22019.0 1998-03-01 571.0 0.0 1489.0 21510.0 1998-04-01 518.0 0.0 919.0 22133.0 1998-05-01 459.0 0.0 1029.0 22082.0 1998-06-01 446.0 0.0 1060.0 22064.0 12purchase_states_m.plot.area(figsize = (16,8))plt.show() 图表解析： 蓝色和灰色区域占了绝大部分面积，推断为某段时间消费过的用户的后续行为 红色部分代表的活跃用户较为稳定，属于核心用户 紫色部分代表的回流用户也表现的较为稳定 4.3 回流用户占比 回流用户比：某个时间段内回流用户在总用户中的占比 回流用户率：相比上月有多少不活跃用户在本月进行了消费 活跃用户比：某个时间段内活跃用户在总用户中的占比 123456plt.figure(figsize=(20,6))rate = purchase_states_m.apply(lambda x: x/x.sum())plt.plot(rate['return'],label = '回流用户')plt.plot(rate['active'],label = '活跃用户')plt.legend()plt.show() 图表解析： 用户每月回流用户比在5%~8%之间，且有小幅度下降趋势，说明客户有流失可能 数据源中不活跃用户量基本保持不变，所以回流率近似于回流比 活跃用户的占比基本稳定在3%~5%之间，下降趋势更明显 结合活跃用户和回流用户，在后期的消费用户中，近60%是回流用户，40%为活跃用户，说明整体用户质量相对不错。 用户质量分析用户购买周期12order_diff = data.groupby('用户ID').apply(lambda x: x['购买日期'] - x['购买日期'].shift())order_diff.head(10) 用户ID 1 0 NaT 2 1 NaT 2 0 days 3 3 NaT 4 87 days 5 3 days 6 227 days 7 10 days 8 184 days 4 9 NaT Name: 购买日期, dtype: timedelta64[ns] 1order_diff.describe() count 46089 mean 68 days 23:22:13.567662 std 91 days 00:47:33.924168 min 0 days 00:00:00 25% 10 days 00:00:00 50% 31 days 00:00:00 75% 89 days 00:00:00 max 533 days 00:00:00 Name: 购买日期, dtype: object 1234# 订单周期分布(order_diff / np.timedelta64(1,'D')).hist(bins = 20,figsize = (16,8))plt.show() 图表解析： 订单周期呈指数分布 用户的平均购买周期是68天，而且大多数用户的购买周期低于100天 从图像看属于典型的长尾图，说明绝大部分用户的消费间隔较短。可以考虑将时间召回点设为消费后立即赠送优惠券，且在消费后10天左右对客户进行回访，消费后30天后提醒优惠券即将到期，消费后60天短信推送进行尝试召回。 用户生命周期12user_life = data.groupby('用户ID')['购买日期'].agg(['min','max'])user_life.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } min max 用户ID 1 1997-01-01 1997-01-01 2 1997-01-12 1997-01-12 3 1997-01-02 1998-05-28 4 1997-01-01 1997-12-12 5 1997-01-01 1998-01-03 用户消费占比5.1 仅消费一次用户占比12345678# 只消费过一次的用户占比user_once = (user_life['min'] == user_life['max']).value_counts()labels = ['只消费一次用户','多次消费用户']plt.axis('equal')plt.pie(user_once,explode=(0,0.15),labels=labels,autopct='%2.1f%%',startangle=90, colors=('g','blue'),radius=1.5)plt.show() 123# 具体描述(user_life['max'] - user_life['min']).describe() count 23570 mean 134 days 20:55:36.987696 std 180 days 13:46:43.039788 min 0 days 00:00:00 25% 0 days 00:00:00 50% 0 days 00:00:00 75% 294 days 00:00:00 max 544 days 00:00:00 dtype: object 图表解析： 仅消费一次的用户占比为51.1%，进行多次消费用户占比48.9%. 而通过描述可知，用户的平均生命周期为134天，但中位数为0，说明大部分用户第一次消费也是最后一次，所以平均生命周期并不具有准确性。 用户生命周期最大值为544天，几乎等同于数据源总天数，代表这部分用户属于忠实拥护者，也是核心用户。 5.2 消费多次用户周期分布1234567891011121314151617181920# 多次消费用户的生命周期分布plt.figure(figsize=(20,8))plt.subplot(121)((user_life['max'] - user_life['min']) / np.timedelta64(1,'D')).hist(bins = 15)plt.title('二次消费以上用户的生命周期直方图')plt.xlabel('天数')plt.ylabel('用户数')# 过滤生命周期为0的用户plt.subplot(122)user_twice = ((user_life['max'] - user_life['min']).reset_index()[0] / np.timedelta64(1,'D'))user_twice[user_twice &gt; 0].hist(bins = 30)plt.title('二次消费以上用户的生命周期直方图')plt.xlabel('天数')plt.ylabel('用户数')plt.show() 图表解析：通过对比可知，去除周期为0的用户后，图像呈双峰结构。 右图为过滤后的生命周期分布图，但仍然存在生命周期趋于0天的用户，说明部分低质量用户，虽然消费过两次，但并不能持续消费。需要在消费后短时间内进行再次消费引导。 少部分用户集中在50~300天之间，忠诚度一般。 另一个高峰出现在400天之后，这部分用户属于高质量用户，忠诚度较高，而且后期用户数还在增加，需要维护这批用户的利益，长期保持回访、让利。 123# 消费两次以上的用户的平均生命周期user_twice[user_twice &gt; 0].mean() 276.0448072247308 图表解析：消费两次以上的用户平均生命周期为276天，远高于总体平均生命周期。所以在用户首次消费后进行引导及召回可以很有效提高用户的生命周期。 复购率 &amp; 回购率分析6.1 复购率1pivoted_counts.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1997-01-01 1997-02-01 1997-03-01 1997-04-01 1997-05-01 1997-06-01 1997-07-01 1997-08-01 1997-09-01 1997-10-01 1997-11-01 1997-12-01 1998-01-01 1998-02-01 1998-03-01 1998-04-01 1998-05-01 1998-06-01 用户ID 1 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3 1.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 4 2.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 5 2.0 1.0 0.0 1.0 1.0 1.0 1.0 0.0 1.0 0.0 0.0 2.0 1.0 0.0 0.0 0.0 0.0 0.0 12345# 复购率 -- 自然月内，购买多次的用户占比# 消费两次及以上为1，消费一次为0，没有消费为空purchare_r = pivoted_counts.applymap(lambda x: 1 if x &gt; 1 else np.NaN if x == 0 else 0)purchare_r.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1997-01-01 1997-02-01 1997-03-01 1997-04-01 1997-05-01 1997-06-01 1997-07-01 1997-08-01 1997-09-01 1997-10-01 1997-11-01 1997-12-01 1998-01-01 1998-02-01 1998-03-01 1998-04-01 1998-05-01 1998-06-01 用户ID 1 0.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 1.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 0.0 NaN 0.0 0.0 NaN NaN NaN NaN NaN NaN 1.0 NaN NaN NaN NaN NaN 0.0 NaN 4 1.0 NaN NaN NaN NaN NaN NaN 0.0 NaN NaN NaN 0.0 NaN NaN NaN NaN NaN NaN 5 1.0 0.0 NaN 0.0 0.0 0.0 0.0 NaN 0.0 NaN NaN 1.0 0.0 NaN NaN NaN NaN NaN 1234# 复购率折线图(purchare_r.sum() / purchare_r.count()).plot(figsize = (16,8))plt.show() 图表解析：复购率稳定在20%~22%，而前三个月复购率低，可能是有大量新用户所导致。 6.2 回购率 回购率 – 曾经购买的用户在某一时期内的再次购买占比 123456789101112131415161718192021222324def purchare_back(data): status = [] for i in range(17): # 本月进行过消费 if data[i] == 1: # 下个月是否进行了消费 if data[i+1] == 1: # 消费为1 -- 回购了 status.append(1) if data[i+1] == 0: # 消费为0 -- 没有回购 status.append(0) else: # 之前没消费则不进行计算 status.append(np.NaN) # 主要对最后一个月进行判断处理 status.append(np.NaN) return pd.Series(status,data_purchase.columns) 12purchare_b = data_purchase.apply(purchare_back,axis = 1)purchare_b.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1997-01-01 1997-02-01 1997-03-01 1997-04-01 1997-05-01 1997-06-01 1997-07-01 1997-08-01 1997-09-01 1997-10-01 1997-11-01 1997-12-01 1998-01-01 1998-02-01 1998-03-01 1998-04-01 1998-05-01 1998-06-01 用户ID 1 0.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 0.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 0.0 NaN 1.0 0.0 NaN NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN NaN 0.0 NaN 4 0.0 NaN NaN NaN NaN NaN NaN 0.0 NaN NaN NaN 0.0 NaN NaN NaN NaN NaN NaN 5 1.0 0.0 NaN 1.0 1.0 1.0 0.0 NaN 0.0 NaN NaN 1.0 0.0 NaN NaN NaN NaN NaN 12345678910111213141516# 折线图plt.figure(figsize = (20,8))plt.subplot(211)(purchare_b.sum() / purchare_b.count()).plot()plt.title('用户回购率分析')plt.ylabel('百分比%')plt.subplot(212)plt.plot(purchare_b.sum(),label = '每月消费人数')plt.plot(purchare_b.count(),label = '每月回购人数')plt.xlabel('月份')plt.ylabel('用户数')plt.legend()plt.show() 图表分析： 回购率基本稳定在25%～35%，较高于复购率，但波动性较大。新用户回购率在15%左右，与老用户相差不大 从消费人数分布图可以发现，回购人数在前三个月后趋于稳定，所以波动的原因有可能是营销淡旺季所导致 结合回购率复购率，可以看出新用户的整体忠诚度低于老用户，而老用户回购率较高，消费频次较低。 留存率分析12user_purchare = data[['用户ID','订单数','订单金额','购买日期']]user_purchare.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 用户ID 订单数 订单金额 购买日期 0 1 1 11.77 1997-01-01 1 2 1 12.00 1997-01-12 2 2 5 77.00 1997-01-12 3 3 2 20.76 1997-01-02 4 3 2 20.76 1997-03-30 1234567user_purchare_retention = pd.merge(left = user_purchare, right = user_life['min'].reset_index(), how = 'inner', on = '用户ID')user_purchare_retention['order_dt_diff'] = user_purchare_retention['购买日期'] - user_purchare_retention['min']user_purchare_retention['dt_diff'] = user_purchare_retention.order_dt_diff.apply(lambda x: x / np.timedelta64(1,'D'))user_purchare_retention.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 用户ID 订单数 订单金额 购买日期 min order_dt_diff dt_diff 0 1 1 11.77 1997-01-01 1997-01-01 0 days 0.0 1 2 1 12.00 1997-01-12 1997-01-12 0 days 0.0 2 2 5 77.00 1997-01-12 1997-01-12 0 days 0.0 3 3 2 20.76 1997-01-02 1997-01-02 0 days 0.0 4 3 2 20.76 1997-03-30 1997-01-02 87 days 87.0 12345# 区间分组bin = [0,30,60,90,120,150,180,365]user_purchare_retention['date_diff_bin'] = pd.cut(user_purchare_retention.dt_diff,bins = bin)user_purchare_retention.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 用户ID 订单数 订单金额 购买日期 min order_dt_diff dt_diff date_diff_bin 0 1 1 11.77 1997-01-01 1997-01-01 0 days 0.0 NaN 1 2 1 12.00 1997-01-12 1997-01-12 0 days 0.0 NaN 2 2 5 77.00 1997-01-12 1997-01-12 0 days 0.0 NaN 3 3 2 20.76 1997-01-02 1997-01-02 0 days 0.0 NaN 4 3 2 20.76 1997-03-30 1997-01-02 87 days 87.0 (60.0, 90.0] 5 3 2 19.54 1997-04-02 1997-01-02 90 days 90.0 (60.0, 90.0] 6 3 5 57.45 1997-11-15 1997-01-02 317 days 317.0 (180.0, 365.0] 7 3 4 20.96 1997-11-25 1997-01-02 327 days 327.0 (180.0, 365.0] 8 3 1 16.99 1998-05-28 1997-01-02 511 days 511.0 NaN 9 4 2 29.33 1997-01-01 1997-01-01 0 days 0.0 NaN 12pivoted_retention = user_purchare_retention.pivot_table(index='用户ID',columns='date_diff_bin',values='订单金额',aggfunc=sum)pivoted_retention.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date_diff_bin (0, 30] (30, 60] (60, 90] (90, 120] (120, 150] (150, 180] (180, 365] 用户ID 3 NaN NaN 40.3 NaN NaN NaN 78.41 4 29.73 NaN NaN NaN NaN NaN 41.44 5 13.97 38.90 NaN 45.55 38.71 26.14 155.54 7 NaN NaN NaN NaN NaN NaN 97.43 8 NaN 13.97 NaN NaN NaN 45.29 104.17 1pivoted_retention.mean() date_diff_bin (0, 30] 51.540649 (30, 60] 50.215070 (60, 90] 48.975277 (90, 120] 48.649005 (120, 150] 51.399450 (150, 180] 49.932592 (180, 365] 91.960059 dtype: float64 12pivoted_retention_trans = pivoted_retention.fillna(0).applymap(lambda x: 1 if x&gt;0 else 0)pivoted_retention_trans.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date_diff_bin (0, 30] (30, 60] (60, 90] (90, 120] (120, 150] (150, 180] (180, 365] 用户ID 3 0 0 1 0 0 0 1 4 1 0 0 0 0 0 1 5 1 1 0 1 1 1 1 7 0 0 0 0 0 0 1 8 0 1 0 0 0 1 1 12345((pivoted_retention_trans.sum() / pivoted_retention_trans.count()) * 100).plot.bar(figsize = (16,10))plt.ylabel('百分比%')plt.xticks(fontsize = 16,rotation = 360)plt.title('各时间区间的用户留存率')plt.show() 图表解析： 第一个月的留存率达到了38%，历经两个月下滑，之后趋于稳定。说明用户在通过前三个月的使用，逐渐出现分歧，即部分用户成为忠实用户，剩余部分用户流失。有20%左右的用户在第一次购买后的三个月到半年之间有过再次消费，27%的用户在半年后至一年内进行了再次购买。 相比于拉新而言，应该更注重用户忠诚度的培养。如果进行活动，应该放在前三个月。 结合用户生命周期，用户的平均消费时间间隔为68天，所以应该在60天左右的间隔对用户进行召回和引导。 写在最后以上为CDNow网站用户消费行为分析的全部内容，有错误之处，恳请指正。感谢阅读！","link":"/2021/04/22/CDNow%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E6%B6%88%E8%B4%B9%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/"},{"title":"对比分析案例：IMDB电影数据分析及可视化","text":"摘要对比分析IMDB部分电影数据，包括电影类型、产地、原创&amp;改编、评分&amp;票房之间的关系等～ 项目描述项目名称：电影分析可视化 数据来源：数据来源于kaggle，针对IMDb近5000部电影数据进行分析。互联网电影资料库（InternetMovie Database，简称IMDb）是一个关于电影演员、电影、电视节目、电视明星和电影制作的在线数据库。 字段说明 id : TMDB电影标识号 title : 电影名称 cast ：演员列表 director ：导演 budget ：预算（美元） genres ：风格列表，电影类型 popularity ：在 Movie Database 上的相对页面查看次数 production_companies ：制作公司 production_countries ：制作国家 release_date ：上映时间 revenue ：收入 runtime ：电影时长 spoken_languages ：口语 status ：状态 vote_average ：平均评分 vote_count ：评分次数 项目目的： 通过对IMDb近5000部电影数据进行分析，对以下几个方面进行对比分析 电影类型分析 电影产地分析 电影导演分析 电影预算与评分与票房的关系分析 Universal Pictures公司和Paramount Picture公司之间的对比分析 原创电影与改编电影对比分析 环境解释：基于jupyter notebook，可视化工具包：seaborn、matplotlib 12345678910111213141516# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport jsonimport warningswarnings.filterwarnings('ignore')import osos.chdir(r'/Users/nanb/Documents/数据存放')%matplotlib inlineplt.style.use('ggplot') 1234# 数据导入movies = pd.read_csv('tmdb_5000_movies.csv',sep = ',')credits = pd.read_csv('tmdb_5000_credits.csv',sep = ',') 1234# 数据查看# movies.head()credits.head() 数据整理1234# 删除不必要的列credits = credits.drop(columns = ['title','movie_id'])movies = movies.drop(columns = ['homepage','spoken_languages','original_language','original_title','overview','tagline','status']) 1234# 数据合并data = pd.concat([movies,credits],axis = 1)data.head() 缺失值处理123# 检查数据 -- 缺失值data.apply(lambda x:sum(x.isnull())) budget 0 genres 0 id 0 keywords 0 popularity 0 production_companies 0 production_countries 0 release_date 1 revenue 0 runtime 2 title 0 vote_average 0 vote_count 0 cast 0 crew 0 dtype: int64 图表解析：release_date项有一条缺失数据；runtime项有两条缺失数据 123# 找到空值的数据行data[data.isnull().values==True] 12345# 数据处理 -- 缺失数量少，从网上获取到正确的信息进行补充data.loc[2656,'runtime'] = '98'data.loc[4140,'runtime'] = '82'data.loc[4553,'release_date'] = '2014-06-01' 数据类型转换12345678# 数据类型转换#data.dtypes# release_date类型转为日期类型，并提取年份作为新字段data['release_date'] = pd.to_datetime(data['release_date'],format='%Y-%m-%d')data['release_year'] = data['release_date'].dt.yeardata.loc[:,'release_year'].head() 0 2009 1 2007 2 2015 3 2012 4 2012 Name: release_year, dtype: int64 1234567891011121314# json格式转换df = data.copy()json_cols = ['genres','keywords','production_companies','production_countries','cast','crew']for i in json_cols: df[i] = df[i].apply(json.loads)def get_names(x): return ','.join(i['name'] for i in x)df['genres'] = df['genres'].apply(get_names)df['keywords'] = df['keywords'].apply(get_names)df['production_companies'] = df['production_companies'].apply(get_names)df['production_countries'] = df['production_countries'].apply(get_names) 电影类型分析1234567891011# 获取所有的电影类型real_genres = set()for i in df['genres'].str.split(','): real_genres = real_genres.union(i)# 转换成列表，并删除空格real_genres = list(real_genres)real_genres.remove(' ')print(real_genres) ['Foreign', 'War', 'Mystery', 'Action', 'TV Movie', 'Crime', 'Animation', 'Horror', 'Thriller', 'Romance', 'Music', 'Adventure', 'Western', 'Drama', 'Documentary', 'Science Fiction', 'History', 'Fantasy', 'Family', 'Comedy'] 12345# 将所有电影类型添加到数据中for i in real_genres: df[i] = df['genres'].str.contains(i).apply(lambda x:1 if x else 0)#df.head() 1234567# 获取子数据集part1_df = df[['release_year','Mystery', 'Science Fiction', 'Documentary', 'TV Movie', 'Comedy', 'Romance', 'Thriller', 'Music', 'Western', 'Drama', 'Crime', 'Horror', 'History', 'Fantasy', 'War', 'Adventure', 'Animation', 'Family', 'Foreign', 'Action']]# 按照年份进行分组统计每年的各个类型电影数量year_cnt = part1_df.groupby('release_year').sum()# year_cnt.tail() 类型分析可视化1234# 每年电影类型数量折线图year_cnt.plot(kind = 'line',figsize = (15,8),fontsize = 20)plt.title('每年电影类型数量') Text(0.5, 1.0, '每年电影类型数量') 图表解析：从图中可以看出在1916年到2017年之间，数据虽然存在波动但是总体上不同的电影类型数量都在不断扩大，其中戏剧类，喜剧类和惊悚类的增长较为迅速。这3类电影类型在市场占着主导地位。同时也可以预测这3类类型的电影人仍然会占主导地位。 1234567891011121314151617# 电影类型对比genre = year_cnt.sum(axis = 0)genre = genre.sort_values(ascending = True)# 绘制条形图v = genre.valueslabels = list(genre.index)genre.plot(kind = 'barh',label = '',figsize = (15,8),fontsize = 20)plt.title('不同电影类型对比',fontsize = 20)# 添加数据标签显示for x,y in zip(v,range(len(labels))): plt.text(x,y,'{}'.format(x),ha='left',va='center')plt.show() 图表解析：从图中可以看出戏剧类，喜剧类，惊悚类，动作类和爱情类的占比较大，数量较多，说明这5种类型在1916年到2017年受众较广。 导演分析12345678# 导演数据提取def director(x): for i in x: if i['job'] == 'Director': return i['name']df['crew'] = df['crew'].apply(director)df.head() 1234crew1 = df['crew'].value_counts()[:10].sort_values(ascending = True)crew1.plot(kind = 'barh',figsize = (15,8),fontsize = 20,colormap = 'viridis')plt.title('导演电影数量')plt.show() 图表解析：从图中可以看出Steven Spielberg和Woody Allen是最高产的两位电影导演。 电影产地分析12345678910111213141516171819202122232425262728293031# 获取所有的电影产地countries = set()for i in df['production_countries'].str.split(','): countries = countries.union(i)# 转换列表，并进行格式处理countries = list(countries)countries.remove(' ') # print(countries)# 将所有电影类型添加到数据中part2_df = pd.DataFrame()for i in countries: part2_df[i] = df['production_countries'].str.contains(i).apply(lambda x:1 if x else 0)# data.head()# 获取子数据集part2 = part2_df.sum(axis = 0).sort_values(ascending = False)part2_1 = part2[:9]rate = part2_1/part2.sum()other = {'other':1-rate.sum()}part2_df = rate.append(pd.Series(other))part2_df.plot(kind='pie', startangle=50, shadow=False, figsize=(10,10), autopct=&quot;%1.1f%%&quot;,fontsize = 13,colormap = 'Set3')plt.title('电影产出国分布图',fontsize=20) Text(0.5, 1.0, '电影产出国分布图') 图表解析：从图中可以看出超过一半的电影产出国为美国，其次是英国占比约为11%，紧跟的是德国法国。美国无疑是电影业制造大亨，是世界第一的电影强国。 电影预算与评分与票房的关系分析123456789101112# 收入统计r = {}for i in real_genres: r[i] = df.loc[df[i] == 1,'revenue'].sum(axis = 0)/100000000revenue = pd.Series(r).sort_values(ascending = True)# 绘制条形图revenue.plot(kind = 'barh',figsize = (15,8),fontsize = 20,color = 'c')for x,y in zip(revenue.values,range(len(revenue.index))): plt.text(x,y,'{:.1f}'.format(x)) 1234567# 计算各变量间的相关系数矩阵corr = data.corr()corrplt.figure(figsize = (12,8))sns.heatmap(corr,vmin = 0,vmax = 1,cmap = 'Set3') 12345678910# 电影预算与票房散点图plt.figure(figsize = (15,8))x1 = data['budget']y1 = data['revenue']plt.scatter(x1,y1,color = 'g')plt.xlabel('budget',fontsize = 20)plt.ylabel('revenue',fontsize = 20)plt.title('电影预算与票房散点图',fontsize = (20))plt.show() 12345678910# 电影评分与票房散点图plt.figure(figsize = (15,8))x2 = data['vote_average']y2 = data['revenue']plt.scatter(x2,y2,color = 'g')plt.xlabel('vote value',fontsize = 20)plt.ylabel('revenue',fontsize = 20)plt.title('电影评分与票房散点图',fontsize = (20))plt.show() 图表解析：从图中以及计算出的相关系数可以得知电影预算和票房存在正线性相关关系，电影评分与票房也存在着正线性相关关系，但前者的相关性较大，有0.73，后者较小只有0.2。 可以得知一般预算较大的电影票房收入也会不错，但是电影评分高的电影票房收入不见得就多。例如一些小众电影评分会较高但是由于电影排档日期较短的原因票房表现也较为一般。 原创电影与改编电影分析1234part3_df = data.loc[:,['release_year','keywords']]part3_df['adaptation'] = part3_df['keywords'].str.contains('based on novel').apply(lambda x:1 if x else 0)part3_df['original'] = part3_df['keywords'].str.contains('based on novel').apply(lambda x:0 if x else 1)part3_df.head() 123456org_ada_all = org_ada_df.sum(axis = 0)org_ada_rate = org_ada_all/sum(org_ada_all)org_ada_all.plot(kind = 'pie',figsize=(10,10),shadow = False, autopct = '%1.1f%%',legend = True,colormap='Set3')plt.legend(fontsize = 13)plt.title('原创电影与改编电影数量分布',fontsize = 20) Text(0.5, 1.0, '原创电影与改编电影数量分布') 123org_ada_all.plot(kind = 'barh',figsize = (15,8))plt.yticks(fontsize = 20)plt.title('原创电影与改编电影数量对比',fontsize = 20) Text(0.5, 1.0, '原创电影与改编电影数量对比') 图表解析：从图中可以看出占96%的电影为原创电影，原创电影的数量远远大于改编电影，说明原创电影比较受市场的追捧。 12345678part3_df['vote_value'] = data['vote_average']avg_vote = part3_df.groupby(['adaptation','original'])['vote_value'].mean()#平均分对比avg_voteavg_vote.plot(kind='barh',label='',figsize=(12,6))plt.xlabel('评分',fontsize = 20)plt.title('改编电影与原创电影评分平均分对比',fontsize=20)plt.show() 图表解析：改编电影的平均分为6.60，原创电影的平均分为6.07，两者相差不大，改编电影略高，说明改编电影的质量较高，口碑较好。 12org_ada_df = part3_df.groupby('release_year')['original','adaptation'].sum(axis = 0)org_ada_df.tail() 12345org_ada_df.plot(figsize = (15,8))plt.legend(fontsize = 20)plt.yticks(fontsize = 20)plt.xticks(fontsize = 20)plt.title('原创电影与改编电影数量趋势',fontsize = 20) Text(0.5, 1.0, '原创电影与改编电影数量趋势') 图表解析：由图可看出，原创电影一直在不断上涨，且在1990年至2000年后出现了指数型增长，而改编电影的数量一直变化不大 公司对比分析 Universal Pictures公司和Paramount Picture公司之间的对比分析 产出对比12345678910company_df = pd.DataFrame()company_df['universal']=data['production_companies'].str.contains('Universal Pictures').map(lambda x:1 if x else 0)company_df['paramount']=data['production_companies'].str.contains('Paramount Pictures').map(lambda x:1 if x else 0)part5_df = company_df.sum(axis = 0)company_rate = part5_df/part5_df.sum()explode =(company_rate&gt;0.5)/20+0.03part5_df.plot(kind='pie', label='', startangle=50, shadow=False, figsize=(10,10), autopct=&quot;%1.1f%%&quot;,colormap = 'Set3', explode=explode)plt.title('Universal Pictures和Paramount Picture公司产出对比图',fontsize=20) Text(0.5, 1.0, 'Universal Pictures和Paramount Picture公司产出对比图') 图表解析：可以看出，universal picture比paramout picture 产出占比略大 票房对比123456789company_revenue = pd.DataFrame()company_revenue['universial'] = company_df['universal'] * data['revenue']company_revenue['paramount'] = company_df['paramount'] * data['revenue']part6_df = company_revenue.sum()part6_df.plot(kind='barh',label='',figsize=(12,6))#柱状图plt.xlabel('数量',fontsize=20)plt.yticks(fontsize = 20)plt.title('Universal Pictures和Paramount Picture公司票房对比图',fontsize=30)plt.show() 图表解析：可以看出，虽然universal picture比paramout picture 票房略高，但相差不大。 1234567891011company_revenue.index = data['release_year']company_revenue.sort_index()part7_df = company_revenue.groupby('release_year').sum()part7_df.plot(figsize = (15,8))plt.xlabel('时间',fontsize=15)plt.ylabel('票房',fontsize=15)plt.legend(fontsize = 15)plt.title('Universal Pictures和Paramount Picture公司随时间变化票房图',fontsize=20)plt.grid(True)# plt.show() 图表解析：可以看出1960年后票房开始增长，2000年派拉蒙的票房成绩大部分时间都比环球要好，2000年环球的票房成绩大部分比派拉蒙要好，且环球的最高票房要高于派拉蒙。总体来说两家电影公司的票房相差的不是很大，都有波动。 总结 戏剧类与喜剧类所占市场份额最大，最受欢迎。 Steven Spielberg和Woody Allen是最高产的两位电影导演。 总体来说派拉蒙与环球电影公司在电影市场上平分秋色。 电影市场上的主体是原创电影。改编电影的口碑略好于原创电影。 写在最后以上是对kaggle上的IMDb电影数据进行的一部分对比分析与可视化。数据本身还有待分析与挖掘的角度，有任何错误的地方，恳请指正，感谢阅读～","link":"/2021/03/01/IMDB%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"title":"Matplotlib - 常用图表 &amp; python表格样式","text":"摘要Matplotlib库的使用，包括常用图表的绘制，以及表格样式 前言Matplotlib 是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。 通俗地说，matplotlib可能是数据分析中最常用的绘图Python包了。它可以对Python中的数据进行快速的可视化，并以多种格式输出。接下来，我们将以互动的方式介绍matplotlib中的大多数情况 导入模块本文基于Jupiter notebook环境下进行举例介绍，先导入使用到的python模块 1234import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 初步认识先从一维数组和二维数组来简单认识一下Matplotlib 1、一维数组​ 借助numpy模块构建一个一维数组，并生成折线图 1234567891011121314151617181920212223242526272829303132333435363738# 数据构建ts = pd.Series(np.random.randn(1000),index=pd.date_range('1/1/2018',periods=1000))ts = ts.cumsum()# 参数设置ts.plot( # line(折线图),bar(柱状图),barh(柱状图-横),kde(密度图) kind = 'line', # 图例标签，Dataframe格式以列名为label label = 'nb', # 风格字符串，这里包括了linestyle，marker，color style = '--g.', color = 'b', alpha = 0.6, grid = True, # 是否以index作为横坐标轴 use_index = True, # 横坐标旋转角度 rot = 45, # y轴界限 ylim = [-50,50], # y轴刻度值 yticks = list(range(-50,50,10)), figsize = (12,8), title = 'normal', # 是否显示图例，一般直接用plt.legend() legend = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x19d3c90&gt; 2、二维数组1234567891011121314151617181920# 数据构建df = pd.DataFrame(np.random.randn(1000,4),index=ts.index,columns=list('abcd'))# 返回累计和df = df.cumsum()# 参数设置df.plot(kind = 'line', style = '--.', grid = True, alpha = 0.3, use_index = True, rot = 30, figsize = (12,8), title = True, legend = True, subplots = False, colormap = 'Greens') &lt;matplotlib.axes._subplots.AxesSubplot at 0xd2cc490&gt; 常用图表下面介绍matplotlib在数据分析中的一些常用图表的绘制以及参数设置 柱状图plt.plot(kind = ‘bar/barh’) / plt.bar() 1234567891011121314151617181920212223# 数据、画布构建fig,axes = plt.subplots(4,1,figsize = (18,16))s = pd.Series(np.random.randint(0,10,16),index=list('abcdefghijklmnop'))df = pd.DataFrame(np.random.rand(10,3),columns=['a','b','c'])# 单系列s.plot(kind = 'bar',ax = axes[0], rot = 0)# 多系列df.plot(kind = 'bar',ax = axes[1])# 多系列堆叠图df.plot(kind = 'bar',stacked = True,ax = axes[2])# 另一种写法df.plot.bar(ax = axes[3])# plt.axis('tight') &lt;matplotlib.axes._subplots.AxesSubplot at 0x14008550&gt; 堆叠图123456789101112131415161718192021222324plt.figure(figsize = (12,8))x = np.arange(10)y1 = np.random.rand(10)y2 = -np.random.rand(10)plt.bar(x,y1,width = 1,facecolor = 'yellowgreen',edgecolor = 'white',yerr = y1*0.1)plt.bar(x,y2,width = 1,facecolor = 'lightskyblue',edgecolor = 'white',yerr = y2*0.1)# 参数解析：# width:宽度比例# facecolor:柱状图里填充的颜色，edgecolor:边框的颜色# left - 每个柱x轴左边界，bottom - 每个柱y轴下边界 → bottom扩展可化成甘特图 Gantt Chart# align：决定整个bar图分布，默认left表示从左边界开始绘制，center会将图绘制在中间位置# fig.tight_layout()plt.grid()for i,j in zip(x,y1): plt.text(i-0.15,0.05,'%.2f' % j ,color = 'white')for i,j in zip(x,y2): plt.text(i-0.15,-0.1,'%.2f' % -j ,color = 'white')# zip() 函数用于将可迭代对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表 外嵌图表图与表的结合，能更全面的展示数据，在展示了数据的可读性的同时，也具备了 1234567891011121314151617181920212223242526272829303132333435# 外嵌图表 plt.table()data = [[ 66386, 174296, 75131, 577908, 32015], [ 35713,21312,32133,12414,57565], [ 36588,56856,58756,98356,87589], [35356,36432,68886,87875,95636], [21415,56936,68587,98793,87935]]columns = ('Freeze','Wind','Flood','Quake','Hail')rows = ['%d year' % x for x in (100,50,20,10,5)]df = pd.DataFrame(data,columns=columns,index=rows)print(df)df.plot(kind = 'bar',grid = True,colormap = 'Blues_r',stacked = True,figsize = (12,8))# 创建堆叠图plt.table(cellText = data, cellLoc = 'center', cellColours = None, rowLabels = rows, rowColours = plt.cm.BuPu(np.linspace(0,0.5,5))[::-1],#BuPu相当于colormap colLabels = columns, colColours = plt.cm.Reds(np.linspace(0,0.5,5))[::-1], rowLoc = 'right', loc = 'bottom')# 参数解析：# cellText：表格文本# cellLoc：cell内文本对齐位置# rowLabels：行标签# colLabels：列标签# rowLoc：行标签对齐位置# loc：表格位置 → left,right,top,bottomplt.xticks([]) Freeze Wind Flood Quake Hail 100 year 66386 174296 75131 577908 32015 50 year 35713 21312 32133 12414 57565 20 year 36588 56856 58756 98356 87589 10 year 35356 36432 68886 87875 95636 5 year 21415 56936 68587 98793 87935 ([], &lt;a list of 0 Text xticklabel objects&gt;) 面积图1234567891011121314# 面积图: plot.areafig,axes = plt.subplots(2,1,figsize = (12,8))df1 = pd.DataFrame(np.random.rand(10,4),columns = ['a','b','c','d'])df2 = pd.DataFrame(np.random.randn(10,4),columns = ['a','b','c','d'])df1.plot.area(colormap = 'Greens_r',alpha = 0.6,ax = axes[0])df2.plot.area(stacked = False,colormap = 'Set2',alpha = 0.6,ax = axes[1])# stacked : 是否堆叠，默认情况下，区域图被堆叠# 为了产生堆积面积图，每列必须是全部为正值或全部为负值# 当数据有NaN时，自动填充0，所以图标签需要清洗掉缺失值 &lt;matplotlib.axes._subplots.AxesSubplot at 0x115591f0&gt; 填图123456789101112# 填图x1 = np.linspace(0,5*np.pi,1000)y3 = np.sin(x1)y5 = np.sin(2*x1)axes[1].fill_between(x1,y3,y5,color = 'b',alpha = 0.5,label = 'area')#填充两个函数之间的区域，使用fill_between函数for i in range(2): axes[i].legend() axes[i].grid() 饼图1234567891011121314151617181920212223242526272829303132333435363738# 饼图s = pd.Series(3*np.random.rand(4),index=['a','b','c','d'],name = 'series')plt.axis('equal')plt.pie(s, # 指定每部分的偏移量 explode=[0.1,0,0,0], # 标签 labels = s.index, # 颜色 colors = ['r','g','b','c'], # 饼图上的数据标签显示方式 autopct='%.2f%%', # 每个饼切片的中心和通过autopct生成的文本开始之间的比例 pctdistance=0.6, # 被画饼标记的直径，默认值：1.1 labeldistance=1.2, # 阴影 shadow = True, # 开始角度 startangle=0, # 半径 radius=1.5, # 图框 frame=False )# counterclock:指定指针方向，顺时针或者逆时针print(s) a 0.791267 b 2.460981 c 2.458505 d 2.777957 Name: series, dtype: float64 直方图1234567891011121314151617181920212223# 直方图 plt.hist()s = pd.Series(np.random.randn(1000))s.hist( # 箱子个数 bins = 20, # 风格：bar,barstacked,step,stepfilled histtype = 'bar', # 对齐方式：left,right,mid align = 'mid', # 水平还是垂直：'horizontal','vertical' orientation = 'vertical', alpha = 0.5, # 标准化 normed = True )s.plot(kind = 'kde',style = 'k--') &lt;matplotlib.axes._subplots.AxesSubplot at 0x15226d10&gt; 堆叠直方图1234567891011121314151617# 堆叠直方图plt.figure(num = 1)df = pd.DataFrame({'a':np.random.randn(1000) + 1, 'b':np.random.randn(1000), 'c':np.random.randn(1000) - 1, 'd':np.random.randn(1000) - 2}, columns=['a','b','c','d'])df.plot.hist(stacked = True, bins = 20, colormap = 'Greens_r', alpha = 0.5, grid = True, edgecolor = 'black')df.hist(bins = 50)# 直接生成多个直方图 array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1510E290&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x15745CD0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x15761AF0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1577CA10&gt;]], dtype=object) &lt;Figure size 432x288 with 0 Axes&gt; 散点图123456789101112131415161718# 散点图 # plt.scatter('x', 'y', 's=None', 'c=None', 'marker=None', 'cmap=None', 'norm=None','vmin=None', 'vmax=None', # 'alpha=None', 'linewidths=None', 'verts=None', 'edgecolors=None', '*', 'data=None', '**kwargs'）plt.figure(figsize = (8,6))x = np.random.randn(1000)y = np.random.randn(1000)plt.scatter(x,y,marker = '.', s = np.random.randn(1000)*100, c = y*100, cmap = 'Reds', alpha = 0.8)plt.grid()# vmin,vmax：亮度设置，标量 矩阵散点图123456789101112# 矩阵散点图df = pd.DataFrame(np.random.randn(100,4),columns=['a','b','c','d'])pd.scatter_matrix(df,figsize = (10,6), marker = 'o', diagonal='kde', alpha = 0.5, range_padding=0.1)# diagonal:({'hist','kde'})，必须只能在两个中选其一， → 每个指标的频率图# range_padding : (float,可选),图像在x轴、y轴原点附近的留白，值越大，留白距离越大，图像远离坐标原点 极坐标图123456789101112131415161718192021222324252627# 极坐标图s = pd.Series(np.arange(20))theta = np.arange(0,2*np.pi,0.02)# print(s.head())# print(theta[:10])# 创建数据fig = plt.figure(figsize = (12,8))ax1 = plt.subplot(121,projection = 'polar')ax2 = plt.subplot(122)# projection = 'polar' → 创建极坐标图# ax = fig.add_subplot(111,polar = True)ax1.plot(theta,theta*3,linestyle = '--',lw = 1)ax1.plot(s,linestyle = '--',marker = '.',lw = 2)ax2.plot(theta,theta*3,linestyle = '--',lw = 1)ax2.plot(s)plt.grid()# 创建极坐标图，参数1为角度(弧度制)，参数2为value# lw → 线宽 1234567891011121314151617181920212223242526272829# 极坐标参数设置theta = np.arange(0,2*np.pi,0.02)plt.figure(figsize = (8,4))ax1 = plt.subplot(121,projection = 'polar')ax2 = plt.subplot(122,projection = 'polar')ax1.plot(theta,theta/6,'--',lw = 2)ax2.plot(theta,theta/6,'--',lw = 2)# set_theta_direction : 坐标轴正方形，默认逆时针ax2.set_theta_direction(-1)# set_thetagrids : 设置极坐标角度网格线显示及标签 → 网格和标签数量一致ax2.set_thetagrids(np.arange(0.0,360.0,90),['a','b','c','d'])# set_rgrids : 设置极径网格线显示，其中参数必须是正数ax2.set_rgrids(np.arange(0.2,2,0.4))# set_theta_offset : 设置角度偏移,逆时针,弧度制ax2.set_theta_offset(np.pi/2)# set_rlim : 设置显示的极径范围ax2.set_rlim(0.2,1.2)# set_rmax : 设置显示的极径最大值ax2.set_rmax(2)# set_rticks : 设置极径网格线的显示范围ax2.set_rticks(np.arange(0.1,1.5,0.2)) 雷达图123456789101112131415161718192021# 雷达图1 - 极坐标的折线图/填图# plt.plot画出的雷达图首尾不相连plt.figure(figsize = (12,8))ax1 = plt.subplot(111,projection = 'polar')ax1.set_title('radar map\\n')ax1.set_rlim(0,12)data1 = np.random.randint(1,10,10)data2 = np.random.randint(1,10,10)data3 = np.random.randint(1,10,10)theta = np.arange(0,2*np.pi,2*np.pi/10)ax1.plot(theta,data1,'--',label = 'data1')ax1.fill(theta,data1,alpha = 0.2)ax1.plot(theta,data2,'--',label = 'data1')ax1.fill(theta,data2,alpha = 0.2)ax1.plot(theta,data3,'--',label = 'data1')ax1.fill(theta,data3,alpha = 0.2) [&lt;matplotlib.patches.Polygon at 0xe585fd0&gt;] 雷达图进阶使用​ 雷达图与极坐标图、填图的组合使用 123456789101112131415161718192021# 雷达图2 - 极坐标的折线图/填图 # plt.polar() → 首尾闭合labels = np.array(['a','b','c','d','e','f'])dataLenth = 6data1 = np.random.randint(0,10,6)data2 = np.random.randint(0,10,6)angles = np.linspace(0,2*np.pi,dataLenth,endpoint=False) #分割圆周长data1 = np.concatenate((data1,[data1[0]])) #闭合data2 = np.concatenate((data2,[data2[0]]))angles = np.concatenate((angles,[angles[0]]))plt.polar(angles,data1,'o-',linewidth = 1)plt.fill(angles,data1,alpha = 0.25)plt.polar(angles,data2,'o-',linewidth = 1)plt.fill(angles,data2,alpha = 0.25)plt.thetagrids(angles * 180/np.pi,labels)plt.ylim(0,10) (0, 10) 极轴图12345678910111213141516# 极轴图 - 极坐标的柱状图plt.figure(figsize = (12,8))ax1 = plt.subplot(111,projection = 'polar')ax1.set_title('rader map\\n')ax1.set_rlim(0,12)data = np.random.randint(1,10,10)theta = np.arange(0,2*np.pi,2*np.pi/10)bar = ax1.bar(theta,data,alpha = 0.5)for r,bar in zip(data,bar): bar.set_facecolor(plt.cm.jet(r/10.))plt.thetagrids(np.arange(0.0,360.0,90),[]) (&lt;a list of 8 Line2D thetagridline objects&gt;, &lt;a list of 4 Text thetagridlabel objects&gt;) 箱型图​ 箱型图：又称箱线图、盒须图，是一种用作显示一组数据分散情况资料的统计图 123456789101112131415161718192021222324252627# 1、中位数：一组数据平均分成两份，中间的数# 2、上四分位数Q1：是将序列平均分成四份，计算(n+1)/4与(n-1)/4两种，一般使用(n+1)/4# 3、下四分位数Q3：是将序列平均分为四份，计算(1+n)/4*3 = 0.75# 4、内限 → T形的盒须就是内限，最大值区间Q3+1.5IQR,最小区间Q1-1.5IQR(IQR = Q3 - Q1)# 5、外限 → T形的盒须就是内限，最大值区间Q3+3IQR,最小区间Q1-3IQR(IQR = Q3 - Q1)# 6、异常值：内限之外→中度异常，外限之外→极度异常fig,axes = plt.subplots(2,1,figsize = (12,8))df = pd.DataFrame(np.random.rand(10,5),columns=['a','b','c','d','e'])color = dict(boxes = 'DarkGreen',whiskers = 'DarkOrange',medians = 'DarkBlue', caps = 'Gray')df.plot.box(ylim = [0,1.2], grid = True, color = color, ax = axes[0])df.plot.box(vert = False, positions = [1,4,5,6,8], ax = axes[1], grid = True, color = color) &lt;matplotlib.axes._subplots.AxesSubplot at 0x109bccd0&gt; 箱型图-11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# .boxplotdf = pd.DataFrame(np.random.rand(10,5),columns=['a','b','c','d','e'])plt.figure(figsize = (12,8))# 构建箱型图f = df.boxplot( # 异常点的形状 sym = 'o', # 是否垂直 vert = True, # IQR，默认1.5，可以设置区间[5,95]，代表上下边缘为数据的95%和5% whis = 1.5, # 上下四分位框内是否填充 patch_artist = True, # 是否有均值线及其形状 meanline = False,showmeans = True, # 是否显示箱线 showbox = True, # 是否显示边缘线 showcaps = True, # 是否显示异常值 showfliers = True, # 中间箱体是否缺口 notch = False, # 返回类型为字典 return_type = 'dict')plt.title('boxplot')print(f)# 其余参数设置for box in f['boxes']: # 箱体边框颜色 box.set(color = 'b',linewidth = 1) # 箱体内部填充颜色 box.set(facecolor = 'b',alpha = 0.5)for whisker in f['whiskers']: whisker.set(color = 'k',linewidth = 0.5,linestyle = '-')for cap in f['caps']: cap.set(color = 'gray',linewidth = 2)for median in f['medians']: median.set(color = 'DarkBlue',linewidth = 2)for flier in f['fliers']: flier.set(marker = 'o',color = 'y',alpha = 0.5)# 参数解析：# boxes：箱线# medians：中位线的横线# whiskers：从box到error bar之间的竖线# fliers：异常值# caps：error bar横线# means：均值的横线 箱型图-2123456789101112131415# plt.boxplot() 绘制# 分组汇总df = pd.DataFrame(np.random.rand(10,2),columns = ['col1','col2'])df['x'] = pd.Series(['a','a','a','a','a','b','b','b','b','b'])df['y'] = pd.Series(['a','b','a','b','a','b','a','b','a','b'])print(df.head())# df.boxplot(by = 'x')df.boxplot(column = ['col1','col2'],by = ['x','y'])# columns:按照数据的列分子图# by：按照列分组做箱型图 col1 col2 x y 0 0.507773 0.223859 a a 1 0.128340 0.482120 a b 2 0.955340 0.912310 a a 3 0.170645 0.949025 a b 4 0.821798 0.059242 a a array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x13B3BA30&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1115FED0&gt;], dtype=object) python的表格样式1、表格样式创建1234567891011121314151617181920212223# 样式创建# 1、styler.applymap: elementwise → 按元素方式处理df，也就是按照每一个值处理# 2、styler.apply: column- / row- / tabel_wise → 按行/列处理dfdf = pd.DataFrame(np.random.randn(10,4),columns=['a','b','c','d'])sty = df.style# print(sty,type(sty))def color_neg_red(val): if val &lt; 0: color = 'red' else: color = 'black' return('color:%s' % color)df.style.applymap(color_neg_red)# 创建样式方法，使得小于0的数变成红色# style.applymap() → 自动调用其中的函数 2、样式处理12345678910111213141516171819202122# 按列/行处理样式：style.apply()df = pd.DataFrame(np.random.randn(10,4),columns=['a','b','c','d'])sty = df.styledef highlight_max(s): is_max = s == s.max() lst = [] for v in is_max: if v: lst.append('background-color: yellow') else: lst.append('') return(lst)# df.style.apply(highlight_max,axis = 0,subset = ['b','c'])# subset:选择索引进行函数处理df.style.apply(highlight_max,axis = 1, subset = pd.IndexSlice[2:5,['b','d']])# df[2:5].style.apply(highlight_max,subset = ['b','d']) 3、内容显示123456789101112131415# 表格显示控制df = pd.DataFrame(np.random.randn(10,4),columns=['a','b','c','d'])# df.head().style.format('{:.2%}') #显示百分比# df.head().style.format('{:.4f}') #显示小数点# df.head().style.format(&quot;{:+.2f}&quot;) #显示正负数# 分列显示df.head().style.format({'b':&quot;{:.2%}&quot;,'c':&quot;{:+.3f}&quot;,'d':&quot;{:.3f}&quot;}) 表格进阶1、应用 — 空值定位1234567# 内置样式调用# 1、定位空值df = pd.DataFrame(np.random.rand(5,4),columns=list('abcd'))df['a'][2] = np.nandf.style.highlight_null(null_color = 'yellow') 2、应用 — 色彩映射1234# 2、色彩映射df = pd.DataFrame(np.random.rand(10,4),columns=list('abcd'))df.style.background_gradient(cmap = 'Greens',axis = 1,low = 0,high = 1) 3、应用 — 条形图显示12345# 3、条形图df = pd.DataFrame(np.random.rand(10,4),columns=list('abcd'))df.style.bar(subset = ['a','b'],color = '#d65f5f',width = 100)# width: 最长长度在格子的占比 4、应用 — 分段式显示12345678# 分段式构建样式df = pd.DataFrame(np.random.rand(10,4),columns=list('abcd'))df['a'][3] = np.nandf['b'][7] = np.nandf.style.\\ bar(subset = ['a','b'],color = '#d33f5f',width = 100).\\ highlight_null(null_color = 'yellow') 总结以上为数据分析中matplotlib库中的常用图表的绘制以及各个图表的参数说明，记录下来方便查漏补缺的同时，可供随时翻阅，有哪些地方出现错误或者疑义，欢迎讨论，感谢阅读～ 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/09/Matplotlib%E5%85%A5%E9%97%A8%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"MySQL练习 - 场景模拟初级篇(持续更新)","text":"摘要MySQL练习题，基于不同的场景模拟进一步加强MySQl知识点练习～ MySQL练习 - 场景模拟初级篇 薪水涨幅升序 [题目] 现在有两个表： 雇员表：记录了雇员基本信息，字段包括：雇员编号、出生日期、姓名、性别、雇用日期 薪水表：记录了雇员的薪水金额以及签订周期，字段包括：雇员编号、薪水、起始日期、结束日期 两张表通过雇员编号进行连结 现在需要查找当前所有雇员入职以来的薪水涨幅，给出雇员编号以及其对应的薪水涨幅，并按照薪水涨幅进行升序 注意：薪水表中最大结束日期为2004-01-01，说明结束日期在此之前的均表示为已离职员工 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#建雇员表create table employees( id int not null comment &quot;雇员编号&quot;, birthday date comment &quot;出生日 期&quot;, name varchar(64) comment &quot;姓名&quot;, sex varchar(64) comment &quot;性别&quot;, employ_date date comment &quot; 雇用日期&quot;);#查看表desc employees;+-------------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------------+-------------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || birthday | date | YES | | NULL | || name | varchar(64) | YES | | NULL | || sex | varchar(64) | YES | | NULL | || employ_date | date | YES | | NULL | |+-------------+-------------+------+-----+---------+-------+#插入数据insert into employees(id,birthday,name,sex,employ_date) values (10002,&quot;1976-09-09&quot;,&quot;周周&quot;,&quot;女&quot;,&quot;2001-08-02&quot;),(10005,&quot;1978-09-09&quot;,&quot;小明&quot;,&quot;男&quot;,&quot;2001-09-09&quot;),(10006,&quot;1979-08-29&quot;,&quot;西西&quot;,&quot;女&quot;,&quot;2001-08-02&quot;);#查看数据select * from employees;+-------+------------+--------+------+-------------+| id | birthday | name | sex | employ_date |+-------+------------+--------+------+-------------+| 10002 | 1976-09-09 | 周周 | 女 | 2001-08-02 || 10005 | 1978-09-09 | 小明 | 男 | 2001-09-09 || 10006 | 1979-08-29 | 西西 | 女 | 2001-08-02 |+-------+------------+--------+------+-------------+#建薪水表create table salary( id int not null comment &quot;雇员编号&quot;, pay int comment &quot;薪资&quot;, start_date date comment &quot;起始日期&quot;, finish_date date comment &quot;结束日期&quot;);#查看表desc salary;+-------------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------------+---------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || pay | int(11) | YES | | NULL | || start_date | date | YES | | NULL | || finish_date | date | YES | | NULL | |+-------------+---------+------+-----+---------+-------+#插入数据insert into salary(id,pay,start_date,finish_date) values (10002,72527,&quot;2001-08-02&quot;,&quot;2003-01-01&quot;),(10002,75432,&quot;2003-01-01&quot;,&quot;2004-01-01&quot;),(10005,94692,&quot;2001-09-09&quot;,&quot;2003-01-01&quot;),(10006,43311,&quot;2001-08-02&quot;,&quot;2004-01-01&quot;);#查看数据select * from salary;+-------+-------+------------+-------------+| id | pay | start_date | finish_date |+-------+-------+------------+-------------+| 10002 | 72527 | 2001-08-02 | 2003-01-01 || 10002 | 75432 | 2003-01-01 | 2004-01-01 || 10005 | 94692 | 2001-09-09 | 2003-01-01 || 10006 | 43311 | 2001-08-02 | 2004-01-01 |+-------+-------+------------+-------------+ [思路] 读题：查找当前所有雇员入职以来的薪水涨幅，给出雇员编号以及对应的薪水涨幅，并按照薪水涨幅进行升序 解题： 输出为：雇员编号、薪水涨幅，且按照薪水涨幅进行升序排列 限定条件为：当前雇员，即结束日期为表中最大日期2004 - 01 - 01 薪水涨幅 = 当前薪水 - 入职薪水 ，根据最终输出要求，先分别找出雇员编号+当前薪水作为临时表a，在找出雇员编号+入职薪水作为临时表b，最后将临时表a和临时表b进行join方能运算出结果 123456789101112131415161718192021222324select a.id,(a.pay - b.pay) as &quot;薪水涨幅&quot; from (select id,pay from salary where finish_date = &quot;2004-01-01&quot;) as a join (select e.id,s.pay from employees as e join salary as s on e.id = s.id where e.employ_date = s.start_date and e.id in (select id from salary where finish_date = &quot;2004-01-01&quot;)) as b on a.id = b.id order by &quot;薪水涨幅&quot;; +-------+--------------+| id | 薪水涨幅 |+-------+--------------+| 10002 | 2905 || 10006 | 0 |+-------+--------------+ 比前一天高的数据 [题目] “日销”表记录了某公司每天的营业额，字段有：id，日期，营业额(万元) 现在需要找出所有比前一天(昨天)营业额更高的数据 12345678910111213141516171819202122232425262728293031323334353637383940414243#创建表create table daysale( id int not null, saledate date comment &quot;日期&quot;, turnover int comment &quot;营业额(万元)&quot;); #查看表desc daysale;+----------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || saledate | date | YES | | NULL | || turnover | int(11) | YES | | NULL | |+----------+---------+------+-----+---------+-------+#插入数据insert into daysale(id,saledate,turnover) values (1,&quot;2019-01-01&quot;,97),(2,&quot;2019-01-02&quot;,87),(3,&quot;2019-01-03&quot;,88),(4,&quot;2019-01-04&quot;,98),(5,&quot;2019-01-05&quot;,100),(6,&quot;2019-01-06&quot;,80),(7,&quot;2019-01-07&quot;,77),(8,&quot;2019-01-08&quot;,92);#查看数据select * from daysale;+----+------------+----------+| id | saledate | turnover |+----+------------+----------+| 1 | 2019-01-01 | 97 || 2 | 2019-01-02 | 87 || 3 | 2019-01-03 | 88 || 4 | 2019-01-04 | 98 || 5 | 2019-01-05 | 100 || 6 | 2019-01-06 | 80 || 7 | 2019-01-07 | 77 || 8 | 2019-01-08 | 92 |+----+------------+----------+ [思路] 读题：找出所有营业额比**前一天(昨天)**营业额更高的数据 解题： 比较同一个表内不同行的内容需要自join，连结条件为日期差为1天 日期比较方式1：datediff(日期1,日期2) ，即日期1 - 日期2的天数差 日期比较方式2：timestampdiff(day/hour/second,日期1,日期2)，即日期2 - 日期1的差，与上面相反 结果限定条件为：营业额比前一天的高 12345678910111213141516171819202122232425262728293031#datediffselect b.* from daysale as a join daysale as b on datediff(b.saledate,a.saledate) = 1where b.turnover &gt; a.turnover;+----+------------+----------+| id | saledate | turnover |+----+------------+----------+| 3 | 2019-01-03 | 88 || 4 | 2019-01-04 | 98 || 5 | 2019-01-05 | 100 || 8 | 2019-01-08 | 92 |+----+------------+----------+#timestempdiffselect b.* from daysale as a join daysale as b on timestampdiff(day,a.saledate,b.saledate) = 1where b.turnover &gt; a.turnover;+----+------------+----------+| id | saledate | turnover |+----+------------+----------+| 3 | 2019-01-03 | 88 || 4 | 2019-01-04 | 98 || 5 | 2019-01-05 | 100 || 8 | 2019-01-08 | 92 |+----+------------+----------+ 最小的n个数 [题目] 现在有两个表： 学生表里记录了学生的学号、入学时间等信息，字段有：姓名、学号、班级、入学时间、年龄、专业 成绩表里记录了学生选课成绩的信息，字段有：学号、课程号、分数 两张表通过学号进行连结 现在需要： 筛选出2017年入学的”计算机”专业年龄最小的3位同学，输出格式为姓名、年龄 统计每个班各位同学成绩平均分大于80分的人数和人数占比 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#创建学生表create table stu_07( name varchar(64) comment &quot;姓名&quot;, id int not null comment &quot;学号&quot;, class varchar(64) comment &quot;班级&quot;, entrancedate date comment &quot;入学时间&quot;, age int comment &quot;年龄&quot;, professional varchar(64) comment &quot;专业&quot;);#查看表desc stu_07;+--------------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+--------------+-------------+------+-----+---------+-------+| name | varchar(64) | YES | | NULL | || id | int(11) | NO | | NULL | || class | varchar(64) | YES | | NULL | || entrancedate | date | YES | | NULL | || age | int(11) | YES | | NULL | || professional | varchar(64) | YES | | NULL | |+--------------+-------------+------+-----+---------+-------+#插入数据insert into stu_07(name,id,class,entrancedate,age,professional) values (&quot;赵一&quot;,0001,&quot;1班&quot;,&quot;2016-09-01&quot;,19,&quot;计算机&quot;),(&quot;钱二&quot;,0002,&quot;1班&quot;,&quot;2017-09-01&quot;,21,&quot;计算机&quot;),(&quot;孙三&quot;,0003,&quot;2班&quot;,&quot;2017-09-01&quot;,19,&quot;金融&quot;),(&quot;李四&quot;,0004,&quot;3班&quot;,&quot;2017-09-01&quot;,17,&quot;计算机&quot;),(&quot;周周&quot;,0005,&quot;3班&quot;,&quot;2017-09-01&quot;,20,&quot;计算机&quot;),(&quot;吴五&quot;,0006,&quot;3班&quot;,&quot;2017-09-01&quot;,18,&quot;计算机&quot;);#查看数据select * from stu_07;+--------+----+-------+--------------+------+--------------+| name | id | class | entrancedate | age | professional |+--------+----+-------+--------------+------+--------------+| 赵一 | 1 | 1班 | 2016-09-01 | 19 | 计算机 || 钱二 | 2 | 1班 | 2017-09-01 | 21 | 计算机 || 孙三 | 3 | 2班 | 2017-09-01 | 19 | 金融 || 李四 | 4 | 3班 | 2017-09-01 | 17 | 计算机 || 周周 | 5 | 3班 | 2017-09-01 | 20 | 计算机 || 吴五 | 6 | 3班 | 2017-09-01 | 18 | 计算机 |+--------+----+-------+--------------+------+--------------+#创建成绩表create table score_07(id int not null comment &quot;学号&quot;,courseid int comment &quot;课程号&quot;,score int comment &quot;分数&quot;);#查看表desc score_07;+----------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || courseid | int(11) | YES | | NULL | || score | int(11) | YES | | NULL | |+----------+---------+------+-----+---------+-------+#插入数据insert into score_07(id,courseid,score) values(0001,01,90),(0002,01,70),(0002,02,84),(0003,01,90),(0003,03,80),(0004,01,90),(0004,02,60),(0005,01,85),(0006,02,70);#查看数据select * from score_07;+----+----------+-------+| id | courseid | score |+----+----------+-------+| 1 | 1 | 90 || 2 | 1 | 70 || 2 | 2 | 84 || 3 | 1 | 90 || 3 | 3 | 80 || 4 | 1 | 90 || 4 | 2 | 60 || 5 | 1 | 85 || 6 | 2 | 70 |+----+----------+-------+ [解题1] 读题：筛选出2017年入学的”计算机“专业年龄最小的3位同学，输出格式为姓名、年龄 解题： 限定条件1：2017年入学，根据入学时间的年份进行限定 限定条件2：专业 - 计算机 限定条件3：年龄最小的3位同学，根据年龄进行倒序排列，limit输出前3行 1234567891011121314select name,agefrom stu_07where year(entrancedate) = 2017and professional = &quot;计算机&quot;order by agelimit 3;+--------+------+| name | age |+--------+------+| 李四 | 17 || 吴五 | 18 || 周周 | 20 |+--------+------+ [解题2] 读题：统计每个班各位同学成绩平均分大于80分的人数和人数占比 解题： 每位同学成绩平均分 &gt; 80，成绩表根据 id 进行分组算出平均分，作为临时表 统计每个班，则需要按照班级来分组，班级信息在学生表，而平均分信息在临时表，需要两表进行join 输出为人数、人数占比，使用case when 进行统计平均分大于80的人数，以及后续的计算占比 1234567891011121314151617select sum(case when 平均成绩 &gt; 80 then 1 else 0 end) as &quot;人数&quot;,sum(case when 平均成绩 &gt; 80 then 1 else 0 end)/count(st.id) as &quot;人数占比&quot;from stu_07 as st left join(select id,avg(score) as 平均成绩 from score_07 group by id) as a on st.id = a.idgroup by st.class;+--------+--------------+| 人数 | 人数占比 |+--------+--------------+| 1 | 0.5000 || 1 | 1.0000 || 1 | 0.3333 |+--------+--------------+ 连续出现n次的数据 [题目] 成绩表记录了学生id以及分数，字段有：id、score 先需要查找出所有至少连续出现3次的分数 12345678910111213141516171819202122232425262728293031323334353637#创建表create table score_09( id int not null comment &quot;学号&quot;, score int comment &quot;分数&quot;);#查看表desc score_09; +-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || score | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+#插入数据insert into score_09(id,score) values(0001,89),(0002,76),(0003,76),(0004,83),(0005,83),(0006,83),(0007,77),(0008,90),(0009,88),(0010,86);#查看数据select * from score_09;+----+-------+| id | score |+----+-------+| 1 | 89 || 2 | 76 || 3 | 76 || 4 | 83 || 5 | 83 || 6 | 83 || 7 | 77 || 8 | 90 || 9 | 88 || 10 | 86 |+----+-------+ **[思路]**： 读题：查找出所有、至少、连续出现3次的分数 解题： 注意：这个题目要求学号必须连续，以下语句才能正确执行 比较同一个表里的同一列的不同行的数据，需要自join 连续出现3次，意思就是学号a的成绩=学号a+1的成绩=学号a+2的成绩，说明需要自join3次 输出为这个至少出现3次的成绩是多少，distinct去除重复 12345678910select distinct a.scorefrom score_09 as a join score_09 as b join score_09 as con a.id = b.id - 1 and b.id = c.id - 1where a.score = b.score and b.score = c.score;+-------+| score |+-------+| 83 |+-------+ 课程满意度分析 [题目] 满意度记录了教师和学生对课程的满意程度，字段有：教师编号、学生编号、是否满意； 其中是否满意代表老师和学生对课程的评价，值”是”代表教师和学生都满意 用户表记录了学校教师和学生的信息，字段有：用户编号、是否在系统、角色 每个用户有唯一的编号 是否在系统表示 这个用户是否还在这所学校里 角色表示这个用户是教师还是学生 满意度表中的学生编号、教师编号与用户表的编号联结 现需要分析学校里人员对课程的满意度 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#创建满意度表create table satisfaction( tno varchar(10) comment &quot;教师编号&quot;, sno varchar(10) comment &quot;学生编号&quot;, satisfied varchar(64) comment &quot;是否满意&quot;);#查看表desc satisfaction;+-----------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-----------+-------------+------+-----+---------+-------+| tno | varchar(10) | YES | | NULL | || sno | varchar(10) | YES | | NULL | || satisfied | varchar(64) | YES | | NULL | |+-----------+-------------+------+-----+---------+-------+#插入数据insert into satisfaction(tno,sno,satisfied)values(&quot;01&quot;,&quot;2&quot;,&quot;学生不满意&quot;),(&quot;01&quot;,&quot;1&quot;,&quot;是&quot;),(&quot;02&quot;,&quot;1&quot;,&quot;老师不满意&quot;),(&quot;02&quot;,&quot;2&quot;,&quot;是&quot;),(&quot;03&quot;,&quot;1&quot;,&quot;是&quot;),(&quot;03&quot;,&quot;2&quot;,&quot;是&quot;);#查看数据select * from satisfaction;+------+------+-----------------+| tno | sno | satisfied |+------+------+-----------------+| 01 | 2 | 学生不满意 || 01 | 1 | 是 || 02 | 1 | 老师不满意 || 02 | 2 | 是 || 03 | 1 | 是 || 03 | 2 | 是 |+------+------+-----------------+#创建用户表create table sch_user( id varchar(10) not null primary key comment &quot;编号&quot;, in_not_sys varchar(64) comment &quot;是否在系统中&quot;, roles varchar(64) comment &quot;角色&quot;);#查看表desc sch_user;+------------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+------------+-------------+------+-----+---------+-------+| id | varchar(10) | NO | PRI | NULL | || in_not_sys | varchar(64) | YES | | NULL | || roles | varchar(64) | YES | | NULL | |+------------+-------------+------+-----+---------+-------+#插入数据insert into sch_user(id,in_not_sys,roles)values(&quot;1&quot;,&quot;是&quot;,&quot;学生&quot;),(&quot;2&quot;,&quot;是&quot;,&quot;学生&quot;),(&quot;01&quot;,&quot;是&quot;,&quot;教师&quot;),(&quot;02&quot;,&quot;否&quot;,&quot;教师&quot;),(&quot;03&quot;,&quot;是&quot;,&quot;教师&quot;);#查看数据select * from sch_user;+----+------------+--------+| id | in_not_sys | roles |+----+------------+--------+| 01 | 是 | 教师 || 02 | 否 | 教师 || 03 | 是 | 教师 || 1 | 是 | 学生 || 2 | 是 | 学生 |+----+------------+--------+ [思路] 读题：现需要分析学校里人员对课程的满意度 解题： 满意度信息在满意度表中，人员信息在用户表中，需要两表联结 满意度计算：(对课程都满意且在系统中的教师和学生) / (在系统中的所有用户) 限定条件1：是否在系统 - 是 限定条件2：满意度 - 是 找出在系统中的所有id，然后筛选满意度表中的数据，再进行计算 12345678910select sum(if(a.satisfied = &quot;是&quot;,1,0)) / count(a.satisfied) as &quot;课程满意度&quot;from satisfaction as awhere a.tno in (select id from sch_user where in_not_sys = &quot;是&quot;)and a.sno in (select id from sch_user where in_not_sys = &quot;是&quot;);+-----------------+| 课程满意度 |+-----------------+| 0.7500 |+-----------------+ 红包领取情况 [题目] 用户活跃表记录了用户的登录信息，字段有：登录日期、用户ID、新用户 其中新用户列的值为0、1；值为0为老用户、值为1为新用户 领取红包表里记录了用户领取红包的信息，字段有：抢红包日期、抢红包时间、用户id、金额 现需要分析以下问题： 计算2019年6月1日至今，每天DAU (即活跃用户，定义：有登录的用户) 分析每天领红包的用户数、人均领取金额、人均领取次数，其中还有用户属性以及领取红包未登录的情况 分析每个月按领红包取天数为1、2、3……30、31天区分，计算取每个月领取红包的用户数、人均领取金额、人均领取次数 分析每个月领过红包用户和未领红包用户的数量 12345678910111213create table dau_users( log_date varchar(64) comment &quot;登录日期&quot;, uid int comment &quot;用户id&quot;, new_old tinyint comment &quot;新用户&quot;);create table grabred( grab_date varchar(64) comment &quot;抢红包日期&quot;, grab_time datetime comment &quot;抢红包时间&quot;, uid int comment &quot;用户id&quot;, amount float comment &quot;金额&quot;); [解题1] 读题：计算2019年6月1日至今，每天DAU (即活跃用户，定义：有登录的用户) 解题 2019年6月1日到今天，限定条件为登录日期 &gt;= 2019年6月1日 统计每天的登录用户 1234select log_date,count(uid) as &quot;DAU-活跃用户&quot;from dau_userswhere log_date &gt;= &quot;20190601&quot;group by log_date; [解题2] 读题：分析每天领红包的用户数、人均领取金额、人均领取次数，其中还有用户属性以及领取红包未登录的情况 解题 每天领红包的用户分为三种：新用户、老用户以及领取红包但是未登录的用户，而新用户、老用户的数据统计在用户活跃表中，领取红包但是未登录的用户需要用两表进行联结比对 人均领取金额 = 领红包总金额 / 领红包总人数 (去重用户数) 人均领取次数 = 总领取次数 / 领红包总人数 (去重用户数) 12345678910111213141516select c.grabdate,count(distinct case when c.new_or_old = &quot;新用户&quot; then uid else null end) as &quot;日领新用户数&quot;,count(distinct case when c.new_or_old = &quot;老用户&quot; then uid else null end) as &quot;日领老用户数&quot;,count(distinct case when c.new_or_old = &quot;未登录用户&quot; then uid else null end) as &quot;日领未登录用户数&quot;,sum(c.amount)/ count(distinct c.uid) as &quot;人均领取金额&quot;,count(*)/ count(distinct c.uid) as &quot;人均领取次数&quot;from(select b.grabdate,b.uid,b.amount,( case when a.new_old = 1 then &quot;新用户&quot;, when a.new_old = 0 then &quot;老用户&quot;, else &quot;未登录用户&quot;end) as new_or_oldfrom dau_users as a right join grabred as bon a.log_date = b.grabdate and a.uid = b.uid) as cgroup by c.grabdate; [解题3] 读题：分析每个月按领红包取天数为1、2、3……30、31天区分，计算取每个月领取红包的用户数、人均领取金额、人均领取次数 解题： 按照月份进行分组，然后统计该月有多少天有领红包行为，因为领红包日期为字符串格式，所以选用领红包时间列使用month()函数进行分组 统计分组后每个月有多少用户领取红包，需要distinct 人均领取金额 = 该月份领取红包总金额 / 该月份领取红包用户数 (distinct) 人均领取次数 = 该月份总领取红包次数 / 该月份领取红包用户数 (distinct) 123456789101112select month(g.grab_time) as &quot;月份&quot;,count(distinct g.grab_time) as &quot;领取天数&quot;count(distinct uid) as &quot;用户数&quot;,sum(g.amount) / count(distinct uid) as &quot;人均领取金额&quot;,count(*) / count(distinct uid) as &quot;人均领取次数&quot;from grabred as ggroup by month(g.grab_time);#小问题1：这样显示月份并不准确，month()筛选分组后显示的只是月份，即03、04,如果有年份的区别，例如2019、2018这样的就会使数据不明确#解决：1、再取出年份进行拼接concat() ；2、直接用领红包日期来分组，因为是字符串，所以使用转换格式，然后在month()，或者截取前6个数字进行分组，这需要数据录入的格式以及准确性保持很高的要求 [解题4] 读题：分析每个月 领过红包用户和未领红包用户的数量 解题： 每个月，即按照月份来分组，month() 领过红包用户信息存在领红包表中，而未领红包用户存在用户活跃表中，需要两表联结进行比对，用户活跃表作为 left join 的左表 未领取红包用户 = 活跃用户 - 领红包用户 12345678910111213141516select month(c.grab_time),sum(case when 是否领过红包 = &quot;领过红包用户&quot; then 1 else 0 end) as &quot;领过红包用户数&quot;,sum(case when 是否领过红包 = &quot;未领过红包用户&quot; then 1 else 0 end) as &quot;未领过红包用户数&quot;from(select b.grab_time,a.uid,b.uid as &quot;领红包用户id&quot;, ( case when b.uid is not null then &quot;领过红包用户&quot; else &quot;未领过红包用户&quot; end) as &quot;是否领过红包&quot;from dau_users as a left join grabred as b on a.log_date = b.grab_date and a.uid = b.uid) as cgroup by month(c.grab_time);#注意点#1、还是跟问题3一样的月份函数的使用列问题；2、要注意联结后的列名在之后的语句中使用，即使用时要准确无误的引用 登录统计排序 [题目] 用户登录时间表中记录了用户登录的信息，字段有：用户id、姓名、邮箱、最后登录时间 现需要输出一张表，字段为：姓名、最后登录时间、登录时间排名、登录天数排名 登录时间排名：按时间给出每个人的登录次数，登录时间最早为1，依次排下去 登录天数排名：按天给出每个人的登录次数，同一天多次登录设定为同一次，最多天数标记为1，之后依次类推 1234567891011121314151617181920212223242526272829303132333435363738394041#创建表create table user_loginfo( uid int not null comment &quot;用户id&quot;, name varchar(64) comment &quot;姓名&quot;, email_addr varchar(64) comment &quot;邮箱地址&quot;, last_logtime datetime comment &quot;最后登录时间&quot;);#查看数据表desc user_loginfo;+--------------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+--------------+-------------+------+-----+---------+-------+| uid | int(11) | NO | | NULL | || name | varchar(64) | YES | | NULL | || email_addr | varchar(64) | YES | | NULL | || last_logtime | datetime | YES | | NULL | |+--------------+-------------+------+-----+---------+-------+#插入数据insert into user_loginfo(uid,name,email_addr,last_logtime)values (100,&quot;test4&quot;,&quot;test4@163.com&quot;,&quot;2007/11/25 16:31&quot;),(13,&quot;test1&quot;,&quot;test1@163.com&quot;,&quot;2007/3/22 16:27&quot;),(19,&quot;test1&quot;,&quot;test1@163.com&quot;,&quot;2007/10/25 14:13&quot;),(42,&quot;test1&quot;,&quot;test1@163.com&quot;,&quot;2007/10/25 14:20&quot;),(45,&quot;test2&quot;,&quot;test2@163.com&quot;,&quot;2007/4/25 14:17&quot;),(49,&quot;test2&quot;,&quot;test2@163.com&quot;,&quot;2007/5/25 14:22&quot;);#查看数据select * from user_loginfo;+-----+-------+---------------+---------------------+| uid | name | email_addr | last_logtime |+-----+-------+---------------+---------------------+| 100 | test4 | test4@163.com | 2007-11-25 16:31:00 || 13 | test1 | test1@163.com | 2007-03-22 16:27:00 || 19 | test1 | test1@163.com | 2007-10-25 14:13:00 || 42 | test1 | test1@163.com | 2007-10-25 14:20:00 || 45 | test2 | test2@163.com | 2007-04-25 14:17:00 || 49 | test2 | test2@163.com | 2007-05-25 14:22:00 |+-----+-------+---------------+---------------------+ [思路] 读题：现需要输出一张表，字段为：姓名、最后登录时间、登录时间排名、登录天数排名 登录时间排名：按时间给出每个人的登录次数，登录时间最早为1，依次排下去 登录天数排名：按天给出每个人的登录次数，同一天多次登录设定为同一次，最多天数标记为1，之后依次类推 解题(与出题者给出的答案理解不同，出于我自设定的业务场景)： 我觉得应该需要给出以下这样的列表 姓名 最后登录时间 最早登录时间 登录时间排名 登录天数 登录天数排名 test1 2007-05-25 14:22:00 2007-02-25 14:17:00 1 2 1 test2 2007-10-25 14:20:00 2007-03-22 16:27:00 2 2 1 test4 2007-11-25 16:31:00 2007-11-25 16:31:00 3 1 2 下面语句分开的原因是：这种方式语句需要对要进行排名的列进行先排序，而同时对两列进行排列，会影响第二列的排名不准确，暂时不知道如何处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445select a.name,a.最后登录时间,a.最早登录时间,(case when @predate = a.最早登录时间 then @rank when @predate := a.最早登录时间 then @rank := @rank + 1 end) as &quot;登录时间排名&quot;from(select name,max(last_logtime) as &quot;最后登录时间&quot;,min(last_logtime) as &quot;最早登录时间&quot;from user_loginfogroup by name) as a,(select @rank := 0 as &quot;rank&quot;,@predate := NULL as &quot;predate&quot;) as torder by a.最早登录时间;+-------+---------------------+---------------------+--------------------+| name | 最后登录时间 | 最早登录时间 | 登录时间排名 |+-------+---------------------+---------------------+--------------------+| test2 | 2007-05-25 14:22:00 | 2007-02-25 14:17:00 | 1 || test1 | 2007-10-25 14:20:00 | 2007-03-22 16:27:00 | 2 || test4 | 2007-11-25 16:31:00 | 2007-11-25 16:31:00 | 3 |+-------+---------------------+---------------------+--------------------+select a.name,a.最后登录时间,a.登录天数,(case when @predate = a.登录天数 then @rank when @predate := a.登录天数 then @rank := @rank + 1 end) as &quot;登录天数排名&quot;from(select name,max(last_logtime) as &quot;最后登录时间&quot;, count(distinct date(last_logtime)) as &quot;登录天数&quot;from user_loginfogroup by name) as a,(select @rank := 0 as &quot;rank&quot;,@pretimes := NULL as &quot;pretimes&quot;) as torder by a.登录天数 desc;+-------+---------------------+--------------+--------------------+| name | 最后登录时间 | 登录天数 | 登录天数排名 |+-------+---------------------+--------------+--------------------+| test1 | 2007-10-25 14:20:00 | 2 | 1 || test2 | 2007-05-25 14:22:00 | 2 | 1 || test4 | 2007-11-25 16:31:00 | 1 | 2 |+-------+---------------------+--------------+--------------------+ 持续更新中～","link":"/2021/10/06/MySQL%E5%9C%BA%E6%99%AF%E6%A8%A1%E6%8B%9F%E5%88%9D%E7%BA%A7%E7%AF%87/"},{"title":"MySQL数据库 - 初步认识","text":"摘要有关MySQL数据库的基础知识，包括一些专用名称解析以及必会知识点，有待更正与完善～ 初识MySQL数据库我们在编写任何程序之前，都需要事先写好基于网络操作一台主机上文件的程序（socket服务端与客户端程序），于是有人将此类程序写成一个专门的处理软件，这就是mysql等数据库管理软件的由来，但mysql解决的不仅仅是数据共享的问题，还有查询效率，安全性等一系列问题。 总之，把程序员从数据管理中解脱出来，专注于自己的程序逻辑的编写~。 专有名词数据（Data）：描述事物的符号记录称为数据，描述事物的符号既可以是数字，也可以是文字、图片，图像、声音、语言等，数据由多种表现形式，它们都可以经过数字化后存入计算机；在计算机中描述一个事物，就需要抽取这一事物的典型特征，组成一条记录，就相当于文件里的一行内容。 数据库（Databases，简称DB）：数据库库即存放数据的仓库，只不过这个仓库是在计算机存储设备上，而且数据是按一定的格式存放的；数据库是长期存放在计算机内、有组织、可共享的数据即可。数据库中的数据按一定的数据模型组织、描述和储存，具有较小的冗余度、较高的数据独立性和易扩展性，并可为各种用户共享。 数据库管理系统（DataBase Management System 简称DBMS）在了解了Data与DB的概念后，如何科学地组织和存储数据，如何高效获取和维护数据成了关键~~ 这就用到了一个系统软件—数据库管理系统如MySQL、Oracle、SQLite、Access、MS SQL Server 常见的数据库模型分为关系型数据库（MySQL、Oracle、SQL Server….）和非关系型数据库（文档存储数据库MongoDB；键值存储数据库Redis、Memcached、列存储数据库HBase、图形数据库Neo4J） 数据类型 数字： 整型： tinyint [(m)] [unsigned] [zerofill]：小整数，数据类型用于保存一些范围的整数 数值范围： ​ 有符号：-128 ～ 127 ​ 无符号：0 ～ 255 注意： MySQL中无布尔值，使用tinyint(1)构造。 int [(m)] [unsigned] [zerofill]：整数，数据类型用于保存一些范围的整数 数值范围： ​ 有符号：-2147483648 ～ 2147483647 ​ 无符号：0 ～ 4294967295 bigint [(m)] [unsigned] [zerofill]：大整数，数据类型用于保存一些范围的整数 数值范围： ​ 有符号：-9223372036854775808 ～ 9223372036854775807 ​ 无符号：0 ～ 18446744073709551615 zerofill 使用说明：例如 int(5)表示当数值宽度小于 5 位的时候在数字前面加’0’填满宽度,如果不显示指定宽度则默认为 int(11)，zerofill**默认为int(10)**。 注:当使用zerofill 时，默认会自动加unsigned（无符号）属性，使用unsigned属性后，数值范围是原值的2倍，例如，有符号为-128～+127，无符号为0~256。 小数：m是数字总个数，d是小数点后个数。m最大值为255，d最大值为30。 ​ float [(M,D)] [unsigned] [zerofill]：单精度浮点数（非准确小数值） ​ 特性：随着小数的增多，精度变得不准确 ​ double [(M,D)] [unsigned] [zerofill]：双精度浮点数（非准确小数值）。 ​ 特性：随着小数的增多，精度比float要高，但也会变得不准确 ​ decimal [(M[,D])] [unsigned] [zerofill]：准确的小数值。 ​ m是数字总个数（负号不算），d是小数点后个数。 m最大值为65，d最大值为30。 ​ 特性：随着小数的增多，精度始终准确。decaimal能够存储精确值的原因在于其内部按照字符串存储。 字符串 char（10）：定长 不够的给你补上。特点：简单粗暴，浪费空间，存取速度快。 varchar：变长 传几个给你写几个，但不要超过字符个数精准。特点：节省空间，但存取速度慢。 大于255字符，可以考虑将文件路径存放到数据库中，即数据库中只存路径或者url。 时间类型 year ：年 date ：年月日 time ：时分秒 datetime ：年月日时分秒 枚举类型与集合类型：字段的值只能在给定范围中选择，如单选框，多选框 enum 单选 只能在给定的范围内选一个值，如性别 sex 男male/女female set 多选 在给定的范围内可以选择一个或一个以上的值（爱好1,爱好2,爱好3…） 索引索引简介索引（Index）是帮助MySQL高效获取数据的数据结构。可以得到索引的本质：索引是数据结构。 数据本身之外，数据库还维护着一个满足特定查找算法的数据结构，这些数据结构以某种方式指向数据，这样就可以在这些数据结构的基础上实现高级查找算法，这种数据结构就是索引。 所以，索引就是一种帮助MySQL高效获取数据的排好序的快速查找的数据结构。 索引优劣 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上 优势：类似大学图书馆建书目索引，提高数据检索的效率，降低数据库的IO成本；通过索引列对数据进行排序，降低数据排序的成本，降低了CPU的消耗 劣势：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，都会调整因为更新所带来的键值变化后的索引信息；实际上索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录，所以索引列也是要占用空间的。 索引结构 各种结构探寻：https://www.cs.usfca.edu/~galles/visualization/Algorithms.html 二叉树（红黑树：也叫二叉平衡树） HASH B - TREE 度(Degree)-节点的数据存储个数 叶节点具有相同的深度 叶节点的指针为空 节点中的数据key从左到右递增排列 B + TREE（B - TREE演变） 非叶子节点不存储data,只存储key,可以增大度 叶子节点不存储指针 顺序访问指针,提高区间访问的性能 B+Tree索引的性能分析一般使用磁盘I/O次数评价索引结构的优劣，根据索引获取一条数据使用的I/O次数越少越优。 预读：磁盘一般会顺序向后读取一定长度的数据(页的整数倍)放入内存 局部性原理：当一个数据被用到时,其附近的数据也通常会马上被使用 B+Tree节点的大小设为等于一个页,每次新建节点之间申请一个页的空间，这样就保证了一个节点物理上页存储在一个页里就实现了一个节点的载入只需一次I/O B+Tree的度d一般会超过100,因此h非常小(一般为1到3之间,极限到5) 一般操作系统的最小存储单元为页,1页大小为4K ‘SHOW GLOBAL STATUS like ‘Innodb_page_size’语句可以查看mysql文件页大小 索引分类 单值索引：即一个索引只包含单个列，一个表可以有多个单列索引 唯一索引：索引列的值必须唯一，但允许有空值 主键索引：设定为主键后数据库会自动建立索引，innodb为聚簇索引 复合索引：即一个索引包含多个列 MySQL常见存储引擎 MyISAM索引实现(非聚集) MyISAM索引文件和数据文件时分离的 InnoDB索引实现(聚集) 表数据文件本身就是按B+Tree组织的一个索引结构文件 聚集索引 - 叶节点包含了完整的数据记录 InnoDB表必须有主键,并且推荐使用整型的自增主键 非主键索引结构叶子节点存储的时主键值(一致性和节省存储空间) 完整型约束完整型约束的作用：用于保证数据的完整性和一致性 是否允许为空，默认NULL，可设置NOT NULL，字段不允许为空，必须赋值 字段是否有默认值，缺省的默认值是NULL，如果插入记录时不给字段赋值，此字段使用默认值 是否为key ：主键 primary key、外键 foreign key、索引 (index,unique…) PRIMARY KEY (PK) 标识该字段为该表的主键，可以唯一的标识记录（不为空且唯一） FOREIGN KEY (FK) 标识该字段为该表的外键 NOT NULL 标识该字段不能为空 UNIQUE KEY (UK) 标识该字段的值是唯一的 单列唯一 ：在字段后加unique，指的是这个字段的记录是唯一的不能重复 联合唯一 ：例如ip和端口均是唯一的，这种叫联合唯一 AUTO_INCREMENT 标识该字段的值自动增长（整数类型，而且为主键） DEFAULT 为该字段设置默认值 查询优化单表使用索引以及常见的索引失效 全值匹配 最佳左前缀法则：如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。 不在索引列上做任何操作（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描 存储引擎不能使用索引中范围条件右边的列； mysql 在使用不等于(!= 或者&lt;&gt;)的时候无法使用索引会导致全表扫描； is not null 也无法使用索引,但是is null是可以使用索引的； like以通配符开头(‘%abc…’)mysql索引失效会变成全表扫描的操作； 字符串不加单引号也会引起索引失效 建议： 对于单键索引，尽量选择针对当前query过滤性更好的索引；在选择组合索引的时候，当前Query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。 在选择组合索引的时候，尽量选择可以能够包含当前query中的where字句中更多字段的索引； 在选择组合索引的时候，如果某个字段可能出现范围查询时，尽量把这个字段放在索引次序的最后面； 关联查询优化 保证被驱动表的join字段已经被索引 left join时，选择小表作为驱动表（也就是主表），大表作为被驱动表（从表） inner join时，mysql会自动将小结果集的表选为驱动表 子查询尽量不要放在被驱动表，有可能使用不到索引 能够直接多表关联的尽量直接关联，不使用子查询 子查询优化 尽量不要使用not in 或者 not exists，用left outer join on xxx is null 替代； 排序分组优化～待补充 pymysql使用1234567891011121314151617181920212223242526272829303132333435import pymysqluser=input('user&gt;&gt;: ').strip()pwd=input('password&gt;&gt;: ').strip()# 建立链接conn=pymysql.connect( host='192.168.1.123', port=3306, user='root', password='123', db='db10', charset='utf8')# 拿到游标cursor=conn.cursor()# 执行sql语句# sql='select * from userinfo where user = &quot;%s&quot; and pwd=&quot;%s&quot;' %(user,pwd)# print(sql)# rows=cursor.execute(sql)sql='select * from userinfo where user = %s and pwd=%s'#由execute作为拼接，不用你自己去拼接了，在拼接过程中给你过滤掉这种非法操作rows=cursor.execute(sql,(user,pwd)) #提交给游标执行 execute这个接口拿到的是2 rows in set (0.00 sec) 2那个行数，如果值不为0说明就输对了cursor.close()conn.close()# 进行判断if rows: print('登录成功')else: print('登录失败') 写在后面以上是学习MySQL数据库的一下学习笔记，记录下来以供随时翻阅，查漏补缺，感谢阅读～","link":"/2020/08/03/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%20-%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86/"},{"title":"MySQL练习 - 基础篇(持续更新ing)","text":"摘要MySQL练习题，一些常用的基础知识点的练习.. MySQL - 简易篇 重复值查找 [题目] 编写一个SQL查询，查找学生表中所有重复的学生名。 1234567891011121314151617181920212223242526272829303132# 建表create table exerciseOne( id int not null auto_increment, name varchar(64) not null, primary key(id));desc exerciseOne;+-------+-------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(64) | NO | | NULL | |+-------+-------------+------+-----+---------+----------------+# 插入数据insert into exerciseOne(id,name) values (001,'zhangsan'),(002,'ali'),(003,'luox'),(004,'luox'),(005,'ali');# 数据查看select * from exerciseOne;+----+----------+| id | name |+----+----------+| 1 | zhangsan || 2 | ali || 3 | luox || 4 | luox || 5 | ali |+----+----------+ [思路] 读题：查找学生表中所有重复的学生名 解题：以学生名分组，筛选出各个分组计数大于1的学生名 123456789select name from exerciseOne group by name having count(name) &gt; 1;+------+| name |+------+| ali || luox |+------+ 寻找第N高的数据 [题目] exercise2 表中，记录了学生选修课程的名称以及成绩 找出语文课中成绩第二高的学生成绩，如果不存在第二高成绩的学生，那么查询应返回 null 。 12345678910111213141516171819202122232425262728293031323334353637# 建表create table exercise2( id int not null comment '学号', course varchar(64) not null comment '课程', score int comment '成绩');# 查看desc exercise2;+--------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+--------+-------------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || course | varchar(64) | NO | | NULL | || score | int(11) | YES | | NULL | |+--------+-------------+------+-----+---------+-------+# 插入数据insert into exercise2(id,course,score)values(1,&quot;语文&quot;,90),(1,&quot;数学&quot;,65),(2,&quot;语文&quot;,68),(2,&quot;数学&quot;,96),(3,&quot;数学&quot;,55);# 查看数据select * from exercise2;+----+--------+-------+| id | course | score |+----+--------+-------+| 1 | 语文 | 90 || 1 | 数学 | 65 || 2 | 语文 | 68 || 2 | 数学 | 96 || 3 | 数学 | 55 |+----+--------+-------+ [思路] 读题：找出语文课中成绩第二高的学生成绩，如果不存在第二高成绩的学生，那么查询应返回 null 。 解题 找出所有选修了”语文”课的学生成绩 进行排序并选择第二高成绩 特殊情况：如果不存在第二高的成绩，返回null 123456789101112select ifnull(( select distinct score from exercise2 where course = &quot;语文&quot; order by score desc limit 1,1),null) as &quot;语文第二高成绩&quot; ;+-----------------------+| 语文第二高成绩 |+-----------------------+| 68 |+-----------------------+ 123456789101112131415161718192021222324252627# 验证特殊情况insert into exercise2(id,course,score) values (1,&quot;英语&quot;,77),(2,&quot;英语&quot;,77);+----+--------+-------+| id | course | score |+----+--------+-------+| 1 | 语文 | 90 || 1 | 数学 | 65 || 2 | 语文 | 68 || 2 | 数学 | 96 || 3 | 数学 | 55 || 1 | 英语 | 77 || 2 | 英语 | 77 |+----+--------+-------+select ifnull(( select distinct score from exercise2 where course = &quot;英语&quot; order by score desc limit 1,1),null) as &quot;英语第二高成绩&quot; ;+-----------------------+| 英语第二高成绩 |+-----------------------+| NULL |+-----------------------+ 多表联接 [题目] 现在有两个表： 学生表：记录了学生的基本信息，字段有 “学号”、“姓名” 成绩表：记录了学生选修的课程，以及对于课程的成绩，字段对应为 “课程”、“成绩” 两张表通过 “学号” 进行关联 现在需要查找出所有的学生的学号、姓名、课程和成绩。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# 建学生表create table student_lx3( id int not null comment &quot;学号&quot;, name varchar(64) not null comment &quot;姓名&quot;);# 查看desc student_lx3;+-------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || name | varchar(64) | NO | | NULL | |+-------+-------------+------+-----+---------+-------+# 插入数据insert into student_lx3(id,name)values (1,&quot;张三&quot;),(2,&quot;赵四&quot;),(3,&quot;李五&quot;),(4,&quot;王六&quot;);# 查看数据select * from student_lx3;+----+--------+| id | name |+----+--------+| 1 | 张三 || 2 | 赵四 || 3 | 李五 || 4 | 王六 |+----+--------+#建成绩表create table score_lx3( id int not null comment &quot;学号&quot;, course varchar(64) comment &quot;课程&quot;, score int comment &quot;成绩&quot;);# 查看desc score_lx3;+--------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+--------+-------------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || course | varchar(64) | YES | | NULL | || score | int(11) | YES | | NULL | |+--------+-------------+------+-----+---------+-------+# 插入数据insert into score_lx3(id,course,score) values (1,&quot;语文&quot;,90),(1,&quot;数学&quot;,67),(2,&quot;语文&quot;,88),(3,&quot;数学&quot;,99),(3,&quot;英语&quot;,77);# 查看数据select * from score_lx3;+----+--------+-------+| id | course | score |+----+--------+-------+| 1 | 语文 | 90 || 1 | 数学 | 67 || 2 | 语文 | 88 || 3 | 数学 | 99 || 3 | 英语 | 77 |+----+--------+-------+# 为保证数据的有效性和完整性，添加约束(外键约束)，在多表的一方添加外键约束# 添加了外键约束之后的特征：# - 主表中不能删除从表中已引用的数据# - 从表中不能添加主表中不存在的数据 [思路] 读题：查找所有学生的学号、姓名、课程和成绩。 解题： 其中 “学号” &amp; “姓名” 在student_lx3表中，**”课程”** &amp; “成绩” 则在score_lx3表中 需要 多表联结 进行查询，联结条件为 “学号” 多表联结： 内联结：根据条件取两表的公共数据 左外联结：取 join 左表的所有数据，根据条件关联查询 join 右表的数据，不符合条件以null值填充 右外联结：取 join 右表的所有数据，根据条件关联查询 join 左表的数据，不符合条件以null值填充 1234567891011121314select a.id,a.name,b.course,b.score from student_lx3 as a left join score_lx3 as b on a.id = b.id;+----+--------+--------+-------+| id | name | course | score |+----+--------+--------+-------+| 1 | 张三 | 语文 | 90 || 1 | 张三 | 数学 | 67 || 2 | 赵四 | 语文 | 88 || 3 | 李五 | 数学 | 99 || 3 | 李五 | 英语 | 77 || 4 | 王六 | NULL | NULL |+----+--------+--------+-------+ 多表联接 - 外联接 [题目] 现在有两个表： 学生表：记录了学生的基本信息，字段有 “学号”、“姓名” 近视学生表：记录了序号以及学生学号 两张表通过学号进行关联 现在需要找出不是近视的学生都有哪些？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#建学生表create table student_lx4( id int not null comment &quot;学号&quot;, name varchar(64) not null comment &quot;姓名&quot;); # 查看desc student_lx4;+-------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || name | varchar(64) | NO | | NULL | |+-------+-------------+------+-----+---------+-------+# 插入数据insert into student_lx4(id,name) values (0001,&quot;周周&quot;),(0002,&quot;李李&quot;),(0003,&quot;王网&quot;),(0004,&quot;张张&quot;),(0005,&quot;猴后&quot;);#查看数据select * from student_lx4;+----+---------+| id | name |+----+---------+| 1 | 周周 || 2 | 李李 || 3 | 王网 || 4 | 张张 || 5 | 猴后 |+----+---------+#建近视学生表create table jinshi( id int not null comment &quot;序号&quot;, s_id int not null comment &quot;学生学号&quot;); #查看数据表desc jinshi;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || s_id | int(11) | NO | | NULL | |+-------+---------+------+-----+---------+-------+#插入数据insert into jinshi(id,s_id) values (1,0001),(2,0002),(3,0004);#查看数据select * from jinshi;+----+------+| id | s_id |+----+------+| 1 | 1 || 2 | 2 || 3 | 4 |+----+------+ [思路] 读题：不是近视的学生都有谁？ 解题： 学生表信息包含近视学生表的信息，现在要找出不在近视学生表中但在学生表中的数据，使用left join，学生表为左表 根据笛卡尔乘积，可将筛选条件设定为left join 后近视学生表序号为空的数据行 1234567891011select st.id,st.namefrom student_lx4 as st left join jinshi as js on st.id = js.s_idwhere js.id is null;+----+--------+| id | name |+----+--------+| 3 | 王网 || 5 | 猴后 |+----+--------+ 行列转换 [题目] cook表记录了三个字段，分别是年、月、值 现需要进行行列转换为：年、m1、m2、m3、m4 123456789101112131415161718192021222324252627282930313233343536373839404142#创建表create table cook( year int comment &quot;年&quot;, month int comment &quot;月&quot;, price float comment &quot;值&quot;);#查看表desc cook;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| year | int(11) | YES | | NULL | || month | int(11) | YES | | NULL | || price | float | YES | | NULL | |+-------+---------+------+-----+---------+-------+#插入数据insert into cook(year,month,price) values(2009,1,1.1),(2009,2,1.2),(2009,3,1.3),(2009,4,1.4),(2010,1,2.1),(2010,2,2.2),(2010,3,2.3),(2010,4,2.4);#查看数据select * from cook;+------+-------+-------+| year | month | price |+------+-------+-------+| 2009 | 1 | 1.1 || 2009 | 2 | 1.2 || 2009 | 3 | 1.3 || 2009 | 4 | 1.4 || 2010 | 1 | 2.1 || 2010 | 2 | 2.2 || 2010 | 3 | 2.3 || 2010 | 4 | 2.4 |+------+-------+-------+ [思路] 读题：行列转换为：年、m1、m2、m3、m4 解题： 按照年进行分组，得到年这一列 用case when将原先月这列进行赋值转换，值为原先的”值”一列，并生成新的m1、m2、m3、m4列 去除为0的项，可以直接使用max函数，前提是原先”值”该列类型为数值 123456789101112131415select year,max(case when month = &quot;1&quot; then price else 0 end) as &quot;m1&quot;,max(case when month = &quot;2&quot; then price else 0 end) as &quot;m2&quot;,max(case when month = &quot;3&quot; then price else 0 end) as &quot;m3&quot;,max(case when month = &quot;4&quot; then price else 0 end) as &quot;m4&quot;from cookgroup by year;+------+------------+------------+------------+------------+| year | m1 | m2 | m3 | m4 |+------+------------+------------+------------+------------+| 2009 | 1.1 | 1.2 | 1.3 | 1.4 || 2010 | 2.1 | 2.2 | 2.3 | 2.4 |+------+------------+------------+------------+------------+ MySQL5.7中的排名 MySQL 5.7实现排名(MySQL8.0 中的rank()、row_number()) [题目] 用户访问表中记录了用户的访问信息，字段有：用户编号、用户类型、访问次数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#创建表create table visits( uid int not null comment &quot;用户编号&quot;, utype varchar(10) comment &quot;用户类型&quot;, vtimes int comment &quot;访问次数&quot;);#插入数据insert into visits(uid,utype,vtimes) values (10,&quot;A&quot;,352),(6,&quot;C&quot;,209),(7,&quot;C&quot;,110),(4,&quot;E&quot;,101),(2,&quot;B&quot;,53),(20,&quot;A&quot;,53),(11,&quot;C&quot;,32),(1,&quot;A&quot;,30),(9,&quot;E&quot;,29),(8,&quot;B&quot;,6);#有相同数不并列排名 - 8.0中的row_number()select v.uid,v.utype,v.vtimes,@rank := @rank + 1 as &quot;rank&quot;from visits as v,(select @rank :=0 as &quot;rank&quot;) torder by v.vtimes desc;+-----+-------+--------+------+| uid | utype | vtimes | rank |+-----+-------+--------+------+| 10 | A | 352 | 1 || 6 | C | 209 | 2 || 7 | C | 110 | 3 || 4 | E | 101 | 4 || 2 | B | 53 | 5 || 20 | A | 53 | 6 || 11 | C | 32 | 7 || 1 | A | 30 | 8 || 9 | E | 29 | 9 || 8 | B | 6 | 10 |+-----+-------+--------+------+#有相同数时同排名，并跳跃排名 - 8.0中的rank()select v.uid,v.utype,v.vtimes,@rownum := @rownum + 1 as &quot;rownum&quot;,( case when @prevtimes = v.vtimes then @rank when @prevtimes := v.vtimes then @rank := @rownum end) as &quot;rank&quot;from visits as v,(select @rownum := 0 as &quot;rownum&quot;, @rank := 0 as &quot;rank&quot;, @prevtimes := NULL as &quot;prevtimes&quot; ) torder by v.vtimes desc;+-----+-------+--------+--------+------+| uid | utype | vtimes | rownum | rank |+-----+-------+--------+--------+------+| 10 | A | 352 | 1 | 1 || 6 | C | 209 | 2 | 2 || 7 | C | 110 | 3 | 3 || 4 | E | 101 | 4 | 4 || 2 | B | 53 | 5 | 5 || 20 | A | 53 | 6 | 5 || 11 | C | 32 | 7 | 7 || 1 | A | 30 | 8 | 8 || 9 | E | 29 | 9 | 9 || 8 | B | 6 | 10 | 10 |+-----+-------+--------+--------+------+#有相同数时，连续排名 - 8.0中的dense_rank()select v.*,(case when @prevtimes = v.vtimes then @rank when @prevtimes := v.vtimes then @rank := @rank +1 end) as &quot;rank&quot;from visits as v,(select @rank := 0 as &quot;rank&quot;,@prevtimes := NULL as &quot;prevtimes&quot;) torder by v.vtimes desc;+-----+-------+--------+------+| uid | utype | vtimes | rank |+-----+-------+--------+------+| 10 | A | 352 | 1 || 6 | C | 209 | 2 || 7 | C | 110 | 3 || 4 | E | 101 | 4 || 2 | B | 53 | 5 || 20 | A | 53 | 5 || 11 | C | 32 | 6 || 1 | A | 30 | 7 || 9 | E | 29 | 8 || 8 | B | 6 | 9 |+-----+-------+--------+------+ 持续更新中～","link":"/2021/10/03/MySQL%E7%BB%83%E4%B9%A0%E7%AE%80%E6%98%93%E7%AF%87/"},{"title":"Seaborn - 基础风格展示 &amp; 调色盘","text":"摘要Seaborn库的使用，包括一些基础风格的参数设置，以及调色盘(各色系的设置)～ 写在前面Matplotlib试着让简单的事情更加简单，困难的事情变得可能，而Seaborn就是让困难的东西更加简单。 实际上，Seaborn 是在matplotlib的基础上进行了更高级的 API 封装，从而使得作图更加容易 用Matplotlib最大的困难是其默认的各种参数，而Seaborn则完全避免了这一问题。 seaborn是针对统计绘图的，一般来说，seaborn能满足数据分析大部分的基本绘图需求，就能做出很具有吸引力的图，如果需要复杂的自定义图形，还是要Matplotlib。 官网：http://seaborn.pydata.org/index.html 安装：直接 pip3 install seaborn 即可 seaborn风格设置 Seaborn模块自带许多定制的主题和高级的接口，包括对图表整体颜色、比例等进行风格设置,包括颜色色板等，调用其系统风格即可实现各种不同风格的数据可视化 导入模块 本文基于jupyter notebook环境，先导入使用到的python模块，并创建一个正弦函数及绘制图表 12345import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 1234567# 创建正弦函数及图表def sinplot(flip = 1): x = np.linspace(0,14,100) for i in range(1,7): plt.plot(x,np.sin(x + i * .5) * (7 - i) * flip)sinplot() set set( ) ：通过设置参数可以用来设置背景，调色板等，最为常用。 123456# tips：在jupyter notebook中，一旦设定了风格,所有图表的创建均自动带有风格,重启服务后才会重置为无风格状态# 使用默认设置sns.set()sinplot()plt.grid(linestyle = '--') set_style set_style() ：设置主题，即切换seaborn图表风格 Seaborn有五个预设好的主题： darkgrid , whitegrid , dark , white ,和 ticks 默认： darkgrid 12345678910111213141516fig = plt.figure(figsize = (6,6))# whitegrid主题sns.set_style('whitegrid')ax1 = fig.add_subplot(2,1,1)data = np.random.normal(size = (20,6)) + np.arange(6) / 2sns.boxplot(data = data)plt.title('style -- whitegrid')# dark主题sns.set_style('dark')ax2 = fig.add_subplot(2,1,2)sinplot() despine despine() ：设置图表坐标轴，可以根据需求将坐标轴的展现与否进行设置，更好的讲故事～ 12345678910111213141516171819202122232425262728293031# despine() -- 设置图表坐标轴# sns.despine('fig=None', 'ax=None', 'top=True', 'right=True', 'left=False', 'bottom=False', 'offset=None', 'trim=False')sns.set_style('ticks')fig = plt.figure(figsize = (6,9))plt.subplots_adjust(hspace=0.3)ax1 = fig.add_subplot(3,1,1)sinplot()# 删除上、右坐标轴sns.despine()# 创建小提琴图ax2 = fig.add_subplot(3,1,2)sns.violinplot(data = data)# sns.despine(offset = 10,trim = True)# offset: 与坐标轴之间的偏移# trim：为True时,将坐标轴限制在数据最大值最小值ax3 = fig.add_subplot(3,1,3)sns.boxplot(data = data,palette='deep')# sns.despine(left = True,right = False)# top,right,left,bottom: 布尔型,为True时不显示 1&lt;matplotlib.axes._subplots.AxesSubplot at 0xeddb830&gt; axes_style axes_style() ：设置局部图表风格，配合with使用可以很方便的将想要表达的图表更加凸显出来 1234567891011121314# with：只在sns这个图表中设置风格，其他图表风格还是与之前设置的一致# 设置局部图表风格,用with做代码块区分with sns.axes_style('darkgrid'): plt.subplot(211) sinplot()# 设置外部表格风格sns.set_style('whitegrid')plt.subplot(212)sinplot() set_context set_context() ：设置显示的比例尺度，方便在不用显示器或不同分辨率下设置不同的显示比例 seaborn内置四种显示比例：’paper’ ‘notebook’ ‘talk’ ‘poster’，左往右依次 变大 1234# 默认为notebooksns.set_context('talk')sinplot() 调色盘 seaborn的调色盘用于对图表整体颜色、比例等进行风格设置,包括颜色色板等 调色盘分为三类 Sequential：按顺序渐变的。 - Light colours for low data, dark for high data Diverging：彼此之间差异变化较大的。 - Light colours for mid-range data, low and high contrasting dark colours Qualitative：这个用于最大程度地显示不同类之间的差别。 - Colours designed to give maximum visual difference between classes 一般调用seaborn内置系统风格和简单设置参数即能实现很炫酷的数据可视化 color_palette seaborn.color_palette(palette=None, n_colors =None, desat =None) palette：None，string或sequence，可选，默认有6种主题：deep,muted, pastel, bright, dark, colorblind n_colors：颜色个数 desat：每种颜色去饱和的比例 12current_palette = sns.color_palette()sns.palplot(current_palette) 123456# sns.color_palette(pal, size=1)# pal → 颜色风格, size → 颜色色块个数# Reds/Reds_r 代表 颜色风格反转(不是所有颜色都可以反转)sns.palplot(sns.color_palette('hls',8)) 123# 分组颜色设置 - 'Paired'sns.palplot(sns.color_palette('Paired',8)) husl_palette 设置亮度和饱和度 1234567# 设置亮度、饱和度# 1、husl_palette([n_colors,h,s,l])# 2、hls_palette([n_colors,h,s,l])# l - 亮度、s - 饱和度sns.palplot(sns.hls_palette(8,l = .7,s =1)) cubehelix_palette 按照线性增长计算,设置颜色 123456789101112131415# sns.cubehelix_palette('n_colors=6', 'start=0', 'rot=0.4', 'gamma=1.0', 'hue=0.8', # 'light=0.85', 'dark=0.15', 'reverse=False', 'as_cmap=False')sns.palplot(sns.cubehelix_palette(8,gamma = 2))sns.palplot(sns.cubehelix_palette(8,start = 1.2,rot = -.5))sns.palplot(sns.cubehelix_palette(8,start = 2,rot = 0,dark = 0,light = .95,reverse = True))# 参数解析：# n_colors：颜色个数# start：值区间在0 - 3 , 开始颜色# rot：颜色旋转角度# gamma：颜色伽马值,值越大颜色越暗# dark,light：值区间在0 - 1,颜色深浅# reverse：布尔值,默认为False,由浅到深 dark_palette / light_palette 深色/浅色调色板 seaborn.dark_palette（color，n_colors =6，reverse =False，as_cmap = False，input =’rgb’） color ：高值的颜色 n_colors ：颜色个数 reverse ：默认为False as_cmap ：如果为True，则返回matplotlib colormap；为False，则返回list input：{‘rgb’，’hls’，’husl’，’xkcd’} 12345678910111213141516# dark_palette() / light_palette()# 按照green做浅色调色盘sns.palplot(sns.light_palette('green'))# 按照green做深色调色盘sns.palplot(sns.dark_palette('green',reverse = True))# 设置cmap为Greens风格sns.palplot(sns.color_palette('Greens'))# reverse → 转制颜色sns.palplot(sns.color_palette('Greens_r')) diverging_palette diverging_palette ：创建分散颜色 1234567891011121314# 创建分散颜色# sns.diverging_palette('h_neg', 'h_pos', 's=75', 'l=50', 'sep=10', 'n=6', # &quot;center='light'&quot;, 'as_cmap=False')sns.palplot(sns.diverging_palette(145,280,s = 85,l = 25,n = 7))# 参数解析:# h_neg, h_pos：起始/终止颜色值# s ：值区间0 - 100,饱和度# l ：值区间0 - 100,亮度# n ：颜色个数# center ：中心颜色为浅色还是深色'light' , 'dark' ,默认为light 应用热力图12345678# 例子plt.figure(figsize = (12,8))x = np.arange(25).reshape(5,5)cmap = sns.diverging_palette(200,20,sep = 20,as_cmap = True)sns.heatmap(x,cmap=cmap)# heatmap : 热力图 &lt;matplotlib.axes._subplots.AxesSubplot at 0x5b28c50&gt; 风格演示123456789sns.set_style('whitegrid')with sns.color_palette('PuBuGn_d'): plt.subplot(211) sinplot()sns.set_palette('husl')plt.subplot(212)sinplot() 总结本文介绍了python的绘图模块seaborn的基础认知，也就是绘图的基础风格设置与调色盘的基础知识，感谢阅读 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/11/Seaborn%E9%A3%8E%E6%A0%BC%E8%AE%BE%E7%BD%AE%E4%B8%8E%E8%B0%83%E8%89%B2%E7%9B%98/"},{"title":"博客文章属性","text":"摘要博客文章属性解析.. post 模板本主题下post模板内容 -– thumbnail: title: 博客文章属性 date: 1615769914000 tags: -test1 -test2 categories: -test3 -test4 toc: true recommend: 1 keywords: categories-java uniqueId: 1615769914000/博客文章属性.html mathJax: false -– &gt; 摘要 首页显示摘要内容（替换成自己的） 正文内容（替换成自己的） 参考文章: 参考链接 参数 参数 名称解析 thumbnail 文首缩略图，值：图片url title 文章标题 date 文章创建日期 tags 标签，无顺序，无层级 categories 分类，有顺序，体现分类层级，例：[python,library] toc 启用章内索引，值：true/false recommend 文章推荐，值必须大于0,且值越大越靠前，相等取最新，最多5条 keywords categories-java uniqueId 1615769914000/博客文章属性.html mathJax 数学公式渲染，值：false/true encrypt 文章加密，值：false/true；加密文章最好设置top：-1,将其排在最后 &lt;!--more--&gt; 文章折叠 参考文章: 参考链接 - Icarus用户指南","link":"/2021/03/15/blog_post/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/11/29/hello-world/"},{"title":"一条MySQL语句的执行详析","text":"摘要从常用的MySQL关键字到MySQL语句的书写，再到MySQL语句的执行，一步步深入学习MySQL～ MySQL常用的关键字 必选字段：SELECT、FROM 可选字段：DISTINCT、JOIN、ON、WHERE、GROUP BY、HAVING、ORDER BY、LIMIT 聚合函数：COUNT(计数)、SUM(求和)、max(最大值)、min(最小值)、avg(平均值)、group_concat(字符串连接) #COUNT(1) &amp; COUNT(COLUMN)区别 当数据集中不包含NULL值时，两者表现一致；当出现NULL值时，COUNT(COLUMN)不将NULL值所在行计入总数，相当于进行非NULL值计数。 #实例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647CREATE DATABASE cnt_exp CHARSET utf8mb4;USE cnt_exp;CREATE TABLE stu( id int, name varchar(10), classid int); INSERT into stu values (1,'A',1),(2,'B',1),(3,'C',2),(4,'D',2),(5,'A',1),(6,'F',1),(7,NULL,2);SELECT * FROM stu;+------+------+---------+| id | name | classid |+------+------+---------+| 1 | A | 1 || 2 | B | 1 || 3 | C | 2 || 4 | D | 2 || 5 | A | 1 || 6 | F | 1 || 7 | NULL | 2 |+------+------+---------+SELECT COUNT(1),classid FROM stu GROUP BY classid;+----------+---------+| count(1) | classid |+----------+---------+| 4 | 1 || 3 | 2 |+----------+---------+SELECT COUNT(name),classid FROM stu GROUP BY classid;+-------------+---------+| count(name) | classid |+-------------+---------+| 4 | 1 || 2 | 2 |+-------------+---------+SELECT COUNT(DISTINCT name),classid FROM stu GROUP BY classid;+----------------------+---------+| COUNT(DISTINCT name) | classid |+----------------------+---------+| 3 | 1 || 2 | 2 |+----------------------+---------+ MySQL书写顺序常见的MySQL语句 SELECT -&gt; DISTINCT -&gt; FROM -&gt; JOIN -&gt; ON -&gt; WHERE -&gt; GROUP BY -&gt; HAVING -&gt; ORDER BY -&gt; LIMIT MySQL执行顺序 –查询组合字段 (8) SELECT (9) DISTINCT (1) FROM (3) JOIN (2) ON (4) WHERE (5) GROUP BY (6) WITH {CUBE|ROLLUP} (7) HAVING (10) ORDER BY (11) LIMIT mysql关键词含义 FROM：对FROM子句中的左表和右表执行笛卡尔乘积，产生虚拟表 VT1 ON：对虚拟表VT1根据进行ON筛选，符合的行插入虚拟表VT2 JOIN：如果指定了OUTER JOIN (如LEFT OUTER JOIN、RIGHT OUTER JOIN)，那么保留表中未匹配的行作为外部行添加到虚拟表VT2中，产生虚拟表VT3。如果FROM子句中包含两个以上表，则对上一个连接生产的结果表VT3和下一个表进行重复执行(步骤1～步骤3），直到连接完所有的表 WHERE：对虚拟表VT3根据进行WHERE过滤，符合的行插入虚拟表VT4 GROUP BY：根据GROUP BY子句中的列，对虚拟表VT4中的数据进行分组操作，产生VT5 CUBE|ROLLUP：对虚拟表VT5进行CUBE|ROLLUP操作，产生虚拟表VT6 HAVING：对虚拟表VT6根据进行HAVING筛选，符合的行插入虚拟表VT7 SELECT：根据SELECT选择指定的列，结果插入虚拟表VT8中 DISTINCT：去除重复数据，结果插入虚拟表VT9 ORDER BY：将虚拟表VT9按照进行排序操作，产生虚拟表VT10 LIMIT：根据取出指定行的数据，产生虚拟表VT11，并返回给查询用户 实例剖析创建数据1234567891011121314151617181920212223242526272829303132333435363738CREATE TABLE customers( c_id VARCHAR(10), city VARCHAR(10) NOT NULL, PRIMARY KEY(c_id));INSERT INTO customers VALUES ('163','HangZhou'),('9you','ShangHai'),('baidu','HangZhou'),('TX','HangZhou');SELECT * FROM customers;+-------+----------+| c_id | city |+-------+----------+| 163 | HangZhou || 9you | ShangHai || baidu | HangZhou || TX | HangZhou |+-------+----------+CREATE TABLE orders( order_id INT AUTO_INCREMENT, c_id VARCHAR(10), PRIMARY KEY(order_id));INSERT INTO orders VALUES(1,'163'),(2,'163'),(3,'9you'),(4,'9you'),(5,'9you'),(6,'TX'),(7,NULL);SELECT * FROM orders;+----------+------+| order_id | c_id |+----------+------+| 1 | 163 || 2 | 163 || 3 | 9you || 4 | 9you || 5 | 9you || 6 | TX || 7 | NULL |+----------+------+ 情景模拟 题目：查询来自杭州且订单数少于2的客户，并且查询出他们的订单数量，查询结果按订单数从小到大排序 解答 12345678910111213141516SELECT customers.c_id,COUNT(order_id) AS total_ordersFROM customers LEFT JOIN orders ON customers.c_id = orders.c_idWHERE city='HangZhou'GROUP BY customers.c_idHAVING COUNT(orders.order_id)&lt;2ORDER BY total_orders DESC;+-------+--------------+| c_id | total_orders |+-------+--------------+| TX | 1 || baidu | 0 |+-------+--------------+ 执行过程(1) 执行FROM子句 即对FROM子句前后两张表进行笛卡尔乘积操作，也称作交叉连接，生成虚拟表VT1。如果FROM子句前表有a行数据，后表有b行数据，则虚拟表VT1中将包行a*b行数据。 FROM customers JOIN orders 虚拟表VT1 12345678910111213141516171819202122232425262728293031323334SELECT * FROM customers JOIN orders;+-------+----------+----------+------+| c_id | city | order_id | c_id |+-------+----------+----------+------+| 163 | HangZhou | 1 | 163 || 9you | ShangHai | 1 | 163 || baidu | HangZhou | 1 | 163 || TX | HangZhou | 1 | 163 || 163 | HangZhou | 2 | 163 || 9you | ShangHai | 2 | 163 || baidu | HangZhou | 2 | 163 || TX | HangZhou | 2 | 163 || 163 | HangZhou | 3 | 9you || 9you | ShangHai | 3 | 9you || baidu | HangZhou | 3 | 9you || TX | HangZhou | 3 | 9you || 163 | HangZhou | 4 | 9you || 9you | ShangHai | 4 | 9you || baidu | HangZhou | 4 | 9you || TX | HangZhou | 4 | 9you || 163 | HangZhou | 5 | 9you || 9you | ShangHai | 5 | 9you || baidu | HangZhou | 5 | 9you || TX | HangZhou | 5 | 9you || 163 | HangZhou | 6 | TX || 9you | ShangHai | 6 | TX || baidu | HangZhou | 6 | TX || TX | HangZhou | 6 | TX || 163 | HangZhou | 7 | NULL || 9you | ShangHai | 7 | NULL || baidu | HangZhou | 7 | NULL || TX | HangZhou | 7 | NULL |+-------+----------+----------+------+ (2) 应用ON过滤 根据(1)中生成的虚拟表VT1，过滤条件为： ON customers.c_id = orders.c_id 在产生虚拟表VT2时，会额外增加一个列来表示ON过滤条件的返回值，返回值有TRUE、FALSE、UNKNOWN。再取出比较值为TRUE的数据行，产生虚拟表VT2 虚拟表VT2 1234567891011SELECT * FROM customers JOIN orders ON customers.c_id = orders.c_id;+------+----------+----------+------+| c_id | city | order_id | c_id |+------+----------+----------+------+| 163 | HangZhou | 1 | 163 || 163 | HangZhou | 2 | 163 || 9you | ShangHai | 3 | 9you || 9you | ShangHai | 4 | 9you || 9you | ShangHai | 5 | 9you || TX | HangZhou | 6 | TX |+------+----------+----------+------+ (3) 添加外部行 这一步只有在连接类型为OUTER JOIN时才发生，即LEFT OUTER JOIN、RIGHT OUTER JOIN、FULL OUTER JOIN。LEFT OUTER JOIN把左表记为保留表(即保留左表全部数据行)，RIGHT OUTER JOIN把右表记为保留表，而FULL OUTER JION把左右表都记为保留表。具体可以参考下面图SQL_JOINS(图片来源网络，侵删) 图SQL_JOINS 添加外部行执行内容就是在虚拟表VT2的基础上添加保留表中被过滤条件过滤掉的数据，非保留表中的数据则被赋予NULL值，最后生成虚拟表VT3 虚拟表VT3 12345678910111213SELECT * FROM customers LEFT JOIN orders on customers.c_id = orders.c_id;+-------+----------+----------+------+| c_id | city | order_id | c_id |+-------+----------+----------+------+| 163 | HangZhou | 1 | 163 || 163 | HangZhou | 2 | 163 || 9you | ShangHai | 3 | 9you || 9you | ShangHai | 4 | 9you || 9you | ShangHai | 5 | 9you || TX | HangZhou | 6 | TX || baidu | HangZhou | NULL | NULL |+-------+----------+----------+------+ (4) 应用WHERE过滤 根据(3)生成的虚拟表VT3进行WHERE条件的过滤，符合的数据行输出生成虚拟表VT4。过滤条件为： WHERE customers.city = ‘HangZhou’ 虚拟表VT4 123456789101112SELECT * FROM customers LEFT JOIN orders on customers.c_id = orders.c_idWHERE customers.city = 'HangZhou';+-------+----------+----------+------+| c_id | city | order_id | c_id |+-------+----------+----------+------+| 163 | HangZhou | 1 | 163 || 163 | HangZhou | 2 | 163 || TX | HangZhou | 6 | TX || baidu | HangZhou | NULL | NULL |+-------+----------+----------+------+ tips：在应用WHERE过滤时，需要严格注意之前未使用过的过滤项，例如以下两种情况 未进行数据分组，不能在当前的WHERE过滤中使用 = MIN(col) 这种聚合函数式的过滤 如果别名出现在SELECT中，则在当前WHERE过滤中不能使用该别名；原因是语句的执行顺序 (5) 分组 根据指定的对(4)中生成的虚拟表进行分组，并生成虚拟表VT5；分组条件为： GROUP BY customers.c_id; 虚拟表VT5 123456789101112SELECT * FROM customers LEFT JOIN orders on customers.c_id = orders.c_id WHERE customers.city = 'HangZhou' GROUP BY customers.c_id;+-------+----------+----------+------+| c_id | city | order_id | c_id |+-------+----------+----------+------+| 163 | HangZhou | 1 | 163 || baidu | HangZhou | NULL | NULL || TX | HangZhou | 6 | TX |+-------+----------+----------+------+#实际环境中有 &quot;this is incompatible with sql_mode=only_full_group_by&quot;,因为加入没有后续的过滤条件 (6) 并未应用ROLLUP或CUBE，跳过 (7) 应用HAVING过滤 根据(5)生成的虚拟表VT5进行HAVING条件过滤，符合的数据行输出生成虚拟表VT6，过滤条件为： HAVING count(orders.order_id &lt; 2) 虚拟表VT6 1234567891011SELECT * FROM customers LEFT JOIN orders on customers.c_id = orders.c_id WHERE customers.city = 'HangZhou' GROUP BY customers.c_idHAVING COUNT(orders.order_id) &lt; 2;+-------+----------+----------+------+| c_id | city | order_id | c_id |+-------+----------+----------+------+| baidu | HangZhou | NULL | NULL || TX | HangZhou | 6 | TX |+-------+----------+----------+------+ (8) 筛选 - SELECT列表 将SELECT子句中的列从(7)生成的虚拟表VT6中筛选出来，生成虚拟表VT7，筛选条件为： SELECT customers.c_id,COUNT(order_id) AS total_orders 虚拟表VT7 123456789101112SELECT customers.c_id,COUNT(order_id) AS total_ordersFROM customers LEFT JOIN orders on customers.c_id = orders.c_id WHERE customers.city = 'HangZhou' GROUP BY customers.c_idHAVING COUNT(orders.order_id) &lt; 2;+-------+--------------+| c_id | total_orders |+-------+--------------+| baidu | 0 || TX | 1 |+-------+--------------+ (9) 去重 - DISTINCT ，本实例未应用，跳过 (10) 排序 - ORDER BY 根据指定的对(8)中生成的虚拟表VT7进行排列顺序，返回新的虚拟表VT8，排序条件为： ORDER BY total_orders 虚拟表VT8 12345678910111213SELECT customers.c_id,COUNT(order_id) AS total_orders FROM customers LEFT JOIN orders on customers.c_id = orders.c_id WHERE customers.city = 'HangZhou' GROUP BY customers.c_id HAVING COUNT(orders.order_id) &lt; 2 ORDER BY total_orders DESC;+-------+--------------+| c_id | total_orders |+-------+--------------+| TX | 1 || baidu | 0 |+-------+--------------+ (11) 选取数据 - LIMIT tips：一般来说LIMIT和ORDER BY一起使用，因为没有ORDER BY的返回结果一般都是无序的 本次实例没有应用，跳过～ 总结以上是在学习MySQL的书写以及执行顺序记录的学习笔记，并通过实例进行各个执行步骤的拆解，可以更详细的了解一条MySQL语句的执行顺序以及每个步骤产生的结果。以上学习资源均来自网络，如有侵权，告知删除，感谢阅读～ 参考文章: 掘金菜鸟教程CSDN","link":"/2021/07/11/mysql%E5%85%B3%E9%94%AE%E5%AD%97%E4%BB%A5%E5%8F%8A%E8%AF%AD%E5%8F%A5%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/"},{"title":"python数据类型 - 列表与字典","text":"摘要python数据类型之列表(List)&amp;字典(Dictionary),基础特性以及常用场景～ 写在前面上篇博文介绍了python中最基本的数据类型之一字符串，接下来介绍另外两个最常用的python数据类型：列表与字典，包括其特性与一些必须会的操作函数。在数据分析过程中，这两种数据结构也是用的比较多的，所以掌握这两种基础数据类型的操作是必不可少的～ 列表(List) 列表的表示： [] 列表的元素类型：可以是任意数据类型的python对象类型，包括数字、字符串、列表、布尔值.. 列表内的元素以逗号为分割，逗号与逗号之间即是一个整体 列表是一种有序的集合，可以随时添加和删除其中的元素 列表的索引从0开始 列表的高级特性索引 列表的索引是从0开始的 12345678# 索引取值li = [1,23,8,'giu',['时代','shkj',1737],True]print(li[3])# 输出giu 可以根据索引对列表的元素进行修改与删除 1234567891011121314151617# 根据索引进行列表内元素修改li[2] = 20print(li)#输出[1, 23, 20, 'giu', ['时代', 'shkj', 1737], True]# 根据索引列表内元素删除del li[1]print(li)# 输出[1, 20, 'giu', ['时代', 'shkj', 1737], True] 切片 切片的输出结果也是list 12345678# 切片取值li = [1,23,8,'giu',['时代','shkj',1737],True]print(li[2:5])# 输出[8, 'giu', ['时代', 'shkj', 1737]] 使用切片对列表的多个元素进行修改与删除 1234567891011121314151617# 根据切片进行列表内元素修改li[1:3] = [11,32]print(li)# 输出[1, 11, 32, 'giu', ['时代', 'shkj', 1737], True]# 根据切片列表内元素删除del li[2:5]print(li)# 输出[1, 23, True] in 操作 需要注意的是：列表内的元素以逗号为分割，即是一个整体，当元素是一个列表时，列表内的元素不能单独被in操作识别 1234567891011121314151617# in 操作li = [12,'wudu',[83,'wudi',8]]v = 11 in liprint(v)# 输出Falseli = [12,'wudu',[83,'wudi',8]]v = 'wudi' in liprint(v)# 输出False 常用方法append append : 追加，在原来的列表最后追加输入的参数(输入参数将作为一个整体追加) 123456789# append li = ['qwe','hsdj','sad','aw','12',12]v = li.append('无敌')print(li)# 输出['qwe', 'hsdj', 'sad', 'aw', '12', 12, '无敌'] extend 扩展原列表，与append 的不同之处在于：append输入的参数作为整体追加到列表末尾，而extend输入的参数内部for循环，逐个加入列表 123456789101112131415li = ['qwe','hsdj','aw']for i in [33,'sdh']: li.append(i)print(li)# 输出['qwe', 'hsdj', 'aw', 33, 'sdh']li.extend('nbeu')print(li)# 输出['qwe', 'hsdj', 'aw', 33, 'sdh', 'n', 'b', 'e', 'u'] insert 插入，在指定索引位置插入，第一个参数为索引位置，第二个参数为插入的值 1234567li = [11,22,33,23,32]li.insert(2,668)print(li)# 输出[11, 22, 668, 33, 23, 32] copy 列表拷贝(复制)，属于浅拷贝 123456789# copyli = ['qwe','hsdj','sad','aw','12',12]v = li.copy()print(v)# 输出['qwe', 'hsdj', 'sad', 'aw', '12', 12] 列表 - 删除 除了上面介绍的根据索引和切片对列表元素进行删除以外，还有以下三种方法： 12345678910111213141516171819202122232425262728293031323334# pop# 删除，默认状况下删除列表最后一个值，输入的参数为需要删除的索引位置li = [11,22,33,23,32]v = li.pop(3)print(li)# 特点是，能获取到所删除的值print(v)# 输出[11, 22, 33, 32]23# remove# 删除列表中的指定的值，左边优先li = [11,22,33,23,32]li.remove(22)print(li)# clear# 清空列表li = ['qwe','hsdj','sad','aw','12',12]li.clear()print(li) 内置方法1234567891011121314151617181920212223242526272829303132# count# 计算列表内元素出现的个数li = ['qwe',12,'hsdj','aw','aw','12',12]v = li.count(12)print(v)# index# 根据值获取当前值的索引位置，由左往右，后置位参数为开始寻找的区间li = [11,22,33,23,32,11,13,12]v = li.index(11)v1 = li.index(11,1,6)print(v,v1)# reverse# 将当前列表进行反转li = [11,22,33,23,32]li.reverse()print(li)# sort# 从小到大排序，reverse=True为从大到小排序li = [11,22,33,23,32]li.sort(reverse=True)print(li) 字典(Dictionary)必知 字典的表示：{} 由键值对组成，以逗号分割，且是无序的，具有极快的查找速度 列表、字典不能作为字典的key，字典的value可以是任何类型值 字典是一种可变容器模型，且可存储任意类型对象，如字符串、数字、元组等其他容器模型 字典中键是唯一的，如果重复最后的一个键值对会替换前面的，值不需要唯一 必会1234567# 创建字典info = { 1:'sad', 'k1': ['sad',12,['sd','dw']], (1,22):'123' } 获取键 获取字典中键值对的键 12345678for item in info.keys(): print(item)# 输出1k1(1, 22) 获取值 获取字典中键值对的值 12345678for item in info.values(): print(item)# 输出sad['sad', 12, ['sd', 'dw']]123 获取键与值 同时获取键值对的键与值 12345678for k,v in info.items(): print(k,v) # 输出1 sadk1 ['sad', 12, ['sd', 'dw']](1, 22) 123 get方法 根据传入的键获得值，后置位参数为：如果键不存在时的返回值 123456v = info.get('k',111)print(v)# 输出111 字典 - 更新 update ：更新字典内的键值对的两种方法 12345678# 字典形式传入info.update({'k1':'1111','k3':'v3'})# 赋值形式传入info.update(k1=112,k5='d',k6=668)print(dic) in操作1234567891011 dic = { 'k1':'v1', 'k2':'v2' } v = 'k1' in dic v1 = 'v1' in dic.values() print(v,v1)# 输出True True 内置方法1234567891011121314151617181920212223242526272829303132333435363738394041# .pop# 删除指定键的值，后置位参数为，如果指定键不存在的返回值v = dic.pop('k111','不存在')print(dic,v)# .popitem# 随机删除一个键值对，并获得该键值对，支持将键值对分别赋值给键和值k,v = dic.popitem()print(dic,k,v)# .setdefault# 根据传入的key设置值，若key已存在，不进行设置，并获取当前已存在的key的值# 若key不存在，插入键值对，后置位参数默认为none，并获取当前生成的键值对的key的值dic.setdefault('k3')print(dic)# .clear 删除字典dic.clear()print(dic)# .copy 复制字典v = dic.copy()print(v)# .fromkeys 根据传入的序列，创建字典，并指定统一的值v = dict.fromkeys(['sadj',123,'999'],333)print(v)# dict.setdefault(key, default=None) # 和get()类似, 但如果键不已经存在于字典中，将会添加键并将值设为default 写在后面本篇博客为学习数据分析时，学习python基本使用的笔记整理，很是浅显，不过学习是一件值得开心的事，不论所学是深或浅，千里之行始于足下嘛，只要在走，就不怕远～ 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/08/python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%20-%20%E5%88%97%E8%A1%A8%E4%B8%8E%E5%AD%97%E5%85%B8/"},{"title":"python数据类型 - 字符串","text":"摘要python数据类型之字符串(String),基础特性以及常用场景～ 前言这篇文章主要介绍python在数据分析中的字符串的常用魔法,结合实例形式总结字符串常用魔法的概念、功能、使用方法以及相关注意事项。 常用方法join 字符串拼接：将字符串中的每一个元素按照指定的分隔符进行拼接 1234567test = '你是风儿我是沙'v = ' '.join(test)# 输出你是风儿我是沙你 是 风 儿 我 是 沙 字符串拼接拓展： 连接少量字符串时，可以使用 + 号操作符 或者 f-string 连接大量字符串时，推荐使用 join 和 f-string find 子序列查找：根据传入的参数寻找子序列，找到第一个后，获取其位置 参数：sub 指定要查找的子序列 ​ start和end 指定寻找的区间 1234567test = 'hello world'v = test.find('o',3,-1)v1 = test.find('o',1,4)# 输出4-1 通过上面的实例及输出，可以发现： 在找到第一个符合传入的参数sub后，停止查找并获取sub在字符串中所在的位置(或者说下标) 通过输出v和v1，得到的位置(下标)是在整个字符串中的位置，而当返回为-1时，则为传入的指定区间的末尾，并非整个字符串的末尾 split与rsplit 字符串分割：以传入参数为分割符，默认将字符串全部分割 split：默认从左往右进行分割；rsplit：指定从右往左进行分割 参数：sep 指定分割符 ​ maxsplit 指定分割次数 12345678test = 'hello world'v = test.split('l',2)v1 = test.split('l',10)# 输出['he', '', 'o world']['he', '', 'o wor', 'd'] 通过输出对比，不难发现split需要注意的地方有： 分割时，传入参数的匹配可视为贪婪匹配，如实例中v的输出为连续的 l 分割符视为一个整体进行前后分割 结果输出并不会输出分割符 分割次数若大于字符串中含有的分割符个数，不会报错，仅输出为最大分割次数所得到的结果 关于字符串分割拓展： partition：以参数为分割符，且在拿到第一个参数时，将字符串分割成3份 rpartition：以参数为分割符，由右向左识别且在拿到第一个参数时，将字符串分割成3份 splitlines：只能根据换行符进行分割，传入的参数为True和False，结果为是否保留换行符显示 replace 替换：字符串中的指定子序列与传入参数进行替换 参数：old 指定被替换的子序列 ​ new 传入替换的参数 ​ count 替换个数(次数)，默认为全部替换 123456test = 'hello world'v = test.replace('hello','hi',2)# 输出hi world 字符串替换的另一种方法 maketrans定义对应关系，translate进行替换 1234567s = str.maketrans('aeiou','12345')v = 'osahodioashoioiwueuroiuapfhahweiuip'result = v.translate(s)# 输出4s1h4d341sh4343w525r4351pfh1hw2353p strip 移除指定的字符串，并优先最多匹配，默认(即不传入参数时)情况下，去除左右空白以及 \\t 、\\n lstrip：指定从左往右进行匹配并移除 rstrip：指定从右往左进行匹配并移除 12345678910111213141516test = 'xaxexxaleaxaae'v = test.strip('xa')v1 = test.lstrip('xa')v2 = test.rstrip('xae')# 输出vexxaleaxaae# 输出v1exxaleaxaa# 输出v2xaxexxal strip使用总结： 所谓优先最多匹配，指的是先传入参数为整体进行匹配，如果匹配不上再分解为单个字符或字符组合去进行匹配，如v2中传入的参数为xae，并指定从右往左进行匹配，匹配时优先顺序为 xae — xa、ae — x、a、e strip是指从字符串的左右两边进行匹配移除，并停止于所谓匹配优先顺序中的所有情况都匹配不上 isupper和islower isupper：判断字符串是否全部为大写 islower：判断字符串是否全部为小写 1234567test = 'ASDDss'v1 = test.isupper()v2 = test.islower()# 输出False False 输出结果为布尔值，转换为大写用：upper；转换为小写用：lower startswith和endswith startswith：判断是否以传入的参数开头 endswith：判断是否以传入的参数结尾 1234567891011test = 'djfkjahfkja'v = test.startswith('dj')v1 = test.endswith('aj')# 输出vTrue# 输出v1 False 输出结果为布尔值 其余方法判断型 isalnum：判断字符串中是否只包含字母和数字（汉字也算） isalpha：字符串中是否只包含字母（汉字也算） isdecimal、isdigit、isnumeric 字符串是否是数字，isdigit（包含一些特殊字符的数字）比isdecimal(十进制小数)牛逼点 isnumeric(包括汉字的数字也能识别)👈👈👈👈👈三者之中最牛，但使用最多是isdecimal isidentifier 标识符：查看字符串是否由字母，数字，下划线组成，数字不能为首 isprintable：判断字符串中是否存在不可显示的字符，例如：\\t 制表符 \\n 换行符 isspace：判断是否全部是空格 istitle：判断是否为标题，标题的定义为每个字符首字母为大写；转换为标题用：title center、ljust、rjust 设置字符串的宽度，并使用字符填充 center将内容居中，ljust将内容放左边，rjust将内容放右边 参数：width 指定宽度 ​ fillchar 指定填充字符 123456test = 'hello world'v1 = test.center(21,'*')# 输出*****hello world***** expandtabs 断句：匹配 \\t ，将字符串以6个为一组进行断句，并在匹配到 \\t 的位置，不够6个时以空格补满 参数：tabsize 指定宽度 123456789101112131415161718s = 'hfaushfhoahsfoih\\tuisagfiu\\tjdsba'v = s.expandtabs(6)# 输出hfaushfhoahsfoih uisagfiu jdsba# 实例应用text = 'username\\tage\\tpassw\\tsex\\n张三\\t12\\t123\\t男\\n李四\\t19\\t123\\t男\\n陈陈\\t18\\t123\\t女\\n'v = text.expandtabs(20)# 输出结果username age passw sex张三 12 123 男李四 19 123 男陈陈 18 123 女 swapcase 大小写转换：与lower、upper不同之处在于，swapcase把字符串中每一个元素进行转换，而并非统一转换成大写或小写 123456test = 'LAsjSi'v = test.swapcase()# 输出laSJsI 总结以上为数据分析中python字符串自带的一下常用方法，重温一遍并记录下来，以便查漏补缺，有哪些地方出现错误或者疑义，欢迎讨论，感谢阅读～ 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/07/python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%20-%20%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"title":"如何使用you-get获取视频？","text":"摘要关于you-get的简单使用，方便获取网络上部分视频～ 项目描述今天分享一个各大网站视频下载方法(封装好的pacong)。 目的是为了能够将视频下载到本地，方便学习翻阅；但有以下需要注意的地方： 下载下来的视频不能进行商业用途 下载时最好设置爬取时间间隔，以防对目标网站造成服务器压力倍增 本篇博文纯属技术分享(以bilibili网站视频为例) you-get 安装123# you-get使用需要python3环境(建议使用anaconda)pip install you-get you-get 参数123# 命令行查看you-get参数you-get -h 更多参数详细使用请参考：https://github.com/soimort/you-get 视频下载单个视频下载123# 命令行下载you-get -o /data/Download https://www.bilibili.com/video/bv111111?p=1 参数解析： -o /data/Download 表示输出路径，如果这个路径不存在，会自动新建 后面跟着是你需要下载视频的b站网址 12345678910# 脚本下载import sysfrom you-get import common as you-getdirectory = '/data/Dowmload'url = 'https://www.bilibili.com/video/bv111111?p=1'sys.argv = ['you-get' , '-o' , directory , url ]you-get.main() 多个视频批量下载123# 命令行you-get -o /data/Download https://www.bilibili.com/video/bv111111?p=1 -l 参数解析：-l 参数将网址列表内的所有视频下载下来 12345678910# 脚本下载import sysfrom you-get import common as you-getdirectory = '/data/Dowmload'url = 'https://www.bilibili.com/video/bv111111?p=1'sys.argv = ['you-get' , '-o' , directory , url , '-l']you-get.main() 写在最后以上为 you-get 下载b站视频的技术分享，更多参数使用请根据需求自行查阅。感谢阅读～","link":"/2021/04/16/you-get-%E4%B8%8B%E8%BD%BD%E8%A7%86%E9%A2%91/"},{"title":"会员用户消费行为浅度挖掘","text":"摘要通过会员消费数据，分析消费趋势、复购率以及回购率等;并给会员用户进行分层，给后续工作提供数据依据～ 项目描述项目名称：健身平台会员用户消费行为分析 数据来源：数据集来源于网络，是某健身房2019年3月至2020年2月会员消费数据，数据源格式为.xls 字段说明 user_id ：用户ID order_dt ：购买日期 order_products ：购买数量 order_amount ：购买金额 项目目的： 月度趋势分析 个体消费分析 复购率 &amp; 回购率 用户分层 用户生命周期 数据导入12345678910import pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom datetime import datetimeimport osimport warningswarnings.filterwarnings('ignore')os.chdir('/home/min/data')%matplotlib inline 123456789101112131415# 可视化风格设定large = 22med = 16small = 13params = {'axes.titlesize' : large, 'legend.fontsize' : med, 'figure.figsize' : (19,10), 'axes.labelsize' : med, 'axes.titlesize' : med, 'xtick.labelsize' : med, 'ytick.labelsize' : med, 'figure.titlesize' : large}plt.rcParams.update(params)plt.style.use('seaborn-pastel') 12data = pd.read_excel('cuscapi.xls')data.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id order_dt order_products order_amount 0 vs30033073 2020-01-17 1 20 1 vs30026748 2019-12-04 1 20 2 vs10000716 2019-07-05 1 20 3 vs30032785 2019-08-21 2 0 4 vs10000716 2019-10-24 1 20 5 vs30033073 2019-11-29 2 20 6 vs10000621 2019-07-19 2 20 7 vs30029475 2019-05-17 1 20 8 vs30030664 2019-11-11 1 20 9 vs10000773 2019-11-25 1 20 1data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2013 entries, 0 to 2012 Data columns (total 4 columns): user_id 2013 non-null object order_dt 2013 non-null datetime64[ns] order_products 2013 non-null int64 order_amount 2013 non-null int64 dtypes: datetime64[ns](1), int64(2), object(1) memory usage: 63.0+ KB 123pd.set_option('display.float_format', lambda x : '%.2f' % x)data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } order_products order_amount count 2013.00 2013.00 mean 1.47 22.90 std 0.91 94.94 min 1.00 0.00 25% 1.00 20.00 50% 1.00 20.00 75% 2.00 20.00 max 12.00 2650.00 图表解析： 用户平均每笔订单购买1.5个商品，标准差为0.9，波动性不大 中位数在1个商品，75分位数在2个产品，说明绝大多数订单的购买数量都不多 平均每笔消费金额为22.9元，标准差约为95，中位数为20，平均数大于中位数，说明多数会员消费金额集中在小金额范围，而小部分会员贡献了大额消费，符号二八法则。 12345# 根据用户id进行分组汇总user_group = data.groupby('user_id').sum()user_group.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } order_products order_amount user_id vs10000005 9 189 vs10000621 214 5704 vs10000627 2 0 vs10000716 250 2616 vs10000743 1 20 vs10000757 75 1104 vs10000773 23 460 vs10000775 8 2730 vs10000788 7 144 vs10000794 1 0 1user_group.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } order_products order_amount count 247.00 247.00 mean 11.97 186.59 std 36.70 641.12 min 1.00 0.00 25% 2.00 0.00 50% 2.00 0.00 75% 3.00 66.00 max 277.00 5704.00 图表解析： 用户平均购买数量约为12个商品，最多购买了277个商品 平均消费金额约为187元，标准差为641，中位数为0，也说明了存在小部分会员购买了大数量商品的高消费情况 数据处理类型转换 &amp; 新字段提取123456# 类型转换 &amp; 新字段提取(月份)data['order_dt'] = pd.to_datetime(data['order_dt'])data['month'] = data['order_dt'].astype('datetime64[M]')data.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id order_dt order_products order_amount month 0 vs30033073 2020-01-17 1 20 2020-01-01 1 vs30026748 2019-12-04 1 20 2019-12-01 2 vs10000716 2019-07-05 1 20 2019-07-01 3 vs30032785 2019-08-21 2 0 2019-08-01 4 vs10000716 2019-10-24 1 20 2019-10-01 5 vs30033073 2019-11-29 2 20 2019-11-01 6 vs10000621 2019-07-19 2 20 2019-07-01 7 vs30029475 2019-05-17 1 20 2019-05-01 8 vs30030664 2019-11-11 1 20 2019-11-01 9 vs10000773 2019-11-25 1 20 2019-11-01 123# 检查空值data.apply(lambda x:sum(x.isnull())) user_id 0 order_dt 0 order_products 0 order_amount 0 month 0 dtype: int64 用户消费分析月度总趋势分析1234567# 月度消费data.groupby('month').order_amount.sum().plot()plt.xlabel('月份')plt.ylabel('消费金额(元)')plt.title('不同月份的用户消费金额')plt.legend() 图表解析：按月份统计每个月的商品消费金额，可以看出，各个月份销量波动起伏较大。 1234567# 月度产品购买数量data.groupby('month').order_products.sum().plot()plt.xlabel('月份')plt.ylabel('产品个数')plt.title('不同月份的产品购买数量')plt.legend() 图表解析：每月的产品购买量呈现为前7个月快速上升，而后5个月呈整体下降的趋势。 12345678# 月度消费次数data.groupby('month').user_id.count().plot()plt.xlabel('月份')plt.ylabel('消费次数')plt.title('不同月份的用户消费次数')plt.legend() 图表解析：消费次数在前四个月呈上升趋势，在六月份达到顶峰，而后呈下降趋势。 12345678# 月度消费人数data.groupby('month').user_id.nunique().plot()plt.xlabel('月份')plt.ylabel('消费人数')plt.title('不同月份的消费人数')plt.legend() 图表解析：相比而言，每月的消费人数大多小于每月的消费人数，而在7月份消费人数达到顶峰90人，后续月份的消费人数持续下降。 用户个体消费趋势分析1234567# 用户消费金额 &amp; 产品个数user_consume = data.groupby('user_id').sum()plt.scatter(user_consume['order_products'],user_consume['order_amount'],c = 'r')plt.xlabel('消费产品个数')plt.ylabel('消费金额')plt.title('用户消费金额 &amp; 产品个数散点图') 图表解析：可以看到，图中的离散点较多，订单消费金额和订单商品数量的关系不呈线性，用户消费规律性不强，而且订单的极值较多。 12345678910111213141516171819# 购买数量分布 &amp; 购买金额分布consume_products = user_consume['order_products']consume_amount = user_consume['order_amount']fig = plt.figure(figsize = (19,9))fig.add_subplot(1,2,1)consume_products.hist(bins = 10)plt.title('用户购买产品数量分布')plt.xlabel('购买数量')plt.ylabel('人数')fig.add_subplot(1,2,2)consume_amount.hist(bins = 10)plt.title('用户购买金额分布')plt.xlabel('购买金额')plt.ylabel('人数')plt.show() 图表解析：大部分用户消费能力不高，图中可知，购买数量大多在50以内，而消费金额大多为1000以内。 1234567# 首次消费会员分布data.groupby('user_id').month.min().value_counts().plot()plt.title('首次消费会员数分布')plt.xlabel('首次消费时间')plt.ylabel('会员数')plt.show() 1234567# 最后一次消费会员数分析data.groupby('user_id').month.max().value_counts().plot()plt.title('最后一次消费会员数分布')plt.xlabel('最后一次消费时间')plt.ylabel('会员数')plt.show() 123# 首次消费与最后一次消费时间间隔 -- 会员流失情况(data.groupby('user_id')['month'].agg({'num1':'min','num2':'max'}).num2 - data.groupby('user_id')['month'].agg({'num1':'min','num2':'max'}).num1).value_counts() 0 days 177 31 days 24 61 days 6 92 days 6 122 days 6 337 days 5 30 days 4 306 days 3 153 days 3 184 days 3 62 days 2 123 days 2 215 days 2 245 days 2 275 days 1 276 days 1 dtype: int64 图表解析： 有大量用户的第一次消费集中在7、8月份，有可能是活动引入了大量新用户所导致 相比较最后一次消费，可以观察到大部分用户都在首次消费一个月内流失掉 复购率1234567# 每位会员各月消费次数pivoted_counts = data.pivot_table(index='user_id',columns='month',values='order_dt', aggfunc='count').fillna(0)columns_month = data.month.sort_values().unique()pivoted_counts.columns = columns_monthpivoted_counts.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2019-03-01 2019-04-01 2019-05-01 2019-06-01 2019-07-01 2019-08-01 2019-09-01 2019-10-01 2019-11-01 2019-12-01 2020-01-01 2020-02-01 user_id vs10000005 2.00 0.00 3.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 vs10000621 6.00 17.00 19.00 20.00 17.00 5.00 2.00 18.00 18.00 21.00 16.00 10.00 vs10000627 0.00 0.00 0.00 0.00 2.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 vs10000716 0.00 0.00 0.00 0.00 14.00 19.00 24.00 12.00 30.00 15.00 12.00 5.00 vs10000743 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 123456# 复购定义 ：在某时间段内消费两次或两次以上的会员在总消费会员中的占比# 时间段 ：月 。 如果一个会员在同一天下了两笔订单，也算作复购用户# 消费两次及以上记为1，仅消费一次记为0，没有消费记为NaNpivoted_counts.transf=pivoted_counts.applymap(lambda x:1 if x&gt;1 else np.NaN if x==0 else 0)pivoted_counts.transf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2019-03-01 2019-04-01 2019-05-01 2019-06-01 2019-07-01 2019-08-01 2019-09-01 2019-10-01 2019-11-01 2019-12-01 2020-01-01 2020-02-01 user_id vs10000005 1.00 nan 1.00 nan nan nan nan nan nan 0.00 nan nan vs10000621 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 vs10000627 nan nan nan nan 1.00 nan nan nan nan nan nan nan vs10000716 nan nan nan nan 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 vs10000743 0.00 nan nan nan nan nan nan nan nan nan nan nan 12345678# 用户复购率分布month_counts_rerate = pivoted_counts.transf.sum() / pivoted_counts.transf.count()plt.plot(month_counts_rerate)plt.title('每月会员复购情况分析')plt.xlabel('时间(月)')plt.ylabel('百分比')plt.show() 图表解析： 3月份至6月份新用户加入数量较少，复购率上升 在大量新用户加入且大量用户流失的8月份复购率达到最低点 在进行了一波用户流失后，剩余基本为忠实客户，从而使得复购率呈持续上升趋势 1234567891011# 消费人数和二次消费及以上的用户对比a,b = plt.subplots(figsize=(19,9))b.plot(pivoted_counts.transf.count())b.plot(pivoted_counts.transf.sum())legends = ['消费人数','二次消费及以上会员人数']plt.title('每月消费人数和二次消费及以上会员人数对比')plt.xlabel('时间(月)')plt.ylabel('用户数')plt.legend(legends)plt.show() 图表解析：可以看出用户数相差最多的时间节点为八月份，与八月份用户流失有极大关系。但整体趋势保持一致。 回购率1234567# 回购率 -- 某一时间段内消费的用户，在下一时间段仍然消费的占比# 实例：如1月消费用户1000,1000中有300个2月依然消费，回购率为30%pivoted_amount = data.pivot_table(index='user_id',columns='month', values='order_amount',aggfunc='mean').fillna(0)pivoted_amount.columns = columns_monthpivoted_amount.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2019-03-01 2019-04-01 2019-05-01 2019-06-01 2019-07-01 2019-08-01 2019-09-01 2019-10-01 2019-11-01 2019-12-01 2020-01-01 2020-02-01 user_id vs10000005 25.00 0.00 19.67 0.00 0.00 0.00 0.00 0.00 0.00 80.00 0.00 0.00 vs10000621 414.00 20.00 20.00 20.00 17.65 20.00 20.00 20.00 20.00 20.00 20.00 20.00 vs10000627 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 vs10000716 0.00 0.00 0.00 0.00 20.00 41.84 10.83 15.00 15.33 20.00 20.00 20.20 vs10000743 20.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1234# 统计有回购的会员人数pivoted_purchase = pivoted_amount.applymap(lambda x:1 if x&gt;1 else 0)pivoted_purchase.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2019-03-01 2019-04-01 2019-05-01 2019-06-01 2019-07-01 2019-08-01 2019-09-01 2019-10-01 2019-11-01 2019-12-01 2020-01-01 2020-02-01 user_id vs10000005 1 0 1 0 0 0 0 0 0 1 0 0 vs10000621 1 1 1 1 1 1 1 1 1 1 1 1 vs10000627 0 0 0 0 0 0 0 0 0 0 0 0 vs10000716 0 0 0 0 1 1 1 1 1 1 1 1 vs10000743 1 0 0 0 0 0 0 0 0 0 0 0 123456789101112def purchase_reture(data): status = [] for i in range(11): if data[i] &gt;= 1: if data[i+1] &gt;= 1: status.append(1) else: status.append(0) else: status.append(np.NaN) status.append(np.NaN) return pd.Series(status) 123pivoted_purchase_reture = pivoted_purchase.apply(purchase_reture,axis = 1)pivoted_purchase_reture.columns = columns_monthpivoted_purchase_reture.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2019-03-01 2019-04-01 2019-05-01 2019-06-01 2019-07-01 2019-08-01 2019-09-01 2019-10-01 2019-11-01 2019-12-01 2020-01-01 2020-02-01 user_id vs10000005 0.00 nan 0.00 nan nan nan nan nan nan 0.00 nan nan vs10000621 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 nan vs10000627 nan nan nan nan nan nan nan nan nan nan nan nan vs10000716 nan nan nan nan 1.00 1.00 1.00 1.00 1.00 1.00 1.00 nan vs10000743 0.00 nan nan nan nan nan nan nan nan nan nan nan 1234567pivoted_purchase_reture_rate = pivoted_purchase_reture.sum() / pivoted_purchase_reture.count()plt.plot(pivoted_purchase_reture_rate)plt.title('各月份用户回购率分析')plt.xlabel('时间(月)')plt.ylabel('百分比')plt.xticks(rotation = 60)plt.show() 图表解析： 用户回购率在4月份达到顶峰，而后呈下降趋势，至8月份到达最低点，原因有可能是用户在4月份至八月份的持续流失所导致 八月份往后，回购率由于剩余的忠实用户所呈上升趋势 123456789a,b = plt.subplots(figsize = (19,10))b.plot(pivoted_purchase_reture.count())b.plot(pivoted_purchase_reture.sum())legends = ['每月消费人数','每月回购人数']b.legend(legends)plt.title('各月份消费人数 &amp; 回购人数')plt.xlabel('时间(月)')plt.ylabel('用户数')plt.show() 图表解析：回购人数变化趋势基本与每月消费人数变化趋势保持一致。 123456789a,b = plt.subplots(figsize = (19,10))b.plot(pivoted_purchase_reture_rate)b.plot(month_counts_rerate)legends = ['每月回购率','每月复购率']b.legend(legends)plt.title('每月回购率 &amp; 每月复购率')plt.xlabel('时间(月)')plt.ylabel('百分比')plt.show() 图表解析： 大体上来看，每月用户的复购率高于回购率，波动性也比较大 用户分层RFM 分层123456user_rfm = data.pivot_table(index='user_id', values=['order_dt','order_products','order_amount'], aggfunc={'order_dt':'max','order_products':'count', 'order_amount':'sum'})user_rfm['order_dt'] = pd.to_datetime(user_rfm['order_dt']).dt.normalize()user_rfm.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } order_amount order_dt order_products user_id vs10000005 189 2019-12-27 6 vs10000621 5704 2020-02-28 169 vs10000627 0 2019-07-23 2 vs10000716 2616 2020-02-28 131 vs10000743 20 2019-03-15 1 123user_rfm['period'] = (user_rfm.order_dt.max() - user_rfm.order_dt) / np.timedelta64(1,'D')user_rfm = user_rfm.rename(columns={'period':'R','order_products':'F','order_amount':'M'})user_rfm.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } M order_dt F R user_id vs10000005 189 2019-12-27 6 63.00 vs10000621 5704 2020-02-28 169 0.00 vs10000627 0 2019-07-23 2 220.00 vs10000716 2616 2020-02-28 131 0.00 vs10000743 20 2019-03-15 1 350.00 1234567891011121314# 分层函数定义def rfm_func(x): level = x.apply(lambda x:'1' if x&gt;=0 else '0') label = level.R + level.F + level.M d = {'111':'高价值客户','011':'重点保持客户', '101':'重点发展客户','001':'重点挽留客户', '110':'一般价值客户','010':'一般保持客户', '100':'一般发展客户','000':'潜在客户'} result = d[label] return resultuser_rfm['label'] = user_rfm[['R','F','M']].apply(lambda x:x-x.mean()).apply(rfm_func, axis = 1)user_rfm.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } M order_dt F R label user_id vs10000005 189 2019-12-27 6 63.00 重点挽留客户 vs10000621 5704 2020-02-28 169 0.00 重点保持客户 vs10000627 0 2019-07-23 2 220.00 一般发展客户 vs10000716 2616 2020-02-28 131 0.00 重点保持客户 vs10000743 20 2019-03-15 1 350.00 一般发展客户 1user_rfm.groupby('label').count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } M order_dt F R label 一般保持客户 3 3 3 3 一般发展客户 146 146 146 146 潜在客户 63 63 63 63 重点保持客户 24 24 24 24 重点发展客户 2 2 2 2 重点挽留客户 2 2 2 2 高价值客户 7 7 7 7 1user_rfm.groupby('label').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } M F R label 一般保持客户 352 34 98.00 一般发展客户 2653 272 28793.00 潜在客户 1723 125 6377.00 重点保持客户 32494 1416 846.00 重点发展客户 2091 5 575.00 重点挽留客户 2919 9 165.00 高价值客户 3856 152 1429.00 12345678910111213141516171819202122232425262728293031323334# 字体管理from matplotlib import font_manager as fm# 颜色from matplotlib import cmproptease = fm.FontProperties()proptease.set_size(20)labelindex = user_rfm.groupby('label').count().indexlabelvalues = user_rfm.groupby('label')['M'].count().tolist()s = pd.Series(labelvalues,index=labelindex)labels = s.indexsizes = s.valuesexplode = (0,0,0,0,0.1,0.1,0.2)fig,axes = plt.subplots(1,2,figsize = (24,12))ax1,ax2 = axes.ravel()colors = cm.rainbow(np.arange(len(sizes)) / len(sizes))patches,texts,autotests = ax1.pie(sizes,labels = labels,autopct = '%1.0f%%', explode = explode,shadow = False, startangle = 170, colors = colors,labeldistance = 1.2, pctdistance = 1.05,radius = 1.5)ax1.axis('equal')plt.setp(texts,fontproperties = proptease)for i in autotests: i.set_size('larger')ax1.set_title('用户分层结构图',loc = 'center',fontsize = 32)ax2.axis('off')ax2.legend(patches,labels,loc = 'center left',fontsize = 20)plt.tight_layout()plt.show() 图表解析： 由图可知，一般发展客户占了较大的占比，为59% 潜在客户占比为26%，位居第二，而重点挽留客户以及重点发展客户均只占1%，需要对潜在客户进行引导及转化为忠实用户 根据活跃度分层123456789101112131415161718192021222324252627282930313233343536# 根据消费行为，简单划分为：新用户、活跃用户、不活跃用户、回流用户# 新用户 -- 第一次消费# 活跃用户 -- 老客户，在某一时间区间有过消费# 不活跃用户 -- 在设定时间区间内没有消费的老客户# 回流用户 -- 在上一设定时间区间内没有消费，在当前时间区间内有消费# 时间区间设定为以月为单位统计def active_stauts(data): status = [] for i in range(12): # 本月没有消费 if data[i] == 0: if len(status) &gt; 0: if status[i-1] == 'unreg': status.append('unreg') else: status.append('unactive') else: status.append('unreg') else: if len(status) == 0: status.append('new') else: if status[i-1] == 'unactive': status.append('return') elif status[i-1] == 'unreg': status.append('new') else: status.append('active') return pd.Series(status)pivoted_purchase_status = pivoted_purchase.apply(lambda x:active_stauts(x),axis = 1)pivoted_purchase_status.columns = columns_monthpivoted_purchase_status.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2019-03-01 2019-04-01 2019-05-01 2019-06-01 2019-07-01 2019-08-01 2019-09-01 2019-10-01 2019-11-01 2019-12-01 2020-01-01 2020-02-01 user_id vs10000005 new unactive return unactive unactive unactive unactive unactive unactive return unactive unactive vs10000621 new active active active active active active active active active active active vs10000627 unreg unreg unreg unreg unreg unreg unreg unreg unreg unreg unreg unreg vs10000716 unreg unreg unreg unreg new active active active active active active active vs10000743 new unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive unactive 12pivoted_status_counts = pivoted_purchase_status.replace('unreg',np.NaN).apply(lambda x:pd.value_counts(x))pivoted_status_counts.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2019-03-01 2019-04-01 2019-05-01 2019-06-01 2019-07-01 2019-08-01 2019-09-01 2019-10-01 2019-11-01 2019-12-01 2020-01-01 2020-02-01 active nan 7.00 9 16.00 20.00 18.00 11 12 15 14 16 10 new 13.00 2.00 8 9.00 17.00 17.00 10 9 6 7 7 1 return nan nan 1 nan nan nan 4 5 1 4 2 3 unactive nan 6.00 5 7.00 12.00 31.00 51 59 69 73 80 92 123456plt.plot(pivoted_status_counts.T)plt.title('每月不同用户类型占比',fontsize = 20)plt.legend(pivoted_status_counts.index)plt.xlabel('时间(月)')plt.ylabel('用户数')plt.show() 图表解析： 可以看到，不活跃用户占了较大的比重，而且呈持续上升趋势 活跃用户为蓝色部分，保持较为稳定的状态，而回流用户在9月份产生并维持稳定，但回流人数并不高 12345678910# 回流用户 &amp; 活跃用户return_rate = pivoted_status_counts.apply(lambda x:x/x.sum(),axis = 1)plt.plot(return_rate.loc[['active','return'],].T)plt.title('每月活跃用户 &amp; 回流用户对比')plt.xlabel('时间(月)')plt.ylabel('百分数')plt.xticks(rotation = 60)plt.show() 图表解析：结合回流用户和活跃用户来看，在后期消费用户中，约70%是回流用户，30%为活跃用户，整体用户质量较好。 用户质量123user_amount = data.groupby('user_id').order_amount.sum().sort_values().reset_index()user_amount['amount_cumsum'] = user_amount.order_amount.cumsum()user_amount.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id order_amount amount_cumsum 242 vs10000716 2616 29735 243 vs10000775 2730 32465 244 vs30026748 3296 35761 245 vs30029475 4623 40384 246 vs10000621 5704 46088 1234567amount_total = user_amount.amount_cumsum.max()user_amount['prop'] = user_amount.amount_cumsum.apply(lambda x:x/amount_total)plt.plot(user_amount.prop)plt.title('用户累计贡献金额百分比',fontsize = 24)plt.xlabel('人数')plt.ylabel('百分数')plt.show() 图表解析：数据集用户共247人，其中约50人贡献了超过80%的销售额，也符合二八定律。 用户生命周期123456# 各会员首次 &amp; 最后一次消费时间间隔order_dt_min = data.groupby('user_id').order_dt.min().dt.dateorder_dt_max = data.groupby('user_id').order_dt.max().dt.datelife_time = (order_dt_max - order_dt_min).reset_index()life_time.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id order_dt 0 vs10000005 273 days 1 vs10000621 351 days 2 vs10000627 1 days 3 vs10000716 238 days 4 vs10000743 0 days 1life_time.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } order_dt count 247 mean 32 days 03:59:01.700404 std 73 days 19:15:10.251372 min 0 days 00:00:00 25% 0 days 00:00:00 50% 1 days 00:00:00 75% 13 days 00:00:00 max 351 days 00:00:00 12345((order_dt_max - order_dt_min) / np.timedelta64(1,'D')).hist(bins = 15)plt.title('用户生命周期',fontsize = 24)plt.xlabel('天数')plt.ylabel('人数')plt.show() 图表解析： 用户平均生命周期为32天，中位数为1天，说明存在至少50%的客户的首次消费即最后一次消费。 最大生命周期为351天，约为数据源总时间长度，说明存在了从开始到最后有进行了消费的高质量用户 123456789# 消费两次及以上的用户生命周期life_time['life_time'] = life_time.order_dt / np.timedelta64(1,'D')life_time[life_time.life_time &gt; 0].life_time.hist(bins = 15)plt.title('二次消费及以上的用户生命周期',fontsize = 24)plt.xlabel('天数')plt.ylabel('人数')plt.show() 1life_time[life_time.life_time &gt; 0].life_time.describe() count 155.00 mean 51.26 std 87.84 min 1.00 25% 2.00 50% 7.00 75% 53.50 max 351.00 Name: life_time, dtype: float64 图表解析： 二次消费及以上用户生命周期为51天，高于总体用户生命周期。 从营销策略上看，用户在进行了首次消费后，需要对其进行引导再次消费，时间区间应该在30天之内。 用户留存率1234567891011# 留存率 --&gt; 用户在第一次消费后，有多少比率进行了第二次消费# 与回流率的区别在于：留存倾向计算第一次消费，并有多个时间区间user_purchase_retention = pd.merge(left=data,right = order_dt_min.reset_index(), how = 'inner',on = 'user_id',suffixes=('','_min'))user_purchase_retention['order_dt'] = pd.to_datetime(user_purchase_retention['order_dt'])user_purchase_retention['order_dt_min'] = pd.to_datetime(user_purchase_retention['order_dt_min'])user_purchase_retention['date_diff'] = (user_purchase_retention.order_dt - user_purchase_retention.order_dt_min) / np.timedelta64(1,'D')bin = [0,30,60,90,120,150,180,365]user_purchase_retention['date_diff_bin'] = pd.cut(user_purchase_retention['date_diff'],bins = bin)user_purchase_retention.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user_id order_dt order_products order_amount month order_dt_min date_diff date_diff_bin 0 vs30033073 2020-01-17 1 20 2020-01-01 2019-09-23 116.00 (90, 120] 1 vs30033073 2019-11-29 2 20 2019-11-01 2019-09-23 67.00 (60, 90] 2 vs30033073 2019-11-13 2 20 2019-11-01 2019-09-23 51.00 (30, 60] 3 vs30033073 2019-12-24 2 20 2019-12-01 2019-09-23 92.00 (90, 120] 4 vs30033073 2019-10-29 2 20 2019-10-01 2019-09-23 36.00 (30, 60] 123456pivoted_retention = user_purchase_retention.pivot_table(index='user_id', columns = 'date_diff_bin', values='order_amount', aggfunc=sum, dropna=False)pivoted_retention.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date_diff_bin (0, 30] (30, 60] (60, 90] (90, 120] (120, 150] (150, 180] (180, 365] user_id vs10000005 17.00 59.00 nan nan nan nan 80.00 vs10000621 240.00 280.00 440.00 400.00 200.00 40.00 1700.00 vs10000627 0.00 nan nan nan nan nan nan vs10000716 280.00 795.00 240.00 220.00 420.00 300.00 341.00 vs10000743 nan nan nan nan nan nan nan 1pivoted_retention.mean() date_diff_bin (0, 30] 47.88 (30, 60] 148.62 (60, 90] 169.92 (90, 120] 310.32 (120, 150] 112.90 (150, 180] 112.93 (180, 365] 700.36 dtype: float64 1234567pivoted_retention.transf = pivoted_retention.fillna(0).applymap(lambda x:1 if x&gt;0 else 0)(pivoted_retention.transf.sum() / pivoted_retention.transf.count()).plot.bar()plt.title('各时间段的用户频率',fontsize = 24)plt.xlabel('时间跨度(天)')plt.xticks(rotation = 360)plt.ylabel('百分比')plt.show() 图表解析： 第一个月留存率超过了25%，而第二个月下降至15%左右 之后几个月的留存率基本稳定在5%~10%之间，说明后面第二个月开始的流失率较大，且呈持续流失状态 123456789# 消费召回分析def diff(group): d = group.date_diff.shift(-1) - group.date_diff return dlast_diff = user_purchase_retention.sort_values('order_dt').reset_index().groupby('user_id').apply(diff)last_diff.head() user_id vs10000005 32 0.40 33 41.60 155 1.00 160 0.46 161 230.16 Name: date_diff, dtype: float64 1last_diff.describe() count 1766.00 mean 4.50 std 14.03 min 0.00 25% 0.81 50% 1.43 75% 3.57 max 230.16 Name: date_diff, dtype: float64 12345last_diff.hist(bins = 15)plt.title('用户平均购买周期',fontsize = 23)plt.xlabel('时间跨度(天)')plt.ylabel('百分数')plt.show() 图表解析： 图像呈典型的长尾分布，说明大部分的用户消费时间间隔短 从统计数据来看，用户平均消费间隔为4.5天，说明需要在4.5天左右的消费间隔对用户进行引导召回 总结以上为健身平台用户消费行为分析的全部内容，感谢阅读。","link":"/2021/04/14/%E5%81%A5%E8%BA%AB%E5%B9%B3%E5%8F%B0%E4%BC%9A%E5%91%98%E7%94%A8%E6%88%B7%E6%B6%88%E8%B4%B9%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/"},{"title":"某壳网房屋租赁信息抓取与浅析","text":"摘要收集整合房屋租赁信息，并加以简单的分析及可视化～ 项目描述项目背景：最近在为租房的事情所烦恼，在网站上寻找各种房源信息，看的眼花缭乱。萌生了将房源基本信息汇总起来便于查看的想法，便用python将基本信息爬下来放到excel，并整理成可分析的数据。 数据来源：利用python获取网页代码，并用lxml中的xpath提取信息，最后将数据保存到execl文件 字段意义： community: 小区名称 address：租房地址 landlord：房屋来源 postime：发布时间 introduction：租房说明 price：房屋价格 area：房屋面积 orientation：房屋朝向 pattern：房屋户型 floor：房屋楼层 date_time：查看日期 项目目的：整合某壳网房源信息，方便查看及分析 环境解释： 基于jupyter notebook 爬虫所用库：requests、lxml、xlsxwriter 数据处理所用库：pandas、numpy 可视化：execl 数据获取 利用requests库获取网页信息 lxml解析网页信息 利用xlsxwriter模块将数据保存至execl 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# 导入模块import requestsimport timefrom lxml import etreeimport xlsxwriterdef get_html(page): &quot;&quot;&quot;获取网站html代码&quot;&quot;&quot; url = &quot;https://dg.zu.ke.com/zufang/pg{}/#contentList&quot;.format(page) headers = { 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36' } response = requests.get(url, headers=headers).text return responsedef parse_html(htmlcode, data): &quot;&quot;&quot;解析html代码&quot;&quot;&quot; content = etree.HTML(htmlcode) results = content.xpath('///div[@class=&quot;content__article&quot;]/div[1]/div') for result in results[:]: community = result.xpath('./div[1]/p[@class=&quot;content__list--item--title twoline&quot;]/a/text()')[0].replace('\\n', '').strip().split()[ 0] address = &quot;-&quot;.join(result.xpath('./div/p[@class=&quot;content__list--item--des&quot;]/a/text()')) landlord = result.xpath('./div/p[@class=&quot;content__list--item--brand oneline&quot;]/text()')[0].replace('\\n', '').strip() if len( result.xpath('./div/p[@class=&quot;content__list--item--brand oneline&quot;]/text()')) &gt; 0 else &quot;&quot; postime = result.xpath('./div/p[@class=&quot;content__list--item--time oneline&quot;]/text()')[0] introduction = &quot;,&quot;.join(result.xpath('./div/p[@class=&quot;content__list--item--bottom oneline&quot;]/i/text()')) price = result.xpath('./div/span/em/text()')[0] description = &quot;&quot;.join(result.xpath('./div/p[2]/text()')).replace('\\n', '').replace('-', '').strip().split() area = description[0] count = len(description) if count == 6: orientation = description[1] + description[2] + description[3] + description[4] elif count == 5: orientation = description[1] + description[2] + description[3] elif count == 4: orientation = description[1] + description[2] elif count == 3: orientation = description[1] else: orientation = &quot;&quot; pattern = description[-1] floor = &quot;&quot;.join(result.xpath('./div/p[2]/span/text()')[1].replace('\\n', '').strip().split()).strip() if len( result.xpath('./div/p[2]/span/text()')) &gt; 1 else &quot;&quot; date_time = time.strftime(&quot;%Y-%m-%d&quot;, time.localtime()) &quot;&quot;&quot;数据存入字典&quot;&quot;&quot; data_dict = { &quot;community&quot;: community, &quot;address&quot;: address, &quot;landlord&quot;: landlord, &quot;postime&quot;: postime, &quot;introduction&quot;: introduction, &quot;price&quot;: '￥' + price, &quot;area&quot;: area, &quot;orientation&quot;: orientation, &quot;pattern&quot;: pattern, &quot;floor&quot;: floor, &quot;date_time&quot;: date_time } data.append(data_dict)def excel_storage(response): &quot;&quot;&quot;将字典数据写入excel&quot;&quot;&quot; workbook = xlsxwriter.Workbook('./beikeHouse.xlsx') worksheet = workbook.add_worksheet() &quot;&quot;&quot;设置标题加粗&quot;&quot;&quot; bold_format = workbook.add_format({'bold': True}) worksheet.write('A1', '小区名称', bold_format) worksheet.write('B1', '租房地址', bold_format) worksheet.write('C1', '房屋来源', bold_format) worksheet.write('D1', '发布时间', bold_format) worksheet.write('E1', '租房说明', bold_format) worksheet.write('F1', '房屋价格', bold_format) worksheet.write('G1', '房屋面积', bold_format) worksheet.write('H1', '房屋朝向', bold_format) worksheet.write('I1', '房屋户型', bold_format) worksheet.write('J1', '房屋楼层', bold_format) worksheet.write('K1', '查看日期', bold_format) row = 1 col = 0 for item in response: worksheet.write_string(row, col, item['community']) worksheet.write_string(row, col + 1, item['address']) worksheet.write_string(row, col + 2, item['landlord']) worksheet.write_string(row, col + 3, item['postime']) worksheet.write_string(row, col + 4, item['introduction']) worksheet.write_string(row, col + 5, item['price']) worksheet.write_string(row, col + 6, item['area']) worksheet.write_string(row, col + 7, item['orientation']) worksheet.write_string(row, col + 8, item['pattern']) worksheet.write_string(row, col + 9, item['floor']) worksheet.write_string(row, col + 10, item['date_time']) row += 1 workbook.close()def main(): all_datas = [] &quot;&quot;&quot;网页循环&quot;&quot;&quot; for page in range(1, 100): html = get_html(page) parse_html(html, all_datas) excel_storage(all_datas)if __name__ == '__main__': main() 数据处理获取到数据后，必然要对数据进行清洗与整理。 123456789import pandas as pdimport numpy as npimport osos.chdir(r'/Users/nanb/Documents/数据存放')import warningswarnings.filterwarnings('ignore')df = pd.read_excel('beikeHouse.xlsx')df.drop(['房屋来源','租房说明','查看日期'],axis=1,inplace = True) 房屋朝向1234567891011121314151617181920212223242526272829# 房屋朝向df.apply(lambda x:sum(x.isnull()))df = df[~df['房屋朝向'].isnull()]# df['房屋朝向'].value_counts()# 类似'在租'值 -- 处理df = df[~df['房屋朝向'].str.contains('在租')]# df['房屋朝向'].value_counts()# 错误值简单处理df['房屋朝向'][df['房屋朝向'].isin(['东南南','东东南','东东南南'])] = '东南'df['房屋朝向'][df['房屋朝向'].isin(['西西北','西北北'])] = '西北'df['房屋朝向'][df['房屋朝向'].isin(['东东北'])] = '东北'df['房屋朝向'][df['房屋朝向'].isin(['南西南'])] = '西南'# df['房屋朝向'].value_counts()# 数据筛选fwcx_counts = df['房屋朝向'].value_counts()to_remove = fwcx_counts[fwcx_counts &lt;= 5].indexdf.replace(to_remove,np.nan,inplace=True)df = df[~df['房屋朝向'].isnull()] 租房区域 根据租房地址生成新字段：租房区域 123# 租房区域df['租房区域'] = df['租房地址'].apply(lambda x:x[0:3]) 房屋价格 对房屋价格进行处理并生成新字段：价格类型 因为本次获取的数据仅为租房信息，依据数据观察，房屋价格大多处于1000～2500之间 故价格类型分成5个level：1000以下、1001～1500、1501～2000、2001~2500、2500以上 1df['房屋价格'] = df['房屋价格'].apply(lambda x:x.replace('￥','')) 1df['房屋价格'][df['房屋价格'].str.contains('-')] = df['房屋价格'][df['房屋价格'].str.contains('-')].apply(lambda x:x.split('-',1)[1]) 1df['房屋价格'] = df['房屋价格'].apply(lambda x:int(x)) 123456# 价格类别lst_bins = [0,1001,1501,2001,2501,1000000]lst_labels = ['1000以下','1001~1500','1501~2000','2001~2500','2500以上']df['价格类型'] = pd.cut(df['房屋价格'],bins = lst_bins,labels = lst_labels,include_lowest=True) 户型区分 为方便数据查看以及选择，根据房屋户型生成3个新字段： 户型(室)、户型(厅)、户型(卫) 1234# 户型(室)df = df[~df['房屋户型'].str.contains('未知')]df['户型(室)'] = df['房屋户型'].apply(lambda x:x.partition('室')[0]) 123# 户型(厅)df['户型(厅)'] = df['房屋户型'].apply(lambda x:x.split('室')[1][0]) 123# 户型(卫)df['户型(卫)'] = df['房屋户型'].apply(lambda x:x.split('厅')[1][0]) 房屋楼层 根据数据观察，房屋楼层中地下室在网页中主图显示基本为车位，故将地下室更换成车位，便于区分 123456789# 房屋楼层df['房屋楼层'] = df['房屋楼层'].apply(lambda x:str(x)[0:3])# df[df['房屋楼层'] == 'nan'] = '未知'df['房屋楼层'][df['房屋楼层'] == '地下室'] = '车位'#df['房屋楼层'].value_counts() 租赁类型 根据小区名称生成新字段：租赁类型 租赁类型：整租、合租 12df['租赁类型'] = df['小区名称'].apply(lambda x:x[0:2])df['小区名称'] = df['小区名称'].apply(lambda x:x.split('·')[1]) 当月数据提取以天为单位 将房源的上新时间为天的数据提取出来，生成当月上新数据。 处理逻辑：发布时间为类似 ‘1天前发布’ 这种数据整合为当月上新数据，并更改数据类型为int 1234567# 日报df1 = df[df['发布时间'].str.contains('天')]# 数据截取生成新字段df1['上新时间'] = df1['发布时间'].apply(lambda x:x[0:2]) 1df1['上新时间'] = df1['上新时间'].str.replace('今','0') 1df1['上新时间'] = df1['上新时间'].str.replace('天','') 1df1['上新时间'] = df1['上新时间'].apply(lambda x:int(x)) 以周为单位 将当月上新数据，以周为单位做区分 说明：这里仅以一周为7天来做区间分类，并未加入实际日期进行换算，仅做简单处理 处理逻辑：上新时间为0～7天为第一周，8～14天为第二周，依此类推 1234567# 周报df2 = df1.copy()lst_bins = [0,8,15,22,31]lst_labels = [1,2,3,4]df2['上新周数'] = pd.cut(df1['上新时间'],bins = lst_bins,labels = lst_labels,include_lowest=True) 单间分类 提取户型(室)为1的数据整合为单间数据，并依据房屋面积大小生成新字段：单间分类 单间分类区间为： 0～30：极小单间、31～40：普通单间、41～50：大单间、51及以上：超大单间 12345678910111213# 一居室面积df3 = df[df['户型(室)'] == '1']df3['房屋面积'] = df3['房屋面积'].apply(lambda x:x.replace('㎡',''))df3['房屋面积'] = df3['房屋面积'].astype('int')df3 = df3[df3['房屋面积'] &lt;= 100]# 区间分类lst_bins = [0,31,41,51,1000000]lst_labels = ['极小单间','普通单间','大单间','超大单间']df3['单间分类'] = pd.cut(df3['房屋面积'],bins = lst_bins,labels = lst_labels,include_lowest=True) 数据保存123456# 删除无用字段df.drop(['发布时间','房屋户型'],axis=1,inplace = True)df1.drop(['发布时间','房屋户型'],axis=1,inplace = True)df2.drop(['发布时间','房屋户型'],axis=1,inplace = True)df3.drop(['发布时间','房屋户型'],axis=1,inplace = True) 写入execl12345with pd.ExcelWriter('finall_bkHouse.xlsx') as writer: df.to_excel(writer, sheet_name='源数据',index=False) df1.to_excel(writer, sheet_name='日报数据',index=False) df2.to_excel(writer, sheet_name='周报数据',index=False) df3.to_excel(writer, sheet_name='一居室面积数据',index=False) 可视化本次项目并没有用python的可视化模块去实现可视化分析，而是尝试使用execl来实现自动化报表可视化。 当月房源汇总 将发布时间为天的数据整合为当月房源数据，并添加价格类型的切片器，以便做数据筛选 可根据需求更换切片器内容字段 周上新汇总 以周为单位汇总上新数据 添加以价格类型为筛选的切片器以及周数筛选的切片器，以便数据查看与对比 楼层与价格分析 以柱状图显示不同价格类型的各个楼层统计 可看出无论低中高楼层，价格区间位于1000～2000占绝大多数 面积与楼层分析 制作饼图查看各个单间分类的占比，并结合楼层类别分析在不同楼层中的不同单间分类的统计 写在最后以上是对东莞地区租房信息的获取以及数据整理的过程，并利用execl进行部分数据可视化。若文中有误，恳请指正，感谢阅读～","link":"/2021/04/03/%E4%B8%9C%E8%8E%9E%E5%9C%B0%E5%8C%BA%E6%88%BF%E5%B1%8B%E7%A7%9F%E8%B5%81/"},{"title":"如何用图表直观显示数据的分布情况？","text":"摘要数据分布可视化图表绘制，包括直方图、密度图、柱状图、折线图的一些实例～ 写在前面今天主要聊聊关于数据分布情况的可视化所用到的一部分图表 什么是分布数据：定量化的数据，而非定性化的数据，一般指只是数值的数据 什么是分布数据可视化：查看数据分布情况以及数据分布统计时做的图表可视化 直方图 直方图主要反映数据分布情况 导入模块12345import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 设置绘图风格123456789# 设置风格、尺度sns.set_style('darkgrid')sns.set_context('paper')# 警告处理import warningswarnings.filterwarnings('ignore') 绘制直方图-1123456789101112131415161718192021# 创建数据rs = np.random.RandomState(10)s = pd.Series(rs.randn(100) * 100)# 绘制直方图及基本参数设置sns.distplot(s,bins = 10,hist = True,kde = True,norm_hist = False, rug = True,vertical = False, color = 'y',label = 'distplot',axlabel = 'x')plt.legend()# 参数解析：# bins → 箱子数量# hist、kde → 是否显示箱/密度曲线# norm_hist → 直方图是否按照密度来显示# rug → 是否显示数据分布# vertical → 是否水平显示# color → 设置颜色# label → 设置图例# axlabel → x轴标注 &lt;matplotlib.legend.Legend at 0xf049090&gt; 绘制直方图-2123456789101112sns.distplot(s,rug = True, # 设置数据频率分布 rug_kws = {'color':'g'}, # 设置密度曲线颜色、线宽、标注、线形 kde_kws = {'color':'k','lw':1,'label':'KDE','linestyle':'--'}, # 设置箱子风格、线宽、透明度、颜色 # 风格包括：'bar' 'barstacked' 'step' 'stepfilled' hist_kws = {'histtype':'step','linewidth':'1','alpha':1,'color':'g'} ) &lt;matplotlib.axes._subplots.AxesSubplot at 0xf4fc9f0&gt; 密度图 密度图与直方图用途一致，都是用于反映数据的分布情况，可视化效果炫酷一些，看个人喜好选择 单样本密度图12345678910111213141516171819202122# 密度图 - kdeplot()sns.kdeplot(s, # 是否填充 shade = True, color = 'r', vertical = False)sns.kdeplot(s,bw = 5,label = 'bw: 0.2', linestyle = '-',linewidth = 1.2,alpha = 0.5)sns.kdeplot(s,bw = 20,label = 'bw: 2', linestyle = '-',linewidth = 1.2,alpha = 0.5)# 参数解析：# bw：控制拟合的程度,类似直方图的箱数# 数据频率分布图sns.rugplot(s,height = 0.1,color = 'k',alpha = 0.6) &lt;matplotlib.axes._subplots.AxesSubplot at 0xca6b90&gt; 多样本密度图-1123456789101112131415161718192021222324252627# 密度图 - 两个样本数据密度分布图rs = np.random.RandomState(2)df = pd.DataFrame(rs.randn(100,2), columns = ['a','b'])sns.kdeplot(df['a'],df['b'], # 是否显示颜色图例 cbar = True, # 是否填充 shade = True, # 设置调色盘 cmap = 'Reds', # 最外围颜色是否显示 shade_lowest = False, # 曲线个数(如果非常多,则会越平滑) n_levels = 10, )# x,y轴设置sns.rugplot(df['a'],color = 'g',axis = 'x',alpha = 0.5)sns.rugplot(df['b'],color = 'r',axis = 'y',alpha = 0.5) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10bf0170&gt; 多样本密度图-21234567891011121314# 密度图 - 两个样本数据密度分布 # 多个密度图rs1 = np.random.RandomState(2)rs2 = np.random.RandomState(5)df1 = pd.DataFrame(rs1.randn(100,2)+2,columns=['a','b'])df2 = pd.DataFrame(rs2.randn(100,2)-2,columns=['a','b'])sns.kdeplot(df1['a'],df1['b'],cmap = 'Greens',shade = True, shade_lowest = False)sns.kdeplot(df2['a'],df2['b'],cmap = 'Blues',shade = True, shade_lowest = False) &lt;matplotlib.axes._subplots.AxesSubplot at 0x11147bf0&gt; 柱状图 barplot ：绘制柱状图 分布统计柱状图123456789101112131415161718192021222324252627# 置信区间：样本均值 + 抽样误差titanic = sns.load_dataset('titanic')plt.figure(figsize = (15,7))sns.barplot(x = 'sex',y = 'survived',hue = 'class',data = titanic, palette = 'hls', order = ['male','female'], capsize = 0.05, saturation = 8, # 误差线颜色 errcolor = 'gray', # 误差线宽度 errwidth = 2, # 置信区间误差 → 0-100内,'sd' None ci = 'sd', )# print(titanic.groupby(['sex','class']).mean()['survived'])# print('----------------------------------------------')# print(titanic.groupby(['sex','class']).std()['survived'])titanic.head() 柱状图拆分12345678# 二次拆分tips = sns.load_dataset('tips')plt.figure(figsize = (15,7))sns.barplot(x = 'day',y = 'total_bill',hue = 'sex',data = tips, palette = 'hls',edgecolor = 'w')tips.groupby(['day','sex']).mean() 实例应用12345678910111213141516# 例子crashes = sns.load_dataset('car_crashes').sort_values('total',ascending = False)print(crashes.head())fig,axes = plt.subplots(figsize = (12,17))sns.set_color_codes('pastel')sns.barplot(x = 'total',y = 'abbrev',data = crashes, label = 'Total',color = 'b',edgecolor = 'w')sns.set_color_codes('muted')sns.barplot(x = 'alcohol',y = 'abbrev',data = crashes, label = 'Alcohol-involved',color = 'b',edgecolor = 'w')axes.legend(ncol = 2,loc = 'lower-right')sns.despine(left = True,bottom = True) 12345678910111213 total speeding alcohol not_distracted no_previous ins_premium \\40 23.9 9.082 9.799 22.944 19.359 858.97 34 23.9 5.497 10.038 23.661 20.554 688.75 48 23.8 8.092 6.664 23.086 20.706 992.61 3 22.4 4.032 5.824 21.056 21.280 827.34 17 21.4 4.066 4.922 16.692 16.264 872.51 ins_losses abbrev 40 116.29 SC 34 109.72 ND 48 152.56 WV 3 142.39 AR 17 137.13 KY 计数柱状图123456789# countplot() - 计数柱状图plt.figure(figsize = (12,9))# sns.countplot(x = 'class',hue = 'who',data = titanic,palette='magma')sns.countplot(y = 'class',hue = 'who',data = titanic,palette='magma')# x / y → 以x或者y轴绘图(横向、竖向) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x15dc5cb0&gt; 折线图 pointplot：绘制折线图，置信区间的直观显示 123456789# pointplot() - 折线图 - 置信区间估计sns.pointplot(x = 'time',y = 'total_bill',hue = 'smoker',data = tips, palette = 'hls', dodge = True, join = True, markers = ['o','x'],linestyle = ['-','--'])tips.groupby(['time','smoker']).mean()['total_bill'] 123456time smokerLunch Yes 17.399130 No 17.050889Dinner Yes 21.859429 No 20.095660Name: total_bill, dtype: float64 总结以上为研究数据分布时的可视化图表中较为常用的几种图表展示，看起来不难，实际上需要注意的是对图表背后数据结构的理解，图表可视化只是一种手段，真正重心还是在能否真正理解背后的数据逻辑，感谢阅读～ 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/19/%E5%8F%AF%E8%A7%86%E5%8C%96%20-%20%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E5%9B%BE%E8%A1%A8part1/"},{"title":"一眼就get到数据分布情况的图表？","text":"摘要数据分布可视化图表绘制，包括散点图、蜂窝图、箱型图、小提琴图、LV图表的一些实例～ 前言上篇文章介绍了数据分布情况的可视化的四种图表 (直方图、密度图、柱状图、折线图)的 展示方法，下面将介绍另外几种直观显示数据分布情况的可视化图表 散点图 也称为「点图」、「散布图」或「X-Y 点图」。 所谓的散点图 (Scatterplot) 就是在笛卡尔座标上放置一系列的数据点，用来显示两个变量的数值（每个轴上显示一个变量），并检测两个变量之间的关系或相关性是否存在。 可以很直观的观察到数据的分布情况 导入模块12345678910import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline# 警告处理import warningswarnings.filterwarnings('ignore') 散点图 + 分布图12345678910111213141516171819# 1、综合散点图 - jointplot()rs = np.random.RandomState(2)df = pd.DataFrame(rs.randn(200,2),columns = ['a','b'])sns.jointplot(x = df['a'],y = df['b'], data = df, color = 'k', s = 50,edgecolor = 'w',linewidth = 1, # 类型有'reg','resid','kde','hex','scatter' kind = 'scatter', space = 0.3, size = 8, ratio = 5, # 类型为kde时不能设置marginal_kws marginal_kws = dict(bins = 15,rug = True) ) &lt;seaborn.axisgrid.JointGrid at 0x5759510&gt; 六边形图 / 蜂窝图12345678# 构建数据df = pd.DataFrame(rs.randn(500,2),columns = ['a','b'])with sns.axes_style('white'): sns.jointplot(x = df['a'],y = df['b'],kind = 'hex', color = 'k', marginal_kws = dict(bins = 20)) 密度图 + 散点图12345678910# 密度图rs = np.random.RandomState(15)df = pd.DataFrame(rs.randn(300,2),columns = ['a','b'])g = sns.jointplot(x = df['a'],y = df['b'],data = df, kind = 'kde', color = 'k', shade_lowest = False)g.plot_joint(plt.scatter,c = 'w', s = 30,linewidth = 1,marker = '+') &lt;seaborn.axisgrid.JointGrid at 0x1431ec90&gt; 综合散点图1234567891011121314151617181920# 综合散点图 - JointGrid()# 可拆分绘制的散点图 plot_joint() / ax_marg_x.hist() / ax_marg_y.hist()sns.set_style('white')tips = sns.load_dataset('tips')print('tips.head()')g = sns.JointGrid(x = 'total_bill',y = 'tip',data = tips)g.plot_joint(plt.scatter,color = 'm',edgecolor = 'white')g.ax_marg_x.hist(tips['total_bill'],color = 'b',alpha = 0.6, bins = np.arange(0,60,3))g.ax_marg_y.hist(tips['tip'],color = 'r',alpha = 0.6, orientation = 'horizontal',bins = np.arange(0,12,1))from scipy import statsg.annotate(stats.pearsonr)plt.grid(linestyle = '--') 拆分绘制 - 散点图 分别绘制散点图和直方图 1234567891011# 可拆分绘制的散点图# plot_joint() / plot_marginals()# 直方图g = sns.JointGrid(x = 'total_bill' , y = 'tip' , data = tips)g = g.plot_joint(plt.scatter,color = 'g',s = 40,edgecolor = 'white')plt.grid(linestyle = '--')g.plot_marginals(sns.distplot,kde = True,color = 'g') &lt;seaborn.axisgrid.JointGrid at 0x154c0690&gt; 拆分绘制 - 密度图 分别绘制密度图和面积图 1234567891011# 可拆分绘制的散点图# plot_joint() / plot_marginals()# 密度图g = sns.JointGrid(x = 'total_bill' , y = 'tip' , data = tips)g = g.plot_joint(sns.kdeplot,cmap = 'Reds_r')plt.grid(linestyle = '--')g.plot_marginals(sns.kdeplot,shade = True,color = 'r') &lt;seaborn.axisgrid.JointGrid at 0x154b13b0&gt; 矩阵散点图 散点图矩阵是组织成网格（矩阵）形式的散点图集合。每个散点图显示一对变量之间的关系。 123456789101112131415161718192021222324252627# 矩阵散点图 - pairplot()# 定义风格sns.set_style('white')# 引入数据iris = sns.load_dataset('iris')print(iris.head())sns.pairplot(iris, # 散点图 / 回归分布图('scatter','reg') kind = 'scatter', # 直方图 / 密度图'hist','kde' diag_kind = 'hist', # 按照某一字段进行分类 hue = 'species', # 设置调色板 palette = 'husl', # 设置不同系列的点样式(参考分类个数) markers = ['o','s','D'], size = 3) sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa &lt;seaborn.axisgrid.PairGrid at 0x15511310&gt; 局部变量对比12345# 只提取局部变量进行对比sns.pairplot(iris,vars = ['sepal_width','sepal_length'], kind = 'reg',diag_kind = 'kde', hue = 'species',palette = 'husl',size = 5) &lt;seaborn.axisgrid.PairGrid at 0x1d94b510&gt; 多类显示12345# 其他参数设置sns.pairplot(iris , diag_kind = 'kde',markers = '+', plot_kws = dict(s = 50,edgecolor = 'b',linewidth = 1), diag_kws = dict(shade = True)) &lt;seaborn.axisgrid.PairGrid at 0x14feffb0&gt; 拆分绘制 - 散点图21234567891011121314151617181920# 可拆分绘制的散点图 - PairGrid()# map_diag() / map_offdiag()g = sns.PairGrid(iris,hue = 'species',palette = 'hls', vars = ['sepal_length','sepal_width','petal_length','petal_width'], size = 3)# 创建一个绘图表格区域,设置好x,y对应的数据,按照species分类g.map_diag(plt.hist, histtype = 'barstacked',linewidth = 1,edgecolor = 'w')# 对角线图表,plt.hist / sns.kdeplotg.map_offdiag(plt.scatter,edgecolor = 'w',s = 40,linewidth = 1)# 其他图表g.add_legend()# 添加图例 &lt;seaborn.axisgrid.PairGrid at 0x21652e90&gt; 分类散点图绘制分类散点图12345678910111213141516171819# stripplot() - 按照不同类别对样本数据进行分布散点图绘制sns.set_style('whitegrid')sns.set_context('paper')tips = sns.load_dataset('tips')print(tips.head())sns.stripplot(x = 'day', # 设置分组统计字段 y = 'total_bill', # 数据分布统计字段 data = tips, jitter = True, # 当点数据重合较多时,用该参数调整,可以设置间距如：jitter = 0.1 size = 5,edgecolor = 'w',linewidth = 1,marker = 'o') total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 &lt;matplotlib.axes._subplots.AxesSubplot at 0xc12130&gt; 二次分类1234# 通过hue参数再分类sns.stripplot(x = 'sex',y = 'total_bill',hue = 'day', data = tips,jitter = True) &lt;matplotlib.axes._subplots.AxesSubplot at 0xf54fed0&gt; 二次拆分1234567891011# 二次拆分sns.stripplot(x = 'sex',y = 'total_bill',hue = 'day', data = tips,jitter = True, # 设置调色盘 palette = 'Set2', # 是否拆分 dodge = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0xf5524d0&gt; 二次筛选1234567# 筛选分类类别print(tips['day'].value_counts())sns.stripplot(x = 'total_bill',y = 'day',data = tips,jitter = True, order = ['Sat','Sun'])# order → 筛选类别 Sat 87 Sun 76 Thur 62 Fri 19 Name: day, dtype: int64 &lt;matplotlib.axes._subplots.AxesSubplot at 0xb840b0&gt; 分簇散点图12345# swarmplot() - 分簇散点图sns.swarmplot(x = 'day',y = 'total_bill',data = tips, size = 5,edgecolor = 'w',linewidth = 1,marker = 'o', palette = 'Reds') &lt;matplotlib.axes._subplots.AxesSubplot at 0xa53c70&gt; 箱型图 boxplot ：又称为盒须图、盒式图或箱线图，是一种用作显示一组数据分散情况资料的统计图。因形状如箱子而得名 初步绘制123456789101112131415161718192021222324252627# 定义风格sns.set_style('whitegrid')sns.set_context('paper')# 引入数据tips = sns.load_dataset('tips')sns.boxplot(x = 'day',y = 'total_bill',data = tips, linewidth = 2, width = 0.5, # 异常点大小 fliersize = 3, # 调色板 palette = 'hls', # 设置IQR whis = 1.5, # 是否以中值做凹槽 notch = False, order = ['Thur','Fri','Sat','Sun'])# sns.swarmplot(x = 'day',y = 'total_bill',data = tips,color = 'k',size = 3,alpha = 0.7) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x614e2b0&gt; 二次分类12345# 二次分类sns.boxplot(x = 'day',y = 'total_bill',data = tips,hue = 'sex',palette = 'Reds')# sns.swarmplot(x = 'day',y = 'total_bill',data = tips,color = 'k',size = 3,alpha = 0.7) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x62316f0&gt; 小提琴图 小提琴图其实是箱线图与核密度图的结合，通过小提琴图能更容易看出哪些位置的密度较高，即数据分布的区域 初步绘制123456789101112131415161718192021222324# 小提琴图 - violinplot()sns.violinplot(x = 'day',y = 'total_bill',data = tips, # 线宽 linewidth = 2, # 箱之间的间隔比例 width = 0.8, # 调色盘 palette = 'hls', order = ['Thur','Fri','Sat','Sun'], # 小提琴图的宽度：area-面积相同,count-样本数量决定宽度,width-宽度一样 scale = 'area', # 设置小提琴图边线的平滑度,值越大越平滑 gridsize = 50, # 设置内部显示类型：'box' 'quartile' 'point' 'stick' None inner = 'box', # bw = 0.8 ) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x10c7eab0&gt; 二次分类12345678# 二次分类sns.violinplot(x = 'day',y = 'total_bill',data = tips, hue = 'smoker',palette = 'muted', # 设置是否拆分小提琴图 split = True, inner = 'box') 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x110572d0&gt; 混合图表 小提琴图结合散点图 12345# 小提琴图 - 结合散点图sns.violinplot(x = 'day',y = 'total_bill',data = tips,palette = 'hls',inner = None)sns.swarmplot(x = 'day',y = 'total_bill',data = tips,color = 'w',alpha = 0.5)# 插入散点图 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x11224ed0&gt; LV图表12345678910111213141516# lvplot() - lv图表plt.figure(figsize = (15,7))sns.lvplot(x = 'day',y = 'total_bill',data = tips,palette = 'mako', width = 0.8, hue = 'smoker', linewidth = 12, # 设置框的大小 → 'linear','exonential','area' scale = 'area', # 设置框的数量 → 'proportion','tukey','trustworthy' k_depth = 'proportion', )sns.swarmplot(x = 'day',y = 'total_bill',data = tips,color = 'k',size = 6,alpha = 0.8) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x11263f30&gt; 写在最后这两篇博文列举了对数据分布情况的基础可视化图表，当然只是很基础的图表展示，记录下来以便随时翻阅，查漏补缺，还是那句话，任何图表可视化重点是数据结构与其逻辑，真正理解了数据才能更好的选择图表去展示，感谢阅读～ 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/22/%E5%8F%AF%E8%A7%86%E5%8C%96%20-%20%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E5%9B%BE%E8%A1%A8part2/"},{"title":"剖析天猫双十一的美妆产品销售数据","text":"摘要深度挖掘天猫双十一时刻的美妆产品销售数据，分析各品牌之间的销量、热度、价格等～ 项目描述项目名称：天猫双十一美妆产品销售数据分析 数据来源：数据来源于网络，为2016年双十一美妆产品天猫数据的采集和整理，原始数据为.xlsx格式。 字段说明： update_time : 更新时间 id : 商品id(唯一标识) title : 商品标题 price : 价格 店名 : 即品牌名 项目目的： 通过对数据集进行清理整合，并进行以下几个方面的分析： 双十一期间各品牌的销量以及销售额对比 各大品牌热度 各品牌价格分析 关于男性护肤品销量分析 消费高峰期分析 环境解释：基于jupyter notebook，可视化工具包：matplotlib、seaborn 一、数据导入12345678910111213# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as ssimport warnings%matplotlib inlineimport osos.chdir(&quot;/home/min/data&quot;)warnings.filterwarnings(&quot;ignore&quot;) 1234567# 数据读取data = pd.read_csv('双十一淘宝美妆数据.csv')# 数据查看data.head() 123# 数据基本属性data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 27598 entries, 0 to 27597 Data columns (total 7 columns): update_time 27598 non-null object id 27598 non-null object title 27598 non-null object price 27598 non-null float64 sale_count 25244 non-null float64 comment_count 25244 non-null float64 店名 27598 non-null object dtypes: float64(3), object(4) memory usage: 1.5+ MB 123# 店名统计data['店名'].value_counts() 悦诗风吟 3021 佰草集 2265 欧莱雅 1974 雅诗兰黛 1810 倩碧 1704 美加净 1678 欧珀莱 1359 妮维雅 1329 相宜本草 1313 兰蔻 1285 娇兰 1193 自然堂 1190 玉兰油 1135 兰芝 1091 美宝莲 825 资生堂 821 植村秀 750 薇姿 746 雅漾 663 雪花秀 543 SKII 469 蜜丝佛陀 434 Name: 店名, dtype: int64 二、数据清洗2.1 清除重复数据1234# drop_duplicates -- 重复数据data = data.drop_duplicates(inplace=False)data.shape (27512, 7) 2.2 索引重构1234# 重置索引data.reset_index(inplace=True,drop = True)data.head() 2.3 日期处理123456# 日期处理data['date'] = pd.to_datetime(data['update_time'])data['month'] = data['date'].dt.monthdata['day'] = data['date'].dt.daydata.head() 123# 查看数据描述性统计data.describe() 123# 查看缺失值data.apply(lambda x:sum(x.isnull())) update_time 0 id 0 title 0 price 0 sale_count 2350 comment_count 2350 店名 0 date 0 month 0 day 0 dtype: int64 1data.isnull().any() update_time False id False title False price False sale_count True comment_count True 店名 False date False month False day False dtype: bool 2.4 缺失值处理1234# sale_count &amp; comment_count 考虑使用众数填充data['sale_count'].mode()data['comment_count'].mode() 0 0.0 dtype: float64 12data['sale_count'] = data['sale_count'].fillna(0)data['comment_count'] = data['comment_count'].fillna(0) 123# 再次确认缺失值data.apply(lambda x : sum(x.isnull())) update_time 0 id 0 title 0 price 0 sale_count 0 comment_count 0 店名 0 date 0 month 0 day 0 dtype: int64 1data.duplicated().value_counts() False 27512 dtype: int64 2.5 信息提取2.5.1 表格信息提取123# 表格信息提取import jieba 1234title_cut = []for i in data.title: j = jieba.lcut(i) title_cut.append(j) Building prefix dict from the default dictionary ... Loading model from cache /tmp/jieba.cache Loading model cost 1.520 seconds. Prefix dict has been built successfully. 12data['item_name_cut'] = title_cutdata[['title','item_name_cut']].head() 2.6 添加商品类别1234567891011121314151617181920212223# 给商品添加分类# 子类别sub_type = []# 主类别main_type = []basic_config_data = &quot;&quot;&quot;护肤品 套装 套装 护肤品 乳液类 乳液 美白乳 润肤乳 凝乳 柔肤液' 亮肤乳 菁华乳 修护乳护肤品 眼部护理 眼霜 眼部精华 眼膜 护肤品 面膜类 面膜 护肤品 清洁类 洗面 洁面 清洁 卸妆 洁颜 洗颜 去角质 磨砂 护肤品 化妆水 化妆水 爽肤水 柔肤水 补水露 凝露 柔肤液 精粹水 亮肤水 润肤水 保湿水 菁华水 保湿喷雾 舒缓喷雾护肤品 面霜类 面霜 日霜 晚霜 柔肤霜 滋润霜 保湿霜 凝霜 日间霜 晚间霜 乳霜 修护霜 亮肤霜 底霜 菁华霜护肤品 精华类 精华液 精华水 精华露 精华素 护肤品 防晒类 防晒霜 防晒喷雾 化妆品 口红类 唇釉 口红 唇彩 化妆品 底妆类 散粉 蜜粉 粉底液 定妆粉 气垫 粉饼 BB CC 遮瑕 粉霜 粉底膏 粉底霜 化妆品 眼部彩妆 眉粉 染眉膏 眼线 眼影 睫毛膏 化妆品 修容类 鼻影 修容粉 高光 腮红 其他 其他 其他&quot;&quot;&quot; 123456789category_config_map = {}for config_line in basic_config_data.split('\\n'): basic_category_list = config_line.strip().strip('\\n').strip('\\t').split('\\t') main_category = basic_category_list[0] sub_category = basic_category_list[1] unit_category_list = basic_category_list[2:-1] for unit_category in unit_category_list: if unit_category and unit_category.strip().strip('\\t'): category_config_map[unit_category] = (main_category,sub_category) 123# 查看主类别 &amp; 子类别category_config_map {'乳液': ('护肤品', '乳液类'), '美白乳': ('护肤品', '乳液类'), '润肤乳': ('护肤品', '乳液类'), '凝乳': ('护肤品', '乳液类'), &quot;柔肤液'&quot;: ('护肤品', '乳液类'), '亮肤乳': ('护肤品', '乳液类'), '菁华乳': ('护肤品', '乳液类'), '眼霜': ('护肤品', '眼部护理'), '眼部精华': ('护肤品', '眼部护理'), '洗面': ('护肤品', '清洁类'), '洁面': ('护肤品', '清洁类'), '清洁': ('护肤品', '清洁类'), '卸妆': ('护肤品', '清洁类'), '洁颜': ('护肤品', '清洁类'), '洗颜': ('护肤品', '清洁类'), '去角质': ('护肤品', '清洁类'), '化妆水': ('护肤品', '化妆水'), '爽肤水': ('护肤品', '化妆水'), '柔肤水': ('护肤品', '化妆水'), '补水露': ('护肤品', '化妆水'), '凝露': ('护肤品', '化妆水'), '柔肤液': ('护肤品', '化妆水'), '精粹水': ('护肤品', '化妆水'), '亮肤水': ('护肤品', '化妆水'), '润肤水': ('护肤品', '化妆水'), '保湿水': ('护肤品', '化妆水'), '菁华水': ('护肤品', '化妆水'), '保湿喷雾': ('护肤品', '化妆水'), '面霜': ('护肤品', '面霜类'), '日霜': ('护肤品', '面霜类'), '晚霜': ('护肤品', '面霜类'), '柔肤霜': ('护肤品', '面霜类'), '滋润霜': ('护肤品', '面霜类'), '保湿霜': ('护肤品', '面霜类'), '凝霜': ('护肤品', '面霜类'), '日间霜': ('护肤品', '面霜类'), '晚间霜': ('护肤品', '面霜类'), '乳霜': ('护肤品', '面霜类'), '修护霜': ('护肤品', '面霜类'), '亮肤霜': ('护肤品', '面霜类'), '底霜': ('护肤品', '面霜类'), '精华液': ('护肤品', '精华类'), '精华水': ('护肤品', '精华类'), '精华露': ('护肤品', '精华类'), '防晒霜': ('护肤品', '防晒类'), '唇釉': ('化妆品', '口红类'), '口红': ('化妆品', '口红类'), '散粉': ('化妆品', '底妆类'), '蜜粉': ('化妆品', '底妆类'), '粉底液': ('化妆品', '底妆类'), '定妆粉 ': ('化妆品', '底妆类'), '气垫': ('化妆品', '底妆类'), '粉饼': ('化妆品', '底妆类'), 'BB': ('化妆品', '底妆类'), 'CC': ('化妆品', '底妆类'), '遮瑕': ('化妆品', '底妆类'), '粉霜': ('化妆品', '底妆类'), '粉底膏': ('化妆品', '底妆类'), '眉粉': ('化妆品', '眼部彩妆'), '染眉膏': ('化妆品', '眼部彩妆'), '眼线': ('化妆品', '眼部彩妆'), '眼影': ('化妆品', '眼部彩妆'), '鼻影': ('化妆品', '修容类'), '修容粉': ('化妆品', '修容类'), '高光': ('化妆品', '修容类')} 1234567891011121314# 分类处理for i in range(len(data)): exist = False for temp in data.item_name_cut[i]: if temp in category_config_map: sub_type.append(category_config_map.get(temp)[1]) main_type.append(category_config_map.get(temp)[0]) exist = True break if not exist: sub_type.append('其他') main_type.append('其他') 2.6.1 新增主类别 &amp; 子类别1234# 新增 子类别 &amp; 主类别 两列data['sub_type'] = sub_typedata['main_type'] = main_type 123# 子类别统计data['sub_type'].value_counts() 其他 13100 清洁类 2922 面霜类 2675 化妆水 1955 底妆类 1790 乳液类 1352 眼部护理 1114 精华类 727 口红类 715 眼部彩妆 604 防晒类 494 修容类 64 Name: sub_type, dtype: int64 123# 主类别统计data['main_type'].value_counts() 其他 13100 护肤品 11239 化妆品 3173 Name: main_type, dtype: int64 2.6.2 新增 “是否为男士专用” 类别12345678910111213# 新增 &quot;是否为男士专用&quot; 列gender = []for i in range(len(data)): if '男' in data.item_name_cut[i]: gender.append('是') elif '男士' in data.item_name_cut[i]: gender.append('是') elif '男生' in data.item_name_cut[i]: gender.append('是') else: gender.append('否') 12data['是否男士专用'] = genderdata['是否男士专用'].value_counts() 否 25310 是 2202 Name: 是否男士专用, dtype: int64 2.7 新增销售额 &amp; 购买日期1234# 新增 销售额 &amp; 购买日期(天) 两列# 销售额 = 销售数量 × 价格data['销售额'] = data.sale_count * data.price 1234# 时间格式转换data['update_time'] = pd.to_datetime(data['update_time'])data['update_time'] 0 2016-11-14 1 2016-11-14 2 2016-11-14 3 2016-11-14 4 2016-11-14 ... 27507 2016-11-05 27508 2016-11-05 27509 2016-11-05 27510 2016-11-05 27511 2016-11-05 Name: update_time, Length: 27512, dtype: datetime64[ns] 2.8 时间列类型装换 &amp; 新增以天为单位时间列1234# 将时间列设置为新的index &amp; 新增时间列 &quot;天&quot;data = data.set_index('update_time')data['day'] = data.index.day 1del data['item_name_cut'] 123# 清理后数据查看data.head() 1data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; DatetimeIndex: 27512 entries, 2016-11-14 to 2016-11-05 Data columns (total 13 columns): id 27512 non-null object title 27512 non-null object price 27512 non-null float64 sale_count 27512 non-null float64 comment_count 27512 non-null float64 店名 27512 non-null object date 27512 non-null datetime64[ns] month 27512 non-null int64 day 27512 non-null int64 sub_type 27512 non-null object main_type 27512 non-null object 是否男士专用 27512 non-null object 销售额 27512 non-null float64 dtypes: datetime64[ns](1), float64(4), int64(2), object(6) memory usage: 2.9+ MB 123# 数据备份df = data 三、分析及可视化各品牌SKU数 SKU : Stock Keeping Unit。SKU是库存量单位，区分单品。 ​ 在电商领域，通常每一个商品详情页对应一个独立的SKU编码。 123# 图表风格plt.style.use('ggplot') 1234567891011plt.figure(figsize=(16,10))colors = ['r','b','g','yellow','pink','c','orange','violet','slateblue','gray']df['店名'].value_counts().sort_values(ascending = False).plot.bar(width = 0.8, alpha = 0.6, color = colors)plt.title('各品牌SKU数',fontsize = 18)plt.xticks(rotation = 60)plt.ylabel('商品数量',fontsize = 14)plt.grid(axis='x')plt.show() 图表解析：由图可知，仅悦诗风吟与佰草集的商品数量超过两千，且悦诗风吟商品数量达到了3000+，稳居榜首。 各品牌销量&amp;销售额123456789101112131415161718192021fig,axes = plt.subplots(1,2,figsize=(24,12))colors = ['r','b','g','yellow','pink','c','orange','violet','slateblue','gray']ax1 = data.groupby('店名').sale_count.sum().sort_values(ascending=True).plot(kind='barh', ax=axes[0], width=0.6, color = colors)ax1.set_title('品牌总销售量',fontsize=18)ax1.set_xlabel('总销售量')ax2 = data.groupby('店名')['销售额'].sum().sort_values(ascending=True).plot(kind='barh', ax=axes[1], width=0.6, color = colors)ax2.set_title('品牌总销售额',fontsize=18)ax2.set_xlabel('总销售额') plt.subplots_adjust(wspace=0.4)paraments = {'ytick.labelsize':16,'axes.labelsize':15}plt.rcParams.update(paraments)plt.show() 图表解析：相宜本草的销售量和销售额都是最高的。销量第二至第五，分别为美宝莲、悦诗风吟、妮维雅和欧莱雅；而销售额第二至第五，分别为欧莱雅、佰草集、美宝莲、悦诗风吟。其中，美宝莲、悦诗风吟和欧莱雅都在销量&amp;销售额前五内。 各类别销售量1234567891011121314151617181920212223242526# 各类别的销售量占比情况fig,axes = plt.subplots(1,2,figsize = (18,10))df1 = df.groupby('main_type')['sale_count'].sum()ax1 = df1.plot(kind = 'pie',ax = axes[0],autopct = '%.1f%%', pctdistance = 0.8,labels = df1.index, labeldistance = 1.05, startangle = 60, radius = 1.1,counterclock = False, wedgeprops = {'linewidth':1.2,'edgecolor':'k'}, textprops = {'fontsize':10,'color':'k'})ax1.set_title('主类别销售量占比',fontsize = 15)df2 = df.groupby('sub_type')['sale_count'].sum()ax2 = df2.plot(kind = 'pie',ax = axes[1],autopct = '%.1f%%', pctdistance = 0.8,labels = df2.index, labeldistance = 1.05, startangle = 60, radius = 1.1,counterclock = False, wedgeprops = {'linewidth':1.2,'edgecolor':'k'}, textprops = {'fontsize':12,'color':'k'})ax2.set_title('子类别销售量占比',fontsize = 15)plt.subplots_adjust(wspace=0.4)plt.show() 图表解析： 从主类别销售量占比情况来看，护肤品的销量远大于化妆品销量 从子类别销售量占比情况来看，底妆类、口红类在化妆品中销量最多，清洁类、化妆水、面霜在护肤品中销量最多。 各品牌销量 - 主类别12345678910# 销量plt.figure(figsize=(20,10))ss.barplot(x='店名',y='sale_count',hue='main_type',data=df,saturation=0.75,ci=0)plt.title('各品牌各总类的总销售量')plt.ylabel('销量')plt.text(0,78000,horizontalalignment = 'left',color = 'gray',fontsize = 15)plt.show() 各品牌销售额 - 主类别12345678# 销售额plt.figure(figsize=(20,10))ss.barplot(x = '店名',y = '销售额',hue = 'main_type',data = df, saturation=0.75,ci = 0)plt.title('各品牌各总类的总销售额')plt.ylabel('销售额')plt.show() 图表解析：各品牌的化妆品、护肤品销量以及销售情况均不一样，可能与品牌的定位有关。有的品牌主打化妆品，化妆品销量、销售额表现会相对好很多，例如蜜丝佛陀等；而主打护肤品的品牌，护肤品的销量、销售额会表现较好，如欧莱雅、佰草集等；也有部分品牌如美宝莲、兰蔻、悦诗风吟，化妆品和护肤品的销量、销售额都还不错。 各品牌销量 - 子类别12345678# 各品牌各子类销量plt.figure(figsize=(26,12))ss.barplot(x = '店名',y = 'sale_count',hue = 'sub_type',data = df , saturation=0.75,ci=0)plt.title('各品牌各子类别的总销量')plt.ylabel('销量')plt.show() 各品牌销售额 - 子类别123456plt.figure(figsize=(26,15))ss.barplot(x = '店名',y = '销售额',hue = 'sub_type',data = df,saturation=0.75,ci = 0)plt.title('各品牌各子类别的总销售额')plt.legend(loc = 'upper right',fontsize = 13)plt.ylabel('销售额')plt.show() 图表解析：由于子类别较多，图表大致上能呈现出与主类别数据分析结果一致，其中比较突出的是自然堂的化妆水的销售额在自然堂总销售额中占据极大份额。 品牌热度 根据数据源分析，本次品牌热度呈现主要根据各品牌商品的平均评论数多少来展现。 123456789101112# 各品牌热度plt.figure(figsize=(24,10))prop_iter = iter(plt.rcParams['axes.prop_cycle'])colors = ['r','b','g','yellow','pink','c','orange','violet','slateblue','gray']df.groupby('店名').comment_count.mean().sort_values(ascending = False).plot(kind = 'bar', width = 0.8, color=colors)plt.title('各品牌商品的平均评论数',fontsize = 18)plt.xticks(rotation = 60,fontsize = 18)plt.ylabel('评论数',fontsize =18)plt.show() 1234567891011121314151617181920# 销量热度plt.figure(figsize = (18,10))x = df.groupby('店名')['sale_count'].mean()y = df.groupby('店名')['comment_count'].mean()s = df.groupby('店名')['price'].mean()txt = df.groupby('店名').id.count().indexss.scatterplot(x,y,size = s,hue = s,sizes=(100,1500),data = df)for i in range(len(txt)): plt.annotate(txt[i],xy = (x[i],y[i]))plt.ylabel('热度',fontsize = 18)plt.xlabel('销量',fontsize = 18)plt.legend(loc = 'upper left',fontsize = 15)plt.show() 图表解析： 热度越高的品牌越靠上，销量越高的品牌越靠右，价格越高的品牌颜色越深圈越大 热度与销量呈一定的正相关 美宝莲热度第一，销量第二；妮维雅热度第二，销量第四；且二者价格均相对较低 图中显示，价格低的品牌热度和销量相对较高，价格高的品牌热度和销量相对较低；说明价格在热度和销量中有一定影响，也反映出人们较关注性价比问题。 各品牌价格分析1234567# 各品牌价格plt.figure(figsize = (18,10))ss.boxplot(x = '店名',y = 'price',data = df)plt.ylim(0,4000)plt.xticks(fontsize = 13)plt.show() 12345# 各品牌的平均价格# df.groupby('店名').price.sum()avg_price = df.groupby('店名').price.sum() / df.groupby('店名').price.count()avg_price 店名 SKII 1011.727079 佰草集 289.823171 倩碧 346.092190 兰芝 356.615809 兰蔻 756.400778 妮维雅 73.789053 娇兰 1361.043588 悦诗风吟 121.245945 植村秀 311.786667 欧珀莱 276.218543 欧莱雅 167.282698 玉兰油 329.657294 相宜本草 122.958446 美加净 44.694619 美宝莲 148.757576 自然堂 180.130213 薇姿 281.085791 蜜丝佛陀 142.118894 资生堂 577.438490 雅漾 212.618401 雅诗兰黛 872.470718 雪花秀 901.082873 Name: price, dtype: float64 1234567891011fig = plt.figure(figsize=(19,10))avg_price.sort_values(ascending=False).plot(kind = 'bar',width = 0.8,alpha = 0.6, color = 'lightgreen', label = '各品牌平均价格')y = df.price.mean()plt.axhline(y,0,5,color = 'yellow',label = '全品牌平均价格')plt.ylabel('各品牌平均价格',fontsize = 16)plt.title('各品牌产品的平均价格',fontsize = 24)plt.xticks(rotation = 60,fontsize = 16)plt.legend(loc = 'best')plt.show() 图表解析： 娇兰、SKII、雪花秀、雅诗兰黛、兰蔻、资生堂均是国际大牌，产品平均价格都在500以上，属于一线大牌； 兰芝、倩碧、玉兰油、植村秀、佰草集、薇姿、雅漾的平均价格在300~400之间，其中佰草集是最贵的国货品牌； 美加净作为国货品牌，平均价格最低，性价比高，而妮维雅平均价格第二低； 全品牌的平均价格低于400元，而除了前六个国际大牌外的其他品牌的平均价格都低于全品牌的平均价格 平均销量&amp;平均销售额12345678910111213141516171819# 各品牌平均销量和平均销售额plt.figure(figsize=(20,12))x = df.groupby('店名')['sale_count'].mean()y = df.groupby('店名')['销售额'].mean()s = avg_pricetxt = df.groupby('店名').id.count().indexss.scatterplot(x,y,size = s,sizes=(100,1500),marker='v',alpha=0.5,color = 'b', data=df)for i in range(len(txt)): plt.annotate(txt[i],xy = (x[i],y[i]),xytext = (x[i]+0.2,y[i]+0.2),fontsize = 14) plt.ylabel('销售额',fontsize = 17)plt.xlabel('销量',fontsize = 17)plt.legend(loc = 'upper left',fontsize = 18)plt.show() 图表解析： 销售额越高越靠图像上方，销量越高越靠图像右方，图形越大代表平均价格越高 销量与销售额呈正相关 相宜本草、美宝莲、蜜丝佛陀销量与销售额在所有品牌中位列前三，而且平均价格较低 男性护肤品销量分析12345678# 男性护肤品销量情况gender_data = df[df['是否男士专用'] == '是']gender_data_1 = gender_data[(gender_data.main_type == '护肤品') | (gender_data.main_type == '化妆品')]plt.figure(figsize=(19,9))ss.barplot(x = '店名',y = 'sale_count',hue = 'main_type',data=gender_data_1, saturation = 0.75,ci = 0)plt.show() 图表分析：男用大多购买的是护肤品 12345678910111213141516f,[ax1,ax2] = plt.subplots(1,2,figsize=(19,9))colors = ['r','b','g','yellow','pink','c','orange','violet','slateblue','gray']gender_data.groupby('店名').sale_count.sum().sort_values(ascending = True).plot(kind='barh', width = 0.8, ax = ax1, color=colors)ax1.set_title('男士护肤品销量排名',fontsize = 20)gender_data.groupby('店名')['销售额'].sum().sort_values(ascending = True).plot(kind='barh', width = 0.8, ax = ax2, color = colors)ax2.set_title('男士护肤品销量额排名',fontsize = 20)plt.subplots_adjust(wspace= 0.4)plt.show() 图表分析： 妮维雅是男士护肤品中购买首选，销量遥遥领先 欧莱雅、相宜本草、倩碧分别位于男士护肤品销量的二三四位 购买时间与销量分析1234567891011121314# 时间与销量分析 -- 购买高峰期from matplotlib.pyplot import MultipleLocatorplt.figure(figsize=(19,9))day_sale = df.groupby('day')['sale_count'].sum()day_sale.plot()plt.grid(linestyle = '-.',color = 'gray',axis = 'x',alpha = 0.5)x_major_locator = MultipleLocator(1)ax = plt.gca()ax.xaxis.set_major_locator(x_major_locator)plt.xlabel('日期(11月)',fontsize = 18)plt.ylabel('销量',fontsize = 18)plt.show() 图表解析： 化妆品的购买高峰期在11号的前几天，猜测原因为双十一前商家为提前预热给出了较大优惠，导致了消费者的购买欲望加强 双十一当天销量最低 双十一后3天，销量较于11当天稳步上涨，可能是消费余热导致，也有可能是商家给出了新的优惠或是持续打折导致 总结基于上方分析，做出以下总结整理： 美妆类别中护肤品销量远大于化妆品，而护肤品中又以清洁类、化妆水以及面霜等基础护肤类产品销量最高 男士使用的美妆产品集中在护肤品类别，在众多品牌中妮维雅是最受男士喜爱的品牌 价格和热度对销量有极大关联，平价的基础产品是大多数消费者的首选，也说明了消费者对性价比高的产品的热衷 可能是商家在双十一的提前预热，亦或是给出了巨大的优惠和为了避免11当天的消费高峰，不少消费者选择了提前消费，导致了销量高峰出现在双十一的前几天，而非11日当天 双十一后3天，消费者还留有购物余热，但销量远不如双十一之前 写在最后以上为双十一天猫美妆产品销售数据分析以及可视化。有错误及不足之处，恳请指正，感谢阅读。","link":"/2021/03/29/%E5%A4%A9%E7%8C%AB%E5%8F%8C%E5%8D%81%E4%B8%80%E7%BE%8E%E5%A6%86%E9%94%80%E5%94%AE%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"title":"如何通过对比得出数据差异？以及什么是二八定律","text":"摘要如何得出数据的差异？如何通过二八定律定位问题的决定性因素？ 写在前面上次学习了数据特征分析中的分布分析，今天继续学习数据特征分析中另外两种分析方法，也就是对比分析以及帕累托分析法。 对比分析原理任何事物都既有共性特征，又有个性特征。只有通过对比，才能分辨出事物的性质、变化、发展与别的事物的异同，从而深刻地认识事物的本质和规律。 概念 对比分析通常是把两个互相关系的指标数据进行比较，运用数字展示和说明研究对象规模的大小，水平的高低，速度的快慢，以及各种关系是否协调。 一般有以下几种比较方式： 绝对数比较(相减) / 相对数比较(相除) 结构分析、比例分析、空间比较分析、动态对比分析 实例123456# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 123456# 构建数据data = pd.DataFrame(np.random.rand(30,2)*1000, columns = ['A_sale','B_sale'], index = pd.period_range('20180801','20180830'))data.head() A_sale B_sale 2018-08-01 373.252264 530.096409 2018-08-02 170.119238 288.210134 2018-08-03 755.505567 51.239224 2018-08-04 767.666050 744.299464 2018-08-05 602.851166 676.580069 12345678910111213141516plt.rc('font',family = 'simhei',size = 15)data.plot(kind = 'line', style = '--', alpha = 0.7, figsize = (16,6), title = 'AB产品销量对比-折线图', grid = True)# plt.rc('font',family = 'simhei',size = 15)data.plot(kind = 'bar', width = 0.8, alpha = 0.6, figsize = (16,6), title = 'AB产品销量对比-柱状图', grid = True) 123456789101112131415161718192021# 绝对数比较 - 相减x = range(len(data))y1 = data['A_sale']y2 = -data['B_sale']fig3 = plt.figure(figsize = (16,9))plt.subplots_adjust(hspace=0.3)ax1 = fig3.add_subplot(2,1,1)plt.bar(x,y1,width = 1,facecolor = 'r',edgecolor = 'k')plt.bar(x,y2,width = 1,facecolor = 'g',edgecolor = 'k')plt.grid()ax2 = fig3.add_subplot(2,1,2)y3 = data['A_sale'] - data['B_sale']plt.plot(x,y3,'--go')plt.grid()plt.xlim(0,len(data))plt.axhline(0,color = 'r',linestyle = '-')ax2.set_xticklabels(data.index[::5]) 帕累托分析基本概念 帕累托分析法，是一种得到广泛应用的统计学分析方法。 帕累托分析法应用了“帕累托法则”──关于做20%的事可以产生整个工作80%的效果的法则──通俗地说，柏拉图分析的结果得出百分之八十的后果是由占百分之二十的主要原因造成的，因此帕累托分析法又称二八定律 原因和结果、投入和产出、努力和报酬之间本来就存在着无法解释的不平衡。 一般来说，投入和努力可以分成两种不同类型：多数，他们只能造成少许的影响；少数，他们造成主要的、重大的影响；例如一个公司：80%的利润来自于20%的畅销产品，而其他80%的产品只产生了20%的利润 思路：通过二八原则，去寻找关键的那20%决定性因素 实例123456789101112131415161718192021222324252627282930313233# 创建数据，10个品类产品的销售额data = pd.Series(np.random.randn(10)*1200+3000, index=list('abcdefghij'))print(data)print('---------------------------')# 对数据进行排序data.sort_values(ascending = False,inplace = True)# 营收柱状图plt.figure(figsize = (16,6))plt.rc('font',family = 'simhei',size = 15)data.plot(kind = 'bar',color = 'g',alpha = 0.6,width = 0.7)plt.ylabel('营收_元')p = data.cumsum()/data.sum()key = p[p&gt;0.8].index[0]key_num = data.index.tolist().index(key)print('超过80%累计占比的节点值索引为：',key)print('超过80%累计占比的节点值索引位置为：',key_num)print('-------------------------')p.plot(style = '--ko',color = 'y',secondary_y = True)plt.axvline(key_num,color = 'r',linestyle = '--',alpha = 0.8)plt.text(key_num+0.2,p[key],'累计占比为：%.3f%%' % (p[key]*100),color = 'r')plt.ylabel('营收_占比')key_product = data.loc[:key]print('核心产品为：')print(key_product) 123456789101112131415161718192021222324a 3640.475899b 4259.570101c 1132.691082d 2852.087642e 2411.036428f 3438.507881g 2130.585464h 3009.455301i 1048.431800j 1804.076351dtype: float64---------------------------超过80%累计占比的节点值索引为： g超过80%累计占比的节点值索引位置为： 6-------------------------核心产品为：b 4259.570101a 3640.475899f 3438.507881h 3009.455301d 2852.087642e 2411.036428g 2130.585464dtype: float64 总结帕累托分析法也叫主次因素分析法，原先是项目管理中常用的一种方法。它是根据事物在技术和经济方面的主要特征，进行分类排队，分清重点和一般，从而有区别地确定管理方式的一种分的方法。在数据分析中，帕累托分析法可以快速获取数据的各个部分的贡献度，从而制定分析方向与计划。","link":"/2020/09/29/%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90&%E5%B8%95%E7%B4%AF%E6%89%98%E5%88%86%E6%9E%90/"},{"title":"数据分析中的度量和维度","text":"摘要通过业务指标，了解数据分析中的度量和维度到底是什么？ 写在前面 数据分析和运营脱离不开关系。业务的洞悉决定了数据分析结果的上限，数据技巧只是逼近它。so，每个数据分析都应该洞察业务指标。本文将记录下学习指标和维度的基本认知，并了解在不同的业务场景下的一些指标。 指标通俗来讲，指标是量化衡量的标准，或者说是衡量事物发展程度的单位或方法，也称为度量。例如：人口数、GDP、收入、用户数、利润率、留存率、覆盖率等。很多公司都有自己的KPI指标体系，就是通过几个关键指标来衡量公司业务运营情况的好坏。 指标可以分为绝对数指标和相对数指标，绝对数指标反映的是规模大小的指标，如人口数、GDP、收入、用户数，而相对数指标主要用来反映质量好坏的指标，如利润率、留存率、覆盖率等。在分析一个事物发展程度就可以从数量跟质量两个角度入手分析，以全面衡量事物发展程度。 而一般指标用于衡量事物发展程度，那这个程度是好还是坏，需要通过不同维度来进行对比，才能知道是好还是坏。基本上，在数据分析中指标(度量)与维度都是配合使用来对需求进行分析。 市场营销指标 客户/用户生命周期 生命周期：企业/产品和消费者在整个业务关系阶段的周期 不同业务划分的阶段不同。 传统营销中，分为：潜在用户、兴趣用户、新客户、老/熟客户、流失客户 用户价值 用户价值有不同的评估方式 指数法 例如：用户贡献 = 产出量 / 投入量 * 100% 或者金融行业的：存款 + 贷款 + 信用卡 + 年费 + … - 风险 - 流失 RFM模型 RFM模型：指用户生命周期中，衡量客户价值的立方体模型。 根据RFM模型将用户划分成多个群体，需要注意的是：MF是在同时间区间内的数据 R ：最近一次消费时间 M ：同时间区间内的总消费金额 F ：同时间区间内的消费频次 用户分群 用户分析也就是所谓的营销矩阵，类似于多维度分析和象限法 它是市场营销中的一种常见策略，在提取用户的几个核心维度后，并用象限法将其归纳和分类 产品运营指标AARRR AARRR模型是进行用户分析的经典模型，是一个典型的漏斗结构。它从生命周期的角度，描述了用户进入平台需经历的五个环节，最终获取商业价值 Acquisition：用户获取 渠道到达量：俗称曝光量。有多少人看到了产品推广相关的线索 渠道转化率：有多少用户因为曝光而心动Cost Per，包含CPM、CPC、CPS、CPD、CPT等 渠道ROI：推广营销的熟悉KPI，投资回报率，利润 / 投资 * 100% 日应用下载量：app的下载量，这里指点击下载，不代表下载完成 日新增用户数：以用户注册提交资料为基准 日新增用户数：以用户注册提交资料为基准 获客成本：为获取一个用户需要支付的成本 一次会话用户数占比：指新用户下载完app，仅打开过产品一次，且该次使用市场在两分钟以内 Activation：用户活跃 日 / 周 / 月活跃用户应用下载量：活跃标准是用户用过产品 活跃用户占比：活跃用户数在总用户数的比例，衡量的是产品的健康程度 用户会话session次数：用户打开产品操作和使用，知道推出产品的整个周期。5分钟内没操作默认会话操作结束。 用户访问时长：一次会话的持续时间 用户平均访问会话次数：一段时间内的用户平均产生会话次数 Retention：用户留存 用户在某段时间内使用产品，过了一段时间后仍旧继续使用的用户 次日留存率 七日留存率 Revenue：营收 付费用户数：花了钱的 付费用户数占比：每日付费用户占活跃用书数比，液可以计算总付费用户占总用户数比 ARPU：某时间段内每位用户平均收入（支付金额） ARPPU：某时间段内每位付费用户平均收入，排除了未付费的 客单价：每一位用户平均购买商品的金额。销售总额 / 顾客总数 LTV：用户生命周期价值，和市场营销的客户价值接近，经常用在游戏运营和电商运营中 LTV = ARPU * 1 / 流失率 Refer：传播 K因子：每一个用户能够带来几个新用户。(K因子 = 用户数 * 平均邀请人数 * 邀请转化率) 用户分享率：某功能 / 页面中，分享用户数占浏览页面人数之比 活动 / 邀请曝光量：线上传播活动中，该页面被人浏览的次数。 RARRA RARRA模型是基于AARRR模型基础上重新定义的一种增长模型，因为传统的增长模型，在现在的市场环境下，其实很容易让企业偏离初心，不去追求产品带给用户的核心价值，而是去追求一些无意义的虚荣指标，进而迅速走向衰亡。而RARRA模型恰好符合产品的核心价值一定是体现在用户留存上的这一定律。 用户留存Retention：为用户提供价值，让用户回访 提高用户留存是产品增长的基础。故第一步，评估产品当中当前留存率情况和主要用户流失节点，进而提高用户留存 计算你的N天留存率，以查看有多少用户返回你的产品并准确确定用户主要流失节点，从而进行集中优化和改善 用户激活Activation：确保新用户在首次启动时看到你的产品价值 有效的用户引导可以帮助用户花更少的时间搞清楚如何使用你的产品 简单的演练和可视化提示，帮助用户尽快体验产品价值和优势，加快用户激活 需要注意的是：好的用户引导应该保持简单，而不是覆盖每一个功能，过于繁琐 用户推荐Referral：让用户分享、讨论你的产品 通过激励手段，让已经留存下来的忠诚用户将你的产品推荐给周边的用户，达到快速扩展你的用户群的目的并为潜在用户同样提供激励措施 此外，用户推荐的每次获客成本通常比其他渠道的获客成本要低得多，推荐用户的留存率通常也会更高。 例如病毒式营销 — 通过为当前用户和推荐用户提供现金返还或折扣券/优惠券等推荐奖励机制 商业变现Revenue：一个好的商业模式是可以赚钱的 客户的留存时间越长，他们对你的业务的价值就越大，可以带来提供稳定、可预测的收入增长。 可以通过提高用户的终身价值来提高留存时间 识别追加销售和交叉销售机会； 了解用户的需求可以帮助你确定路线图的优先顺序，从而专注于实际促进用户留存和业务增长的方面 用户拉新Acquisition 鼓励老用户带来新用户 通过群组分析找出哪些获客渠道的效果最适合你的产品，进行再优化 一旦你证明你可以留住用户，你就可以加大营销力度，在渠道顶部添加新用户 ，并开始以指数方式为你的产品进行黑客增长。 用户行为用户行为分析就是通过对这些数据进行统计、分析，从中发现用户使用产品的规律，并将这些规律与网站的营销策略、产品功能、运营策略相结合，发现营销、产品和运营中可能存在的问题，解决这些问题就能优化用户体验、实现更精细和精准的运营与营销，让产品获得更好的增长。 功能使用 功能使用率 / 渗透率：使用某功能的用户占总活跃数之比 比如点赞数、评论、收藏、关注、搜索、添加好友等 用户会话 会话session：用户在一次访问过程中，从开始到结束的整个过程。 在网页端，30分钟内没有操作默认会话操作结束 用户路径 路径图：用户在一次会话的过程中，其访问产品内部的浏览轨迹。通过此，可以加工出关键路径转换率 电子商务指标购物篮分析 购物篮分析中最知名的想必是关联度，简单理解是，买了某类商品的用户更有可能买哪些其他东西。 关联分析有两个核心指标，置信度和支持度。支持度表示某商品A和某商品B同时在购物篮中的比例，置信度表示买了商品A和人有多少同时买了B，表示为A→B。 笔单价：用户每次购买支付的金额，即每笔订单的支出 件单价：商品的平均价格 成交率：支付成功的用户在总的客流量中的占比 购物篮系数：平均每笔订单中，卖出了多少商品。购物篮系数是多多益善，它也和商品关联规则有关系 复购率：一段时间内多次消费的用户占总消费用户数之比 回购率：一段时间内消费过的用户，在下一段时间内仍旧消费的占比（留存或忠诚度） 流量指标 浏览量和访客量 PV：浏览次数 UV：一定时间内访问网页的人数，正式名称独立访客数，在同一天内，不管用户访问了多少网页，他都只算一个独立访客。（在技术上，UV会通过cookie或IP衡量） 访客行为 新老访客占比：衡量网站的生命力 访客时间：衡量内容只狼不是看内容的UV，而是看内容的访问时间 访问平均访问页数：衡量网站对访客的吸引力，是访问的深度 来源：访客从哪里来，技术上，通过来源网站的参数提取，可以区分SEM，SEO或者外链等 用户行为转换率：用户在网站上进行了相应操作的用户在总访客数上的占比 首页访客占比：只看了首页的用户，在总访客数上的占比 退出率：从该页退出的页面访问数 / 进入该页的访问数 （营销中使用） 跳出率：浏览单页即退出的次数 / 访问次数 （网页产品结构中使用） 跳出率一般衡量各个落地页，营销页等；退出率则更偏向产品，任何页面都有退出率 指标生成 指标一般需要经过加和、平均等汇总计算方式得到，并且是需要在一定的前提条件进行汇总计算，如时间、地点、范围，也就是我们常说的统计口径与范围。 访客访问时长 + UV = 重度访问用户占比 （浏览时间5分钟以上的用户在整个访客中占比） 用户会话次数 + 成交率 = 有效消费会话占比 （用户在所有的会话中，其中有多少次有消费） 机器学习 维度简言之，维度是事物或现象的某种特征。如性别、地区、时间等都是维度 维度可以分为定性维度跟定量维度，也就是根据数据类型来划分，数据类型为字符型(文本型)数据，就是定性维度，如地区、性别都是定性维度;数据类型为数值型数据的，就为定量维度，如收入、年龄、消费等 一般我们对定量维度需要做数值分组处理，也就是数值型数据离散化，这样做的目的是为了使规律更加明显，因为分组越细，规律就越不明显，最后细到成最原始的流水数据，那就无规律可循。 时间维度 时间是一种常用、特殊的维度，通过时间前后的对比，就可以知道事物的发展是好是坏 纵比例如用户数环比上月增长10%、同比去年同期增长20%，这就是时间上的对比，也称为纵比 横比例如不同国家人口数、GDP的比较，不同省份收入、用户数的比较、不同公司、不同部门之间的比较，这些都是同级单位之间的比较，简称横比 总结以上是关于不同业务场景下的指标(度量)的一些学习笔记，指标和维度是数据分析中的基础，但又很重要，需要在了解了基本认知的同时，也要掌握理解两者之间的关系，届时再展开数据分析工作就会容易很多。 本文版权归作者所有，欢迎转载，转载请注明出处和链接来源。","link":"/2020/08/29/%E6%8C%87%E6%A0%87%E4%B8%8E%E7%BB%B4%E5%BA%A6/"},{"title":"什么是趋势分析、特征工程、因子分析 ？","text":"摘要关于数据分析中的趋势分析、特征工程、因子分析这些专有名词的基本概念～ 前言以下为数据分析过程中常见的一些专用名词解析，记录下来以便随时翻阅，并进行查漏补缺～ 趋势分析集中趋势 集中趋势分析是指一组数据项某一中心值靠拢的程度，它反映了一组数据中心点的位置所在。 主要靠均值、中数、众数等统计指标来表示数据的集中趋势 均值(连续值)：也称平均数，它是全部数据的算术平均。均值在统计学中具有重要的地位，是集中趋势的最主要测度值。 中位数(异常值)：是一组数据排序后处于中间位置上的变量值 众数：是一组数据中出现次数最多的变量值。众数主要用于测度分类数据的集中趋势，当然也适用于作为顺序数据以及数值型数据集中趋势的测度值。一般情况下，只有在数据量较大的情况下，众数才有意义。 离中趋势 离中趋势是指一组数据中各数据值以不同程度的距离偏离其中心（平均数）的趋势，又称标志变动度。 主要靠极差、四分差、平均差、方差、标准差等统计指标来研究数据的离中趋势。 极差(全距)：极差=最大变量值-最小变量值 分位差：是从一组数据中剔除了一部分极端值之后重新计算的类似于极差的指标。常用的有四分位差等 四分位差=（第三个四分位数-第一个四分位数）/ 2 平均差：是数据组中各数据值与其算术平均数离差绝对值的算术平均数。平均差异大，表明各标志值与算术平均数的差异程度越大，该算术平均数的代表性就越小；平均差越小，表明各标志值与算术平均数的差异程度越小，该算术平均数的代表性就越大。 方差：数据组中各数据值与其算术平均数离差平方的算术平均数。 标准差：方差的平方根就是标准差。 正态分布的离中趋势：数据落在左右一倍标准差内的概率为69%，落在正负1.96倍的概率为95%，落在正负2.58倍的概率为99% 数据分布 偏态系数：数据平均值偏离状态的以一种衡量，值为正为正偏，为负为负偏 峰态系数：数据分布集中强度的衡量，值越大，顶越尖（正太分布的峰态系数一般是3） 正态分布 正态分布又名高斯分布，若随机变量X服从一个数学期望为μ、方差为σ^2 的高斯分布，记为N(μ，σ^2)。其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。 我们通常所说的标准正态分布是μ = 0,σ = 1的正态分布 正态分布的密度函数的特点是：关于μ对称，并在μ处取最大值，在正（负）无穷远处取值为0，在μ±σ处有拐点，形状呈现中间高两边低，图像是一条位于x轴上方的钟形曲线。 卡方分布若n个相互独立的随机变量，均服从标准正态分布N（也称独立同分布于标准正态分布），则这n个服从标准正态分布的随机变量的平方和构成一新的随机变量，其分布规律称为分布。 自由度：通俗讲，样本中独立或能自由变化的自变量的个数，称为自由度 卡方分布特点：卡方值都是正值，呈正偏态（右偏态），随着参数 n 的增大；卡方分布趋近于正态分布；随着自由度n的增大，卡方分布向正无穷方向延伸（因为均值n越来越大），分布曲线也越来越低阔（因为方差2n越来越大）。 F分布设X、Y为两个独立的随机变量，X服从自由度为n的卡方分布，Y服从自由度为m的卡方分布，这两个独立的卡方分布除以各自的自由度以后的比率服从F分布，即两个服从卡方分布的随机变量的比构成 F分布的特点：是一种非对称分布；它有两个自由度，即n-1（分子自由度）和m-1（分母自由度），且不同的自由度决定了F分布的形状。 T分布假设X服从标准正态分布N（0,1），Y服从卡方 （n）分布，那么Z=X/sqrt(Y/n)的分布称为自由度为n的t分布，即正太分布的一个随机变量除于一个服从卡方分布的变量就是T分布 T分布的特点：以0为中心，左右对称的单峰分布；t分布是一簇曲线，其形态变化与n（确切地说与自由度ν）大小有关。自由度ν越小，t分布曲线越低平；自由度ν越大，t分布曲线越接近标准正态分布（u分布）曲线。 数据分类 定类（类别）：根据事物离散、无差别属性进行的分类（例如：性别、名族） 定序（顺序）：可以界定数据的大小，但不能测定差值（例如：收入的低中高） 定距（间隔）：可以界定数据大小的同时，可测定差值，但无绝对零点（乘除无意义，例如：摄氏温度） 定比（比率）：可以界定数据大小，可测定差值，有绝对零点 单因子分析 异常值分析：连续异常值、离散异常值、知识异常值 对比分析：绝对数比较、相对数比较（结构、比例、比较、动态、强度）； ​ 时间维度、空间维度、经验于计划 结构分析：静态结构、动态结构 分布分析：直接获得概率分布、是否是正态分布、极大似然 多因子分析 假设检验 建设原假设H0（包括等号），H0的反命题为H1，也叫备择假设 选择检验统计量 根据显著水平（一般为0.05），确定拒绝域 计算p值或者样本统计值，做出判断（一般取双边检验p值） 正态检验 – scipy.stats.normaltest（偏度和峰度检验方法） 卡方检验（常用于两个因素之间有没有比较强的联系） – scipy.stats.chi2_contingency T分布检验（常用于检验两组样本分布是否一致，例如临床医疗检验药物效果）– scipy.stats.ttest_ind F检验（常用在方差分析） – scipy.stats.f_oneway 相关系数：正相关、负相关、不相关（相关系数越大，越接近1，二者变化趋势越正向同步；相关系数越小，越接近-1，反向同步；相关系数趋近于0可以认为二者是没有关系的） pearson spearman（只和名次差有关，跟具体的数值关系不大） 线性回归：（最小二乘法）因变量和自变量的关系是线性的 决定系数越接近1，回归效果越好；越接近0，回归效果越差 残差不相关（DW检验）DW值范围0~4，值为2为残差不相关，接近于4代表残差正相关，接近于0代表残差负相关；好的回归残差应该是不相关的 主成分分析（PCA） – 降维 求 特征协方差矩阵 求协方差的特征值和特征向量 将特征值按照从大到小的顺序排序，选择其中最大的K个 将样本点投影到选区的特征向量上 奇异值分解（SVD） LDA降维 – 线性判别式分析 核心思想：投影变换后同一标注内距离尽可能小；不同标注间距离尽可能大 特征工程 简而言之，特征工程就是一个把原始数据转变成特征的过程，这些特征可以很好的描述这些数据，并且利用它们建立的模型在未知数据上的表现性能可以达到最优（或者接近最佳性能）。从数学的角度来看，特征工程就是人工地去设计输入变量X。 目的 特征工程的目的就是获取更好的训练数据 特征越好，灵活性越强 特征越好，构建的模型越简单 特征越好，模型的性能越出色 步骤 特征使用：数据选择 – 可用性 特征获取：特征来源 – 特征的规整与存储 特征处理：数据清洗 – 特征预处理 特征监控：现有特征 – 新特征 关于数据清洗 数据清洗 数据样本抽样 样本要具备代表性 样本比例要平衡一级样本不平衡是如何处理 尽量考虑使用全量数据 异常值（空值）处理 识别异常值和重复值 – isnull() / duplicated() 直接丢弃（包括重复数据） – drop() / dropna() / drop_duplicated() 当是否有异常当作一个新的属性，替代原值 – fillna() 集中值指代 – fillna() 边界值指代 – fillna() 插值 – interpolate() – Series 特征预处理 特征预处理 标注（标记、标签、label） 特征选择 – 剔除与标注不相关或者冗余的特征 过滤思想 数据类型 可用方法 连续 — 连续 相关系数、假设检验 连续 — 离散（二值） 相关系数、连续二值化（最小Gini切分，最大熵增益切分） 连续 — 离散（非二值） 相关系数（定序） 离散（二值）— 离散（二值） 相关系数、熵相关、F分值 离散 — 离散（非二值） 熵相关，Gini，相关系数（定序） 包裹思想 – 遍历特征子集 RFE算法：1、列出特征集合；2、构造简单模型，根据系数去掉弱特征；3、余下特征重复过程，直到评价指标下降较大或者低于阈值，停止 嵌入思想 – 根据一个简单模型来分析特征的重要性（正则化/正规化） 特征变换 对指化 – 先进行对指化，再进行归一化 – 函数Softmax 指数化 – 将数值进行指数化 对数化 – 数据缩放到较小的尺度内（例如：收入、声音分贝、地震震级） 离散化 – 将连续变量分成几段（tips：先排序） 等距 等宽 归一化 (x - xmin) / (xmax - xmin) 标准化 数值化 定类数据 – 标签化 （LabelEncode） 定序数据 – 独热 （One-HotEncode ） 正规化 （1、用在每个对象的各个特征的表示，如特征矩阵的行；2、模型的参数，如回归模型） L1 正规化 ：xi / x的绝对值的和 L2 正规化（欧式距离） ：xi / x的平方和的开方 特征降维 PCA 奇异值 LDA 特征衍生 加减乘除 求导与高阶求导 人工归纳 写在末尾本文仅为数据分析学习过程中遇到的一些名词的解析，记录下来，供随时翻阅，达到查漏补缺的作用，未完待续～","link":"/2020/08/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%AD%E9%83%A8%E5%88%86%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/"},{"title":"到底什么是数据指标？","text":"摘要深入了解什么是数据指标？以及数据指标的作用和分类～ 什么是好的数据指标？ 好的数据指标能带来你所期望的变化。 以下是一些衡量数据指标好坏的重要准则： 好的数据指标是有比较性的 例如比较某数据指标在不同时间段、用户群体、竞争产品之间的表现，能更好的洞察产品的实际走向 好的数据指标是简单易懂的，换言之就是人们比较容易记住以及讨论的某数据指标 好的数据指标是一个比率。 比率的可操作性强，是行动的导向。例如开车，里程只是距离信息，而速度则告诉你当前行驶状态，以及是否需要进行调整以确保按时抵达 比率是天生的比较性指标。 比率还适用于比较各种因素间的相生和相克，也就是正相关和负相关。 好的数据指标会改变行为。即随着指标的变化，是否会采取相应的措施。 ‘试验’指标，如一个测试结果，其作用在于帮助你优化产品、定价以及市场定位。这些数据的变化会极大地影响接下来的行为决策 数据指标的作用 一个好的数据指标之所以能改变商业行为，是因为它和你的目标是一致的：保留用户、有效获取新用户、或者创造营收等 一个错误数据指标则会极大影响公司的营运。例如某公司将销售员的季度奖金与其正在接洽的订单数据挂钩，而不是与已签订订单数量或订单利润率挂钩，这样会导致业务员为了个人收入制造大量低质量的潜在客户，并将其停在”接洽”状态，而不是促成订单签订。 数据指标之间的耦合现象。譬如转换率(访客中真正发生购买行为的比例)通常就是和购买所需时间(客户需要花多长时间才能完成购买)相绑定的。类似的，病毒式传播系数(平均每个用户邀请来的新用户数)和病毒传播周期(用户完成一次邀请需要的时间)共同推动产品的普及率。 数据指标的分类定性指标与量化指标 定性指标通常是非结构化的、经验性的、揭示性的、难以归类的；量化指标则涉及很多数值和统计数据，提供可靠的量化结果，但一般缺乏直观的洞察。 定量指标 概念：定量数据是指我们跟踪和衡量的数字 特点：量化数据使用方便，具有科学性，也(假设算得正确)易于归类、外推和置入表格 定性指标 概念：定向指标是杂乱的、主观的、不精确的；类似从采访或者辩论中获得的信息，极难量化 特点：定性数据吸纳主观因素，且极难量化 虚荣指标与可付诸行动指标虚荣指标 概念：如果你有一个数据，却不知如何根据它采取行动，该数据就仅仅是一个虚荣指标 例如：”总注册用户数”，这个数据会随着时间增长，并不能传达关于用户的行为信息 举例一些需要提防的虚荣数据指标 点击量。相比之下，或许更应该统计点击人数 页面浏览量(PV值)。除非商业模式直接与PV值挂钩(类似展示广告)，或许更应该统计访问人数 访问量。问题在于100的访问量是一个用户访问了100次还是100个访问了一次的用户，毫无意义 独立访客数。只能告诉你有多少人访问了网页，不能反馈出用户在这个页面做了什么，为何停留，是否离开 粉丝/好友/赞的数量。统计当你呼吁时，有多少人会响应，这个数据更有意义 网站停留时间/浏览页数。用这两个指标来替代客户参与度或活跃度并不一定有说服力。比如，客户在客服或者投诉页面上停留了很长时间 收集的用户邮件地址数量。同粉丝/好友那个类似，更好的做法是：向一部分注册用户发送测试邮件，看看是否会按照邮件中的提示去做 下载量。尽管会影响应用商店中的排名，但需要重要衡量的是：应用下载后的激活量、帐号创建量等 可付诸行动指标 概念：可付诸行动指标可以帮你遴选出一个行动方案，从而指导你的商业行为。且当产品作出调整，这个指标也会相应地变化 例如：活跃用户占总用户数的百分比。这个指标揭示了产品的用户参与度。如果产品的调整思路是正确的，这个占比就应该相应的上升。也意味着这个可付诸行动指标可以指导试验、学习和迭代。 重点：可付诸行动的指标，并不会直接告诉你该做什么；而是可以指导后续的商业行为。其重点在于：根据收集到的数据行动 探索性指标与报告性指标唐纳德理论 世界上的事物可以分为这样几类：我们知道我们知道的，我们知道我们不知道的，我们不知道我们知道的，以及我们不知道我们不知道的 我们知道我们知道的：是可能并不为真的事实，须经由数据的检验 检验我们手头上的事实和假设(如打开率和转化率)，以确保我们的商业计划是可行的 我们知道我们不知道的：是可以通过调研解答的问题，可使之成为我们的行为基准并流程化 验证我们的直觉，把假设变成证据 我们不知道我们知道的：是直觉，需要我们评估并训练以提高其效能及效率 为业务预测表、瀑布式开发流程图和会议提供数据支持 我们不知道我们不知道的：是探索，蕴含着我们自身独特的优势，能带来有趣的顿悟 帮助我们发现黄金机遇 探索性指标 概念：探索性指标是推测性的，提供原本不为所知的洞见，旨于在商业竞争中取得先手优势 大致上可以归为上述的我们不知道我们不知道的，极其重要，可以在未来转变成秘密武器 报告性指标 概念：报告性指标是让你时刻对公司的日常运营、管理性活动保持信息通畅、步调一致。 属于某种度量行为。由于我们不知道这一类指标的具体数值，类似清点用户量，统计销售总额等，所以需要度量它。主要应用于核算或用于衡量实验的结果。 先见性指标与后见性指标 无论先见性数据指标或后见性数据指标都是有意义的，只不过解决的问题不同 先见性指标 概念：先见性数据指标用于预言未来 例如：透过”销售漏斗”中现有的潜在客户数，可以大致预测将来所能获得的新客户数；所以努力增加潜在客户，将来就可能得到更多的新增客户 如若要启用先见性数据指标，需要首先进行同期群分析并比较客户对照在不同时间段的表现 用户投诉量例子：可以通过跟踪每日接到的客服电话数；另一方面，可能以90天为期来跟踪用户投诉量。这两者都可以作为用户流失的先见性指示剂：即如果投诉量增加，很多客户可能就会停用你的产品或服务。而作为一个先见性数据指标，客户投诉可以帮你深入了解和服务的真实状况，分析投诉量上升原因，可以变相解决产品或服务中出现的问题。 后见性指标 概念：后见性数据指标用于解释过去，即能提示问题的存在 例如：用户流失(即某时间段内离开某产品或服务的客户量)，这一数据指标能指出产品或服务存在某些问题。 tips：在一个公司中，某一团队的后见性数据指标有时会是另一个团队的先见性指标。 相关性指标与因果指标相关性指标 概念：如果两个指标总是一同变化，那么说明它们是相关的 因果指标 概念：如果一个指标可以导致另一个指标的变化，则它们之间具有因果关系 通常，因果关系并不是简单的一对一关系。一些独立的数据指标(分析多个独立的数据指标作为自变量)，其中每一个都能在一定程度上”解释”某个依存的数据指标(也就是因变量)。 如何证明一个因果关系： 找到一个相关性，进行控制变量试验并测量因变量的变化 重点在于满足试验的良好控制条件，而一个足够大的数据样本，最终其他自变量对因变量的影响都会被样本数量拉平，所以在很小的样本容量下进行试验，需要尽可能简化测试 tips：相关性很好，因果性更佳。有时，只能找到一些相关性，但应永不停止的寻找因果性。 总结以上是在阅读《精益数据分析》一书时，所做的读书笔记，记录下来以便时常翻阅。感谢阅读～ 参考文章: [书籍 - 精益数据分析]","link":"/2021/05/10/%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87/"},{"title":"快速了解数据分布的常用方法","text":"摘要介绍几种数据分析中对于数据分布的常用方式，包括极差、频率以及分组的使用实例～ 前言分布分析是数据特征分析中极为常用的一种方法。在数据质量得到保证的前提下，通过绘制图表、计算某些统计量等手段对数据的分布特征和贡献度进行分析，分布分析能够揭示数据的分布特征和分布类型，对于定量数据，可以做出频率分布表、绘制频率分布直方图显示分布特征；对于定性数据，可用饼图和条形图显示分布情况。 分布分析 分布分析 → 研究数据的分布特征和分布类型，分定量数据、定性数据区分基本统计量 一般通过以下三种方式来进行分布分析： 极差 / 频率分布情况 / 分组组距及组数 下面通过一个案例对分布分析进行实操应用： 123456# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 1234567891011121314# 读取数据data = pd.read_csv('E:/深圳罗湖二手房信息.csv',engine = 'python')# 绘制散点图plt.figure(figsize = (12,8))plt.scatter(data['经度'],data['纬度'], s = data['房屋单价']/400, c = data['参考总价'], cmap = 'Reds', alpha = 0.6)plt.grid()data.head() 房屋编码 小区 朝向 房屋单价 参考首付 参考总价 经度 纬度 0 605093949 大望新平村 南北 5434 15.0 50.0 114.180964 22.603698 1 605768856 通宝楼 南北 3472 7.5 25.0 114.179298 22.566910 2 606815561 罗湖区罗芳村 南北 5842 15.6 52.0 114.158869 22.547223 3 605147285 兴华苑 南北 3829 10.8 36.0 114.158040 22.554343 4 606030866 京基东方都会 西南 47222 51.0 170.0 114.149243 22.554370 图表解读：点越大代表房屋的单价越高，颜色越深代表总价越高 极差 本案例中，可通过极差中看到销售的稳定程度 1234567891011121314# 极差def d_range(df,*cols): lst = [] for col in cols: crange = df[col].max() - df[col].min() lst.append(crange) return(lst)key1 = '参考总价'key2 = '参考首付'dr = d_range(data,key1,key2)print('%s极差为：%.2f \\n%s极差为：%.2f' % (key1,dr[0],key2,dr[1])) 12参考总价极差为：175.00 参考首付极差为：52.50 分组1234567891011# 频率分布情况data[key1].hist(bins = 8,edgecolor = 'k')# 分组区间gcut = pd.cut(data[key1],10,right=False)gcut_count = gcut.value_counts(sort = False)data['%s分组区间' % key1] = gcut.valuesdata.head() 图表解读：可以看出主要集中在160万以上，60万以下。 频率分布的划分方式：直方图可以快速的看到它的排列情况，把它拆分：分组划分 房屋编码 小区 朝向 房屋单价 参考首付 参考总价 经度 纬度 参考总价分组区间 0 605093949 大望新平村 南北 5434 15.0 50.0 114.180964 22.603698 [42.5, 60.0) 1 605768856 通宝楼 南北 3472 7.5 25.0 114.179298 22.566910 [25.0, 42.5) 2 606815561 罗湖区罗芳村 南北 5842 15.6 52.0 114.158869 22.547223 [42.5, 60.0) 3 605147285 兴华苑 南北 3829 10.8 36.0 114.158040 22.554343 [25.0, 42.5) 4 606030866 京基东方都会 西南 47222 51.0 170.0 114.149243 22.554370 [165.0, 182.5) 频率统计1234567891011121314151617181920212223242526# 区间出现频率r_zj = pd.DataFrame(gcut_count)r_zj.rename(columns = {gcut_count.name:'参考总价频数'},inplace = True)# 计算频率r_zj['频率'] = r_zj['参考总价频数'] / r_zj['参考总价频数'].sum()# 计算累计频率r_zj['累计频率'] = r_zj['频率'].cumsum()# 以百分比显示频率r_zj['百分比'] = r_zj['频率'].apply(lambda x:'%.2f%%' % (x*100))# 以百分比显示累计频率r_zj['累计频率百分比'] = r_zj['累计频率'].apply(lambda x:'%.2f%%' % (x*100))r_zj.style.bar(subset = ['频率','累计频率']) # 表格显示条形图 绘制直方图12345678910111213141516# 直方图r_zj['频率'].plot(kind = 'bar', figsize = (16,6), grid = True, color = 'k', alpha = 0.6)# 添加标签x = len(r_zj)y = r_zj['频率']m = r_zj['参考总价频数']for i,j,k in zip(range(x),y,m): plt.text(i-0.1,j+0.01,'%i' % k, color = 'k') 字段定性123456789101112# 频率分布：定性字段cx_g = data['朝向'].value_counts(sort = True)r_cx = pd.DataFrame(cx_g)r_cx.rename(columns = {cx_g.name:'朝向频数'},inplace = True)r_cx['频率'] = r_cx['朝向频数'] / r_cx['朝向频数'].sum()r_cx['累计频率'] = r_cx['频率'].cumsum()r_cx['百分比'] = r_cx['频率'].apply(lambda x:'%.2f%%' % (x*100))r_cx['累计频率百分比'] = r_cx['累计频率'].apply(lambda x:'%.2f%%' % (x*100))r_cx.style.bar(subset = ['频率','累计频率'],color = '#d35f5f',width = 100) 可视化1234567891011121314151617181920# 可视化：绘制频率直方图、饼图plt.figure(num = 1,figsize = (16,6))r_cx['频率'].plot(kind = 'bar', width = 0.8, rot = 0, color = 'k', grid = True, alpha = 0.6)plt.title('朝向分布频率直方图')plt.figure(num = 2)plt.pie(r_cx['朝向频数'], labels = r_cx.index, autopct= '%.2f%%', shadow = True)plt.axis('equal')plt.rc('font',family = 'simhei',size = 15) # 标签显示 总结以上是数据特征分析中最基础也是最常用的分布分析方法，用于研究数据的分布特征和分布类型，有错误之处，请指正，感谢阅读～","link":"/2020/09/22/%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81%E4%B9%8B%E5%88%86%E5%B8%83%E5%88%86%E6%9E%90/"},{"title":"什么是正态分布？如何进行正态性检验？","text":"摘要什么是正态分布，以及检验数据样本的正态性的方式有哪些？ 前言本文记录学习正态分布以及数据特征的正态性检验。 正态分布正态分布，又名高斯分布，是一个非常常见的连续概率分布。 定义正态分布是具有两个参数μ和σ2的连续型随机变量的分布。即若一个随机变量X服从一个数学期望为μ、方差为σ^2^的正态分布，记为N( μ , σ^2^ )。 参数μ是服从正态分布的随机变量的均值，参数σ^2^是此随机变量的方差。 其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度，当μ = 0，σ = 1时的正态分布称之为标准正态分布。 特点 集中性：正态曲线的高峰位于正中央，即均数所在的位置 对称性：正态曲线以均数为中心，左右对称，曲线两端永不于横轴相交 均匀变动性：正态曲线由均数所在处开始，分别向左右两侧逐渐均匀下降 服从正态分布的随机变量的概率规律为取 μ邻近的值的概率大 ，而取离μ越远的值的概率越小； μ决定分布的中心位置 σ越大，曲线越矮胖，总体分布越分散，反之曲线越瘦高，总体分布越集中 正态性检验定义 利用观测数据判断总体是否服从正态分布的检验称为正态性检验，是统计判决中重要的一种特殊的拟合优度假设检验 最为基础和常用的正态性检验方法有： 直方图初判 QQ图判断 K-S检验 直方图检验123456# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 1234567891011121314# 直方图初判s = pd.DataFrame(np.random.randn(1000)+10,columns=['value'])print(s.head())fig = plt.figure(figsize = (16,9))ax1 = fig.add_subplot(2,1,1)ax1.scatter(s.index,s.values)plt.grid()ax2 = fig.add_subplot(2,1,2)s.hist(bins = 20,alpha = 0.7,ax = ax2,edgecolor = 'k')s.plot(kind = 'kde',secondary_y = True,ax = ax2)plt.grid() 123456 value0 10.0319981 8.9389712 9.5218493 10.7521104 10.601193 QQ图判断123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# QQ图判断 - 通过把测试样本数据的分位数与已知分布相比较，从而来检验数据的分布情况# QQ图是一种散点图，对应于正态分布的QQ图，就是由标准正态分布的分位数为横坐标，样本值为纵坐标的散点图# 参考直线：四分之一分位点和四分之三分位点这两点确定，看散点是否落在这条线的附近# 绘制思路# 1、在做好数据清洗后，对数据进行排序# 2、排序后，计算出每个数据对应的百分位p{i}，即第i个数据x(i)为p(i)分位数，其中p(i)=(i-0.5)/n# 3、绘制直方图 + QQ图 ， 直方图做参考# 绘制散点图，横坐标是它的分位，就是分布的位置，做下排序，看是否很多的点在某条直线上，这条直线一般是拿它的一分位和三分位做一下相减，s = pd.DataFrame(np.random.randn(1000)+10,columns = ['value'])print(s.head())mean = s['value'].mean()std = s['value'].std()print('均值为：%.2f,标准差为：%.2f' % (mean,std))print('------------')# 计算均值,标准差s.sort_values(by = 'value',inplace = True)# 重新排序s_r = s.reset_index(drop = False)# 计算百分位数p(i)# 计算q值s_r['p'] = (s_r.index - 0.5) / len(s_r)# 每个值标准化后的结果s_r['q'] = (s_r['value'] - mean) / stdst = s['value'].describe()# 1/4位点x1,y1 = 0.25,st['25%']# 3/4位点x2,y2 = 0.75,st['75%']# 绘制数据分布图 -- 散点图fig = plt.figure(figsize = (16,12))ax1 = fig.add_subplot(3,1,1)ax1.scatter(s.index,s.values)plt.grid()# 绘制直方图ax2 = fig.add_subplot(3,1,2)s.hist(bins = 30,alpha = 0.7,ax = ax2)s.plot(kind = 'kde',secondary_y = True,ax = ax2)plt.grid()# 绘制QQ图，直线为四分之一位数、四分之三位数的连线，基本符合正态分布ax3 = fig.add_subplot(3,1,3)ax3.plot(s_r['p'],s_r['value'],'k.',alpha = 0.1)ax3.plot([x1,x2],[y1,y2],'-r')plt.grid() 12345678 value0 10.3152181 11.1689542 11.4689533 10.0997624 8.701746均值为：10.00,标准差为：1.01------------ K-S检验Kolmogorov-Smirnov是比较一个频率分布f(x)与一个理论分布g(x)或者两个观测值分布的检验方法。 以样本数据的累计频数分布与特定的理论分布比较（比如正态分布），如果两者差距小，则推论样本分布取自某特定分布 假设检验问题： H0：样本的总体分布 服从某特定分布 H1：样本的总体分布 不服从某特定分布 Fn(x)：样本的累计分布函数 F0(x)：理论分布的分布函数 D：F0(x) 与 Fn(x) 差值的绝对值最大值 即 D= max IFn(x) - F0(x)I D &gt; D( n , α )相比较：p &gt; 0.05则接受H0，p &lt; 0.05则接受H1. 1234567891011121314151617181920212223242526# K - S 检验 → Kolmogorov-Smirnov是比较频率分布f(x)与理论分布g(x)或者两个观测值分布的检验方法data = [87,77,92,68,80,78,85,77,81,80,80,77,92,86, 76,80,81,75,77,72,81,72,83,86,80,68,77,87, 76,77,78,92,75,80,78]df = pd.DataFrame(data,columns = ['value'])jz = df['value'].mean()bzc = df['value'].std()print('样本均值为：%.2f,样本标准差为：%.2f' % (jz,bzc))print('----------------')s = df['value'].value_counts().sort_index()df_s = pd.DataFrame({'血糖浓度':s.index, '次数':s.values})df_s['累计次数'] = df_s['次数'].cumsum()df_s['累计频率'] = df_s['累计次数'] / len(data)df_s['标准化取值'] = (df_s['血糖浓度'] - jz) / bzcdf_s['理论分布'] = [0.0244,0.0968,0.2148,0.2676,0.3228,0.3859,0.5160,0.5793,0.7054,0.8106, 0.8531,0.8888,0.9803]df_s['D'] = np.abs(df_s['累计频率'] - df_s['理论分布'])dmax = df_s['D'].max()print('实际观测D值为：%.4f' % dmax)df_s 1234样本均值为：79.74,样本标准差为：5.94----------------实际观测D值为：0.1636 把一个非标准正态分布变成一个标准正态分布—–&gt;把非标准正态分布的值变成X = (x-u) /方差—–&gt;可以找到理论值。在将这个标准化取值去跟正态分布表去找对应的值。 因为标准化取值的值它本 身就符合正态分布；系统分布与标准分布相减，如果这个函数满足标准正态分布，它的值就应该满足这个表。比如说标准化取值2.064315，其对应的查正态分布表值为0.9803，它的理论分布值是0.9803； 标准化取值-1.9777，去掉负号，查正态分布表为0.9756，正的是0.9756，负的就是1-0.9756=0.0244.可以看到与理论分布值是相对应的。 理论分布就相当于是g（x）就是F0（x），F（n）就是原来的F（n）累计频率。累计频率 - 理论分布 = D 123456#绘制折线图df_s['累计频率'].plot(style = '--k.')df_s['理论分布'].plot(style = '--r.')plt.legend(loc = 'upper left')plt.grid() 结论：实际观测D值为：0.1597 对应的0.1597放到显著性对照表，我们的样本数据一共35个，在50以内，按0.05的值去算的话，0.1587介于0.158和0.190之间，它所对应的P值是0.2和0.4，这个P值是大于0.05的。 拿到这个D值去那个表里边查，如果大于0.05就说明满足正态分布。 K-S算法1234567891011121314151617#直接用算法做KS检验from scipy import statsdata = [87,77,92,68,80,78,85,77,81,80,80,77,92,86, 76,80,81,75,77,72,81,72,83,86,80,68,77,87, 76,77,78,92,75,80,78]df = pd.DataFrame(data,columns = ['value'])jz = df['value'].mean()bzc = df['value'].std()stats.kstest(df['value'],'norm',(jz,bzc))# .kstest 方法：ks检验，参数分别是：待检验数据，检验方法，均值与标准差# 结果返回两个值：statistic → D值；pvalue → P值# P值大于0.05，满足正态分布 1KstestResult(statistic=0.1590868892818147, pvalue=0.3061435516448461) 总结以上是数据特征分析中的正态分布与正态性检验的全部内容，如有错误，恳请指正，感谢阅读～","link":"/2020/10/19/%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81%E4%B9%8B%E6%AD%A3%E6%80%81%E6%80%A7%E6%A3%80%E9%AA%8C/"},{"title":"泰坦尼克号幸存者分析","text":"摘要通过泰坦尼可号幸存者数据，分析幸存者的特征，从而… 项目描述项目名称：泰坦尼克号生存分析 数据来源：Kaggle数据集 → 共有1309名乘客数据，其中891是已知存活情况（train.csv），剩下418则是需要进行分析预测的（test.csv） 字段意义： PassengerId : 乘客编号 Survived：是否存活（1：存活；0：死亡） Pclass：客舱等级 Name：乘客姓名 Sex：乘客性别 Age：乘客年龄 SibSp：同乘的兄弟姐妹/配偶人数 Parch：同乘的父母/小孩人数 Ticket：船票编号 Fare：船票价格 Cabin：客舱号 Embarked：登船港口 项目目的：通过已知的获救数据，预测乘客的生存情况 环境解释：基于jupyter notebook，可视化工具包：seaborn、matplotlib 1234567891011121314# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlineimport matplotlibmatplotlib.style.use('ggplot')import seaborn as snssns.set(context='notebook',style='white',palette='muted')import warningswarnings.filterwarnings('ignore')import osos.chdir(r'/Users/nanb/Documents/数据存放') 1234# 数据导入train = pd.read_csv('train.csv')test = pd.read_csv('test.csv') 123# 数据查看train.head() 存活比例分析 整体来看，存活比例是怎样 12345678# 饼图 -- 数据类别少使用饼图更为直观sns.set()sns.set_style('ticks')plt.axis('equal')train['Survived'].value_counts().plot.pie(radius=1.5, explode=[0.1,0], autopct = '%1.2f%%') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a25a0cef0&gt; 图表解析：存活比例仅为38.38% 年龄分析 结合年龄数据，分析幸存下来的人有什么特点 问题2：结合性别和年龄，分析幸存下来的人有什么特点 1、年龄数据的分布情况2、男性和女性的存活情况3、老人和小孩的存活情况 123456789101112131415161718192021222324252627# 年龄分布sum(train['Age'].isnull())# 去除缺失值train_age = train[train['Age'].notnull()]# 画布plt.figure(figsize = (12,6))plt.subplot(121)# 绘制直方图train_age['Age'].hist(bins = 70)plt.xlabel('Age')plt.ylabel('Num')# 绘制箱型图plt.subplot(122)train_age.boxplot(column = 'Age',showfliers = False)train_age['Age'].describe()# a = train_age['Age'].describe()# print('总体年龄分布：去掉缺失值后样本值共：%.f条，平均年龄为：%.2f，标准差为：%.f，最小年龄为：%.2f，最大年龄为：%.f' % (a['count'],a['mean'],a['std'],a['min'],a['max'])) count 714.000000 mean 29.699118 std 14.526497 min 0.420000 25% 20.125000 50% 28.000000 75% 38.000000 max 80.000000 Name: Age, dtype: float64 图表解析： 总体年龄分布：去掉缺失值后样本值共：714条，平均年龄为：29.70，标准差为：15，最小年龄为：0.42，最大年龄为：80 性别分析 分析男性和女性的存活情况 12345678910111213# 男性和女性存活情况train[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(figsize = (12,6))survived_sex = train[train['Survived'] == 1]['Sex'].value_counts()dead_sex = train[train['Survived'] == 0]['Sex'].value_counts()df = pd.DataFrame([survived_sex,dead_sex])df.index = ['Survived','Dead']df.plot(kind = 'bar',stacked = True,figsize = (12,6))# survive_sex = train.groupby(['Sex','Survived'])['Survived'].count()# print('女性存活率为%.2f%%，男性存活率为%.2f%%' % (survive_sex.loc['female',1].sum()/survive_sex.loc['female'].sum().sum()*100,survive_sex.loc['male',1].sum()/survive_sex.loc['male'].sum().sum()*100)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2620ee10&gt; 图表解析：不难看出，无论是总体上，还是在幸存者中的女性占比都远大于男性。可得出女性幸存率是远大于男性 年龄分析 - 老人与小孩 结合年龄数据，分析存活者中老人与小孩的占比 12345678910111213# 年龄与存活的关系 -- 老人与小孩的存活率fig,ax = plt.subplots(1,2,figsize = (12,8))sns.violinplot('Pclass','Age',hue = 'Survived',data = train, split = True,ax = ax[0])ax[0].set_title('Pclass and Age vs Survived')ax[0].set_yticks(range(0,110,10))sns.violinplot('Sex','Age',hue = 'Survived',data = train, split = True,ax = ax[1])ax[1].set_title('Sex and Age vs Survived')ax[1].set_yticks(range(0,110,10)) [&lt;matplotlib.axis.YTick at 0x1a2617af60&gt;, &lt;matplotlib.axis.YTick at 0x1a25fb6d68&gt;, &lt;matplotlib.axis.YTick at 0x1a252fcc88&gt;, &lt;matplotlib.axis.YTick at 0x1a25b9c6d8&gt;, &lt;matplotlib.axis.YTick at 0x1a25b9cbe0&gt;, &lt;matplotlib.axis.YTick at 0x1a25b9c160&gt;, &lt;matplotlib.axis.YTick at 0x1a25bc65c0&gt;, &lt;matplotlib.axis.YTick at 0x1a25bc6c18&gt;, &lt;matplotlib.axis.YTick at 0x1a25fd8e10&gt;, &lt;matplotlib.axis.YTick at 0x1a25bc6668&gt;, &lt;matplotlib.axis.YTick at 0x1a25b9c588&gt;] 图表解析： 从小提琴图中可以看出： 1、在按照不同船舱等级划分时：船舱等级越高，存活者的年龄越大；而在船舱等级1中存活年龄集中在20-40岁，在船舱等级2和船舱等级3中有较多的低龄乘客存活 2、在按照性别划分时：男性和女性存活者的年龄多数分布在20-40岁，且均有较多的低龄乘客存活，其中女性存活远多余男性 12345678# 老人与小孩的存活情况plt.figure(figsize = (18,6))train_age['Age_int'] = train_age['Age'].astype(int)average_age = train_age[['Age_int','Survived']].groupby(['Age_int'],as_index = False).mean()sns.barplot(x = 'Age_int',y = 'Survived',data = average_age, palette = 'BuPu')plt.grid(linestyle = '--',alpha = 0.5) 图表解析：灾难中，老人和小孩的存活几率较高 123456789plt.figure(figsize = (12,6))plt.hist([train[train['Survived'] == 1]['Age'],train[train['Survived'] == 0]['Age']], stacked = True, color = ['g','r'], bins = 30, label = ['Survived','Dead'])plt.xlabel('Age')plt.ylabel('Number of passengers')plt.legend() &lt;matplotlib.legend.Legend at 0x1a26a845f8&gt; 图表解析：大概在十二岁以下的小孩与近80岁的老人存活率相对高很多 探索亲人的人数与存活 结合SibSp、Parch字段，探索亲人的人数与存活的关系 1、有无兄弟姐妹/父母子女与是否存活之间的关系 2、亲戚人数和存活与否之间的关系 123456789101112131415161718192021222324252627282930313233343536373839404142# 有无兄弟姐妹/父母子女与是否存活之间的关系# 筛选出有无兄弟姐妹的数据sibsp_df = train[train['SibSp'] != 0]no_sibsp_df = train[train['SibSp'] == 0]# 筛选出有无父母子女的数据parch_df = train[train['Parch'] != 0]no_parch_df = train[train['Parch'] == 0]# 绘制饼图plt.figure(figsize = (15,6))plt.subplot(141)plt.axis('equal')sibsp_df['Survived'].value_counts().plot.pie(labels = ['No Survived','Survived'], autopct = '%1.1f%%', explode=[0.1,0])plt.xlabel('sibsp')plt.subplot(142)plt.axis('equal')no_sibsp_df['Survived'].value_counts().plot.pie(labels = ['No Survived','Survived'], autopct = '%1.1f%%', explode=[0.1,0])plt.xlabel('no_sibsp')plt.subplot(143)plt.axis('equal')parch_df['Survived'].value_counts().plot.pie(labels = ['No Survived','Survived'], autopct = '%1.1f%%', explode=[0.1,0])plt.xlabel('parch')plt.subplot(144)plt.axis('equal')no_parch_df['Survived'].value_counts().plot.pie(labels = ['No Survived','Survived'], autopct = '%1.1f%%', explode=[0.1,0])plt.xlabel('no_parch') 图表解析：相比较而言，有兄弟姐妹的乘客存活率会比无兄弟姐妹高；而有父母子女的乘客存活率也会比无父母子女的乘客存活率高 12345678910# 亲戚人数和存活与否之间的关系fig,ax = plt.subplots(1,2,figsize = (15,4))train[['Parch','Survived']].groupby(['Parch']).mean().plot.bar(ax = ax[0])ax[0].set_title('Parch and Survived')train[['SibSp','Survived']].groupby(['SibSp']).mean().plot.bar(ax = ax[1])ax[1].set_title('SibSp and Survived')train['FamilyNum'] = train['Parch'] + train['SibSp'] + 1train[['FamilyNum','Survived']].groupby(['FamilyNum']).mean().plot.bar(figsize = (15,4)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a27223908&gt; 图表解析：如果乘客为独自一人，其存活率并不高；而如果亲友过多，其存活率会更低 票价分析 结合Fare字段，探究票价与是否存活有无关系 1、研究票价分布与是否存活之间的联系 2、研究生还者与未生还者的票价情况 1234# 票价分布fareplot = sns.distplot(train['Fare'],label = 'skewness:%.2f'% (train['Fare'].skew()))fareplot.legend() &lt;matplotlib.legend.Legend at 0x1a272af7f0&gt; 图表解析：可以看出，票价在0-40区间数量最多 123# 船舱等级与票价sns.barplot(data = train,x = 'Pclass',y = 'Fare') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a276fc438&gt; 图表解析：很明显：船舱等级最高为1（票价也最高），依次后排 123456789101112131415161718# 票价与生存1fare_survived = sns.FacetGrid(train,hue = 'Survived',aspect = 3)fare_survived.map(sns.kdeplot,'Fare',shade = True)fare_survived.set(xlim = (0,150))fare_survived.add_legend()# 柱状图# figure = plt.figure(figsize = (15,6))# plt.hist([fare_survived,fare_not_survived],stacked = True,# color = ['g','r'],bins = 30,# label = ['Survived','Dead'])# plt.xlabel('Fare')# plt.ylabel('Survived')# plt.legend() &lt;seaborn.axisgrid.FacetGrid at 0x1a27758a90&gt; 图表解析：当票价低于18左右时，该部分乘客的生存率较低；而随着票价越高生存率一般较高 123456789101112# 票价与生存2# 基于票价，筛选是否存活的数据fare_not_survived = train['Fare'][train['Survived'] == 0]fare_survived = train['Fare'][train['Survived'] == 1]avg_fare = pd.DataFrame([fare_not_survived.mean(),fare_survived.mean()])std_fare = pd.DataFrame([fare_not_survived.std(),fare_survived.std()])avg_fare.plot(yerr = std_fare,kind = 'bar',legend = False, figsize = (15,4),grid = True)plt.xticks(rotation = 360) (array([0, 1]), &lt;a list of 2 Text xticklabel objects&gt;) 图表解析：生还者的平均票价要大于未生还者的平均票价 12345678# 不同船舱等级的平均票价ax = plt.subplot()ax.set_ylabel('Average Fare')train.groupby('Pclass').mean()['Fare'].plot(kind = 'bar', figsize = (15,6), ax = ax)plt.xticks(rotation = 360) (array([0, 1, 2]), &lt;a list of 3 Text xticklabel objects&gt;) 图表解析：船舱等级高，船票均价自然高 12345678910# 票价、年龄和生存与否plt.figure(figsize = (15,8))ax = plt.subplot()ax.scatter(train[train['Survived'] == 1]['Age'],train[train['Survived'] == 1]['Fare'],c = 'green',s = 40)ax.scatter(train[train['Survived'] == 0]['Age'],train[train['Survived'] == 0]['Fare'],c = 'red',s = 40)ax.set_xlabel('Age')ax.set_ylabel('Fare')ax.legend(('survived','dead'),scatterpoints = 1, loc = 'upper right',fontsize = 15) &lt;matplotlib.legend.Legend at 0x1a27f58e10&gt; 图表解析：基本与上面的分析相吻合，票价高的存活率高，且老人和小孩的生存率也相对而言较高 登船港口分析 结合Embarked字段，分析登船港口与是否存活之间的关系 123456789# 登船港口与是否存活survived_embark = train[train['Survived'] == 1]['Embarked'].value_counts()dead_embark = train[train['Survived'] == 0]['Embarked'].value_counts()df_embark = pd.DataFrame([survived_embark,dead_embark], index = ['Survived','Dead'])df_embark.plot(kind = 'bar',stacked = True,figsize = (15,7))plt.xticks(rotation = 360) (array([0, 1]), &lt;a list of 2 Text xticklabel objects&gt;) 图表解析：S港口登船的乘客基数最大，而在C港口登船的乘客的存活率相对而言高于另外两个港口 123456# 不同港口登船的存活情况sns.barplot(data = train,x = 'Embarked',y = 'Survived')print('Embarked为S的乘客,其生存率为%.2f' % train['Survived'][train['Embarked'] == 'S'].value_counts(normalize = True)[1])print('Embarked为C的乘客,其生存率为%.2f' % train['Survived'][train['Embarked'] == 'C'].value_counts(normalize = True)[1])print('Embarked为Q的乘客,其生存率为%.2f' % train['Survived'][train['Embarked'] == 'Q'].value_counts(normalize = True)[1]) Embarked为S的乘客,其生存率为0.34 Embarked为C的乘客,其生存率为0.55 Embarked为Q的乘客,其生存率为0.39 图表解析：登船港口为C的乘客存活率较高，探究是否与船舱等级比例有关联 1234# 查看不同登船地点乘客的各舱位的乘客数量对比分析sns.factorplot('Pclass',col = 'Embarked',data = train, kind = 'count',size = 3) &lt;seaborn.axisgrid.FacetGrid at 0x1a2821d908&gt; 图表解析：登船港口为C的乘客其头等舱所占的比例相对较高 123# 查看船舱等级与存活率的情况sns.barplot(data = train,x = 'Pclass',y = 'Survived') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a28789438&gt; 图表解析：登船港口为C的乘客其存活率较高的原因是为其是头等舱乘客占比较大，而头等舱乘客存活率也比较高 数据处理1234# 数据合并data = train.append(test,ignore_index = True)data.describe() 123# 查看数据缺失情况data.apply(lambda x:sum(x.isnull())) Age 263 Cabin 1014 Embarked 2 FamilyNum 418 Fare 1 Name 0 Parch 0 PassengerId 0 Pclass 0 Sex 0 SibSp 0 Survived 418 Ticket 0 dtype: int64 缺失值填充 Cabin、Age、Embarked、Fare有缺失值 1234# Cabin缺失值填充,考虑先用U(unknow)来填充data['Cabin'] = data['Cabin'].fillna('U')# data['Cabin'].value_counts() U 1014 C23 C25 C27 6 G6 5 B57 B59 B63 B66 5 ... B73 1 A10 1 A16 1 Name: Cabin, Length: 187, dtype: int64 12345# Embarked缺失值填充：缺失数据只有两条，可以考虑根据Embarked数据分布来填充# data[data['Embarked'].isnull()]data['Embarked'] = data['Embarked'].fillna('S') 1234567# Fare缺失值填充：缺失数据仅一条，观察该数据情况# 考虑用等同舱位、等同登船港口、舱位未知的乘客的平均票价填充# data[data['Fare'].isnull()]data['Fare'] = data['Fare'].fillna(data[(data['Pclass'] == 3)&amp;(data['Embarked'] == 'C')&amp;(data['Cabin'] == 'U')]['Fare'].mean()) 特征工程特征1 - Title 乘客姓名中包含头衔信息，而不同的头衔也在一定程度上反映了乘客的身份，不同身份的乘客其存活率可能有差异，因此可以根据姓名信息提取Title特征，进行分析 12data['Title'] = data['Name'].map(lambda x:x.split(',')[1].split('.')[0].strip())data['Title'].value_counts() Mr 757 Miss 260 Mrs 197 Master 61 Rev 8 Dr 8 Col 4 Mlle 2 Major 2 Ms 2 the Countess 1 Dona 1 Mme 1 Don 1 Capt 1 Lady 1 Sir 1 Jonkheer 1 Name: Title, dtype: int64 123456789101112131415161718192021222324# 进一步处理分类，将相近的Title信息整合为一个类别TitleDict = {}TitleDict['Mr']='Mr'TitleDict['Mlle']='Miss'TitleDict['Miss']='Miss'TitleDict['Master']='Master'TitleDict['Jonkheer']='Master'TitleDict['Mme']='Mrs'TitleDict['Ms']='Mrs'TitleDict['Mrs']='Mrs'TitleDict['Don']='Royalty'TitleDict['Sir']='Royalty'TitleDict['the Countess']='Royalty'TitleDict['Dona']='Royalty'TitleDict['Lady']='Royalty'TitleDict['Capt']='Officer'TitleDict['Col']='Officer'TitleDict['Major']='Officer'TitleDict['Dr']='Officer'TitleDict['Rev']='Officer'data['Title'] = data['Title'].map(TitleDict)data['Title'].value_counts() Mr 757 Miss 262 Mrs 200 Master 62 Officer 23 Royalty 5 Name: Title, dtype: int64 123# 新特征Title分布情况sns.barplot(data = data,x = 'Title',y = 'Survived') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2894ce10&gt; 图表解析：头衔为’Mr’和’Officer’的乘客，存活率明显低于其他几类 1234# 乘客家人人数与是否存活data['FamilyNum'] = data['SibSp'] + data['Parch'] + 1sns.barplot(data = data,x = 'FamilyNum',y = 'Survived') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a28a4b240&gt; 特征2 - FamilySize 根据家庭人数生成FamilySize(家庭规模)类别，有小、中、大三类 1234567891011# 生成新特征def familysize(FamilyNum): if FamilyNum == 1: return 0 elif (FamilyNum &gt;= 2)&amp;(FamilyNum &lt;= 4): return 1 else: return 2data['FamilySize'] = data['FamilyNum'].map(familysize)data['FamilySize'].value_counts() 0 790 1 437 2 82 Name: FamilySize, dtype: int64 1sns.barplot(data = data,x = 'FamilySize',y = 'Survived') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a28b57940&gt; 图表解析：当家庭规模适中时，乘客的生存率更高。 特征3 - Deck Cabin字段的首字母代表客舱的类型，也反映不同乘客群体的特点，可能也与乘客的生存率相关。 1234# 生成新特征Deckdata['Deck'] = data['Cabin'].map(lambda x:x[0])sns.barplot(data = data,x = 'Deck',y = 'Survived') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a28afe748&gt; 图表解析：明显看出，客舱类型为B、D、E的乘客的存活率较高；客舱类型为U、T时，存活率低 特征4 - TicketCount 同一票号的乘客数量有所不同，可能与乘客的生存率也有关系，生成新特征TicketCount 1234567891011121314data['Ticket'].value_counts()#可以看出同一票号的乘客数量有所不同，可能与乘客的生存率也有关系# 生成新特征TicketCountTicketCountdict = {}TicketCountdict = data['Ticket'].value_counts()TicketCountdict.head()# 并入数据集data['TicketCount'] = data['Ticket'].map(TicketCountdict)sns.barplot(data = data,x = 'TicketCount',y = 'Survived') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a28cf7940&gt; 图表解析：当TicketCount大小适中时，乘客的存活率较高 特征5 - TicketGroup123456789101112# 根据TicketCount大小，生成新特征TicketGroup，分为三类def TicketGroup(num): if (num &gt;= 2)&amp;(num &lt;= 4): return 0 elif (num == 1)|((num &gt;=5)&amp;(num &lt;= 8)): return 1 else: return 2 data['TicketGroup'] = data['TicketCount'].map(TicketGroup)sns.barplot(data = data,x = 'TicketGroup',y = 'Survived') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2586a940&gt; 图表解析：与上面分析结果基本一致，TicketGroup较小，也就是TicketCount大概在1-4之间时，存活率最高 未完待续～ 写在最后以上是对泰坦尼克生存预测案例的一部分分析，剩余部分为Age的缺失值处理，以及预测模型的构建，也就是算法部分，目前对算法的了解尚浅，能力不足以处理，待补充。对以上分析有疑义，欢迎指正，感谢阅读～","link":"/2021/03/19/%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E7%94%9F%E5%AD%98%E5%88%86%E6%9E%90/"},{"title":"电商项目：母婴用品购买行为分析","text":"摘要通过母婴类产品客户购买行为数据分析，找出核心的用户群，并分析整理销量的趋势，找出其中需要改进的环节～ 项目描述项目名称：母婴用品数据分析 数据来源：数据来源于阿里云天池，有2个表，记录了用户身份信息，商品购买数量，用户的年龄和商品数等。时间介于12年至15年。 字段说明： baby_trade_history表： ​ user_id：用户id(唯一值) ​ auction_id：购买行为编号 ​ cat_id：商品种类ID ​ cat1：商品属于哪个类别 ​ property：商品属性 ​ buy_mount：购买数量 ​ day：购买时间 mum_baby表： ​ user_id ：用户id ​ birthday ：出生日期 ​ gender： 性别（0 男性；1 女性） 项目目的：通过母婴类产品客户购买行为数据分析，为以下问题提供解释和改进建议：客户对于不同产品的偏好找出核心的用户群，并对其分析整理销量的趋势分析，并找出其中需要改进的环节 环境解释：基于jupyter notebook，可视化工具包：seaborn、matplotlib 12345678910111213# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inlineimport warningswarnings.filterwarnings('ignore')import osos.chdir(r'/Users/nanb/Documents/数据存放') 12goods_df = pd.read_csv('baby_trade_history.csv',sep = ',')baby_df = pd.read_csv('mum_baby.csv',sep = ',') 数据整理字段重命名 为方便字段操作与识别，进行字段重命名 1234# 商品信息表字段重命名goods_df.columns = ['用户id','购买行为编号','商品种类id','商品类别','商品属性','购买数量','购买时间']goods_df.head() 1234# 婴儿信息表字段重命名baby_df.columns = ['用户id','出生日期','性别']baby_df.head() 格式转换 方便以后换算以及添加新特征类别，进行时间格式转换 1234# 时间格式转换goods_df['购买时间'] = pd.to_datetime(goods_df['购买时间'],format = '%Y%m%d')baby_df['出生日期'] = pd.to_datetime(baby_df['出生日期'],format = '%Y%m%d') 字段处理1234# 处理baby表中性别baby_df['性别'] = baby_df['性别'].replace([0,1,2],['男','女','未知'])baby_df.head() 新特征生成123456789101112131415# 购买年份goods_df['购买年份'] = goods_df['购买时间'].dt.year# 购买月份goods_df['购买月份'] = goods_df['购买时间'].astype('datetime64[M]')# goods_df['购买月份'] = goods_df['购买时间'].dt.month# 季度goods_df['季度'] = goods_df['购买时间'].dt.quartergoods_df.head() 123# 根据购买数量进行排序，粗略查看购买数量最多的是哪些商品以及用户goods_df.sort_values(by = '购买数量',ascending = False).head(10) 图表解析：单次购买数量最多的是：50014815；其次为28商品 可视化分析商品类型购买分析123# 商品类别与购买数量goods_df[['商品类别','购买数量']].groupby(['商品类别']).sum().sort_values(by='购买数量',ascending=False).plot.bar() 图表解析：28商品产品销量最高,其次是50014815商品和50008168商品 1234# 查看商品类别与购买的用户之间的联系df1 = goods_df[['商品类别','用户id']].groupby(['商品类别']).count()df1.sort_values(by = '用户id',ascending=False).plot.bar() 图表解析：把汇总依据改成计数项得知，更多用户倾向于5008168产品，其次是28。 由此反应出5008168接受的人群会更多但必须提高用户的粘性让其重复购买的次数增多。 购买时间分析月份分析123456# 根据月份分析购买趋势plt.style.use('ggplot')plt.figure(figsize = (15,8))goods_df.groupby('购买月份')['用户id'].count().plot() 图表解析： 2013年开始到2014年年末，用户的总体消费次数（订单数）趋势呈现上升趋势。其中，2013年和2014年年间，用户的消费波动也是相似的。13年订单数有2个高峰，分别是5月和11月。14年订单数有2个高峰，分别是5月在和11月。 1234# 购买月份与购买数量plt.figure(figsize = (15,8))goods_df.groupby(by = '购买月份')['购买数量'].sum().plot() 图表解析：2013年11月和2014年11月，由于双11活动，销量有明显的上升。其中，14年的销量明显比13年高出很多 原因猜测 ：根据淘宝官方数据显示，14年阿里的销售额相当于13年的1.6倍，说明在14年双11活动开始被越来越受到大众关注，商家的促销活动也更加刺激消费，从而导致了销量的攀升 12345# 根据月份统计购买情况goods_df['月份'] = goods_df['购买时间'].dt.monthgoods_df.groupby('月份')['购买数量'].sum().plot(kind = 'bar',rot = 360,figsize = (15,8)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a22605a20&gt; 图表解析：根据月份统计销售情况，11月是全年最高的，可能由于双十一促销或者商家活动所导致销量攀升 季度分析123456# 根据季度分析购买情况fig,axes = plt.subplots(2,1,figsize = (15,12))goods_df.groupby('季度')['购买数量'].sum().plot(kind = 'bar',ax = axes[0],rot = 360)goods_df.groupby(['购买年份','季度'])['购买数量'].sum().plot(kind = 'bar',ax = axes[1],rot = 360) 图表解析： 根据季度统计，第四季度销量最高；第一第二季度销量都不高而细分到每一年每一季度查看时：也发现每年的第一第二季度销量都不高，可认定为产品淡季；而第三第四季度销量很好，属于产品的旺季，即秋天后是母婴产品销售的旺季而第四季度的销量最好，很大可能是双十一的影响根据前面图表分析入秋后销量增加，到2月又开始下滑。应在2月份发起促销清理库存，入秋后再囤积货物。 产品需求分析购买数量对比12df = pd.merge(baby_df,goods_df,on = '用户id',how = 'right')df.head() 1234# 根据性别划分，统计购买数量plt.figure(figsize = (15,8))df.groupby(by = '性别')['购买数量'].sum().plot(kind = 'bar',rot = 360,figsize = (15,8)) 产品购买趋势12sex_df = df.dropna(axis = 0,how = 'any')sex_df 1234567891011plt.figure(figsize = (15,7))plt.title('男女婴儿产品需求折线图')man_df = sex_df.loc[sex_df['性别'] == '男']man_df.groupby('购买月份')['用户id'].count().plot(label = '男')famale_df = sex_df.loc[sex_df['性别'] == '女']famale_df.groupby('购买月份')['用户id'].count().plot(label = '女')plt.legend()plt.show() 图表解析：上图可以看到，女婴销量比男婴少的原因是在14年的6月到9月，销量有个明显跌落。但在总体的产品需求趋势来看，男女婴儿大致上相同。所以商家的计划上新的产品量可以男女平均分配 商品类别需求12345df5 = df[['性别','商品类别','购买数量']]df_33 = pd.crosstab(index = df5['性别'],columns = df5['商品类别'])df_33.plot(kind = 'bar',figsize = (15,8),fontsize = 15)plt.xticks(rotation = 360) 图表解析：男女婴儿对50008168商品需求最大，且男婴比例较女婴大，正对50008168商品可以多向男婴做推广或打造爆款 用户分析消费分析12group_user = df.groupby('用户id')group_user['购买数量'].sum().describe() count 29944.000000 mean 2.546420 std 64.015736 min 1.000000 25% 1.000000 50% 1.000000 75% 1.000000 max 10000.000000 Name: 购买数量, dtype: float64 图表解析：平均数是2，标准差是64，表明波动比较大。中位数是1，上下四分位数也是1，说明大部分用户只购买1件。最大值是10000，说明有部分刚需用户拉大了平均值。这批用户值得开发维护。 123e = group_user.sum().query('购买数量 &lt; 17')e['购买数量'].plot.hist(bins = 16,figsize = (15,8)) 图表解析：可以看出绝大部分用户只购买了一件 行为分析用户首购12345678910# 用户首购 -- 第一次购买df['购买月份'].value_counts().index.sort_values()# s = df.groupby('用户id')['购买时间'].min().value_counts().sort_values()# s.tail()df.groupby('用户id')['购买时间'].min().value_counts().plot(figsize = (15,8)) 图表解析： 用户首购的时间分布范围比较广，从2012-7到2015-2都有，呈现稳步上升趋势。 其中：2013-11-11 购买用户数：1392013-12-12 购买用户数：1422014-11-11 购买用户数：4542014-12-12 购买用户数：221 说明双十一打折促销活动对用户的吸引很大，新增用户比平时要多很多，建议运营人员多关注好的渠道，即找最少的投入，但有最好效果的渠道去做优化。后续还可以对广告页面做A/B Testing，把好的元素保留，不好的再进行优化。 用户流失123# 用户流失 -- 最后一次购买df.groupby('用户id')['购买时间'].max().value_counts().plot(figsize = (15,8)) 图表解析：最后一次购买和第一次购买图形很类似，原因应该是大部分用户就进行一次消费 1234# 验证大部分用户就进行一次消费user_lv = group_user['购买时间'].agg(['min','max'])(user_lv['min'] == user_lv['max']).value_counts() True 29920 False 24 dtype: int64 图表解析：有29920位用户仅消费一次，只有24位用户进行了多次消费 复购率123456# 复购率b_more = df.pivot_table(index = '用户id',columns = '购买月份', values = '购买时间', aggfunc = 'count').fillna(0)b_more.head() 1234# 数据筛选purchase_r = b_more.applymap(lambda x:1 if x&gt;1 else np.NaN if x == 0 else 0)#purchase_r.head() 1234# 复购率折线图m = (purchase_r.sum()/purchase_r.count())m.plot(figsize = (15,8)) 图表解析：复购率最高在0.14%，可以得知大部分用户只购买一次 总结 每月用户的消费趋势 订单数：2013年开始到2014年年末，用户的总体消费次数（订单数）趋势呈现上升趋势。同时用户的消费波动也是相似的。6月和11月是消费高峰期。 成交数量：随着阿里双11越来越被大众接受，产品成交数量也是越来越高。 成交用户数：每月的消费人数和每月的订单数趋势波动基本相同。 男女婴儿产品成交数量的比例与趋势：男女婴儿的产品需求总体趋势来看是1:1。 建议商家的计划上新的产品量可以男女平均分配。 用户个体的消费情况分析 每位用户的成交数量的描述统计： 平均数是2，标准差是64，表明波动比较大。 中位数是1，上下四分位数也是1，说明大部分用户只购买1件。 最大值是10000，说明有部分刚需用户拉大了平均值。建议这批用户值得开发维护。 用户消费行为分析 用户的第一次消费（首购）：用户首购的时间分布范围比较广，从2012-7到2015-2都有，呈现稳步上升趋势，说明新客的渠道，商家的运营做的都还可以。另外14年双11和双12同比13年上涨了56%，64%，说明打折促销活动对用户的吸引很大，新增用户环比前几月也明显增多。 建议运营人员多关注好的渠道，即找最少的投入，但有最好效果的渠道去做优化。后续还可以对广告页面做A/B Testing，把好的元素保留，不好的再进行优化。 用户的最后一次消费（流失），用户复购率：由于这个数据集是网上来源，有缺失，大部分用户只消费1次，所以流失和复购率不是很准。 写在最后以上是对母婴用品数据做的一下分析与可视化，数据内容还有待挖掘。文中有任何错误，欢迎指正！感谢阅读～","link":"/2021/01/29/%E6%AF%8D%E5%A9%B4%E4%BA%A7%E5%93%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"title":"MySQL中如何对留存进行分析？","text":"摘要关于留存的基本概念认知，以及在MySQL中如何对留存进行分析的场景模拟.. 留存的基本概念关于界定/标准 新增 = 新 + 增，理论上在处于一个流程内的不同的节点，都可以作为一次新增；例如： 用户通过不同渠道衔接进入到渠道页 下载 安装启动 激活行为：例如注册、购买商品等 新的定义 基于设备：用户第一次下载安装启动，记录设备；再次安装则不记录；另还有不同系统的设备的区分，即用户数 = 访问过服务的设备数 基于帐号关联，即用户数 = 访问过服务的ID数 留存的定义：某段时间内的新增用户，经过一段时间后，仍继续使用应用的，为留存用户； 留存一般是离散的概念，不要求用户在N天内每一天都登录或者使用 统计留存用户的时间粒度： 自然日：先列出每个新用户第一次登录的日期，以及此日期之后仍登录的日期 至于是第几天还是几天后，这个根据不同的业务有不同的定义 次日留存：即第一次登录日期之后，第二天也登录的用户；即登录日期的差值为1天 三日留存：即第一次登录日期之后，第三天也登录的用户；即登录日期的差值为3天 七日留存：即第一次登录日期之后，第七天也登录的用户；即登录日期的差值为1天 自然周 需要注意的是，所谓周留存与七日留存并不是同概念 自然月 留存率 概念：登录用户数 / 新增用户数 × 100% 次日留存率 = (当天新增用户中，第二天还登录的用户数) / 第一天新增的总用户数 3日留存率 = (当天新增用户中，往后的第3天还登录的用户数) / 第一天新增的总用户数 7日留存率 = (当天新增用户中，往后的第7天还登录的用户数) / 第一天新增的总用户数 最主要的是该留存率指标的界定标准，例如：怎么样的用户才算新增用户、用户在第三天登录还是第四天还有登录为3日留存等，这个是根据项目的不同来定义的 MySQL - 场景模拟 [题目] 用户行为信息表存有以下字段：用户id、应用名称、启用时长、启动次数以及登录时间 uid：用户的唯一标识 app_name：应用的名称 duration：某一天中使用了某应用多长时间(分钟) times：某一天中启动了某应用多少次 login_date：登录的日期 现需要统计某日活跃用户在后续一周内的留存情况，也就是计算活跃用户数、次日留存用户数、3日留存用户数、7日留存用户数、次日留存率、3日留存率以及7日留存率 创建数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131#创建表CREATE TABLE userinfo( uid varchar(10) not null comment &quot;用户ID&quot;, app_name varchar(20) comment &quot;应用名称&quot;, duration int(10) comment &quot;启用时长&quot;, times int(10) comment &quot;启动次数&quot;, login_date date comment &quot;登录时间&quot;);#查看表desc userinfo;+------------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+------------+-------------+------+-----+---------+-------+| uid | varchar(10) | NO | | NULL | || app_name | varchar(20) | YES | | NULL | || duration | int(10) | YES | | NULL | || times | int(10) | YES | | NULL | || login_date | date | YES | | NULL | |+------------+-------------+------+-----+---------+-------+#插入数据insert into userinfo(uid,app_name,duration,times,login_date)values(01,'相机',1,2,'2018-05-01'),(02,'微信',2,3,'2018-05-02'),(03,'美团',4,2,'2018-05-03'),(04,'微信',6,3,'2018-05-01'),(05,'相机',3,1,'2018-05-03'),(06,'相机',2,3,'2018-05-01'),(07,'相机',2,2,'2018-05-02'),(08,'微信',1,1,'2018-05-01'),(09,'美团',3,2,'2018-05-02'),(10,'相机',4,3,'2018-05-03'),(11,'相机',5,4,'2018-05-02'),(12,'美团',6,5,'2018-05-01'),(13,'微信',7,1,'2018-05-02'),(14,'相机',2,2,'2018-05-03'),(15,'相机',1,3,'2018-05-01'),(01,'美团',1,2,'2018-05-01'),(02,'微信',1,3,'2018-05-04'),(03,'相机',3,2,'2018-05-03'),(04,'微信',4,3,'2018-05-01'),(05,'相机',4,2,'2018-05-03'),(06,'相机',2,2,'2018-05-04'),(07,'美团',2,3,'2018-05-04'),(08,'微信',1,2,'2018-05-04'),(09,'相机',3,3,'2018-05-04'),(10,'相机',4,3,'2018-05-03'),(11,'相机',5,4,'2018-05-01'),(12,'美团',6,5,'2018-05-02'),(13,'微信',5,4,'2018-05-03'),(14,'相机',2,2,'2018-05-02'),(15,'相机',1,2,'2018-05-01'),(01,'美团',1,2,'2018-05-05'),(02,'微信',1,3,'2018-05-06'),(03,'相机',3,2,'2018-05-04'),(04,'微信',4,3,'2018-05-01'),(05,'相机',4,2,'2018-05-04'),(06,'相机',2,2,'2018-05-04'),(07,'美团',2,3,'2018-05-05'),(08,'微信',1,2,'2018-05-06'),(09,'相机',3,3,'2018-05-06'),(10,'相机',4,3,'2018-05-05'),(11,'相机',5,4,'2018-05-04'),(12,'美团',6,5,'2018-05-02'),(13,'微信',5,4,'2018-05-05'),(14,'相机',2,2,'2018-05-06'),(15,'相机',1,2,'2018-05-06'),(01,'美团',1,2,'2018-05-08'),(02,'微信',1,3,'2018-05-06'),(03,'相机',3,2,'2018-05-04'),(04,'微信',4,3,'2018-05-10'),(05,'相机',4,2,'2018-05-08'),(06,'相机',2,2,'2018-05-07'),(07,'美团',2,3,'2018-05-09'),(08,'微信',1,2,'2018-05-09'),(09,'相机',3,3,'2018-05-09'),(10,'相机',4,3,'2018-05-05'),(11,'相机',5,4,'2018-05-05'),(12,'美团',6,5,'2018-05-10'),(13,'微信',5,4,'2018-05-09'),(14,'相机',2,2,'2018-05-06'),(15,'相机',1,2,'2018-05-06'),(01,'美团',1,2,'2018-05-08'),(02,'微信',1,3,'2018-05-06'),(03,'相机',3,2,'2018-05-04'),(04,'微信',4,3,'2018-05-10'),(05,'相机',4,2,'2018-05-10'),(06,'相机',2,2,'2018-05-07'),(07,'美团',2,3,'2018-05-09'),(08,'微信',1,2,'2018-05-08'),(09,'相机',3,3,'2018-05-09'),(10,'相机',4,3,'2018-05-10'),(11,'相机',5,4,'2018-05-05'),(12,'美团',6,5,'2018-05-10'),(13,'微信',5,4,'2018-05-08'),(14,'相机',2,2,'2018-05-07'),(15,'相机',1,2,'2018-05-07'),(01,'美团',1,2,'2018-05-08'),(02,'微信',1,3,'2018-05-06'),(03,'相机',3,2,'2018-05-10'),(04,'微信',4,3,'2018-05-10'),(05,'相机',4,2,'2018-05-10'),(06,'相机',2,2,'2018-05-07'),(07,'美团',2,3,'2018-05-09'),(08,'微信',1,2,'2018-05-08'),(09,'相机',3,3,'2018-05-10'),(10,'相机',4,3,'2018-05-10'),(11,'相机',5,4,'2018-05-05'),(12,'美团',6,5,'2018-05-10'),(13,'微信',5,4,'2018-05-08'),(14,'相机',2,2,'2018-05-07'),(15,'相机',1,2,'2018-05-07');#查看数据select * from userinfo limit 10;+-----+----------+----------+-------+------------+| uid | app_name | duration | times | login_date |+-----+----------+----------+-------+------------+| 1 | 相机 | 1 | 2 | 2018-05-01 || 2 | 微信 | 2 | 3 | 2018-05-02 || 3 | 美团 | 4 | 2 | 2018-05-03 || 4 | 微信 | 6 | 3 | 2018-05-01 || 5 | 相机 | 3 | 1 | 2018-05-03 || 6 | 相机 | 2 | 3 | 2018-05-01 || 7 | 相机 | 2 | 2 | 2018-05-02 || 8 | 微信 | 1 | 1 | 2018-05-01 || 9 | 美团 | 3 | 2 | 2018-05-02 || 10 | 相机 | 4 | 3 | 2018-05-03 |+-----+----------+----------+-------+------------+ 思路&amp;答案 [思路] 读题：现需要统计手机应用中相机的活跃情况，即某日活跃用户在后续一周内的留存情况，也就是计算活跃用户数、次日留存用户数、3日留存用户数、7日留存用户数、次日留存率、3日留存率以及7日留存率 解题： 活跃用户数定义：某日有登录行为记为活跃，首次登录日期记为第一天登录日期 次日留存用户数定义：在第一天登录日期，第二天有登录行为的用户 3日留存用户数定义：在第一天登录日期，第3天有登录行为的用户 7日留存用户数定义：在第一天登录日期，第7天有登录行为的用户 次日留存率定义：次日留存用户数 / 该用户第一天登录日期的活跃用户数 3日留存率定义：3日留存用户数 / 该用户第一天登录日期的活跃用户数 7日留存率定义：7日留存用户数 / 该用户第一天登录日期的活跃用户数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364select login_date,count(distinct uid) as &quot;活跃用户数&quot; from userinfo where app_name = &quot;相机&quot;group by login_date;+------------+-----------------+| login_date | 活跃用户数 |+------------+-----------------+| 2018-05-01 | 4 || 2018-05-02 | 3 || 2018-05-03 | 4 || 2018-05-04 | 5 || 2018-05-05 | 2 || 2018-05-06 | 3 || 2018-05-07 | 3 || 2018-05-08 | 1 || 2018-05-09 | 1 || 2018-05-10 | 4 |+------------+-----------------+select *,timestampdiff(day,a_date,b_date) as &quot;时间间隔&quot;from(select a.uid,a.login_date as a_date,b.login_date as b_datefrom userinfo as a join userinfo as bon a.uid = b.uidwhere a.app_name = &quot;相机&quot;) as c;+-----+------------+------------+--------------+| uid | a_date | b_date | 时间间隔 |+-----+------------+------------+--------------+| 1 | 2018-05-01 | 2018-05-01 | 0 || 3 | 2018-05-03 | 2018-05-03 | 0 || 3 | 2018-05-04 | 2018-05-03 | -1 || 3 | 2018-05-04 | 2018-05-03 | -1 || 3 | 2018-05-04 | 2018-05-03 | -1 || 3 | 2018-05-10 | 2018-05-03 | -7 || 5 | 2018-05-03 | 2018-05-03 | 0 || 5 | 2018-05-03 | 2018-05-03 | 0 || 5 | 2018-05-04 | 2018-05-03 | -1 || 5 | 2018-05-08 | 2018-05-03 | -5 || 5 | 2018-05-10 | 2018-05-03 | -7 || 5 | 2018-05-10 | 2018-05-03 | -7 || 6 | 2018-05-01 | 2018-05-01 | 0 || 6 | 2018-05-04 | 2018-05-01 | -3 || 6 | 2018-05-04 | 2018-05-01 | -3 |#最终答案select a_date,count(distinct d.uid) as &quot;活跃用户数&quot;,count(distinct case when 时间间隔 = 1 then d.uid else null end) as &quot;次日留存用户数&quot;,count(distinct case when 时间间隔 = 3 then d.uid else null end) as &quot;3日留存用户数&quot;,count(distinct case when 时间间隔 = 7 then d.uid else null end) as &quot;7日留存用户数&quot;,count(distinct case when 时间间隔 = 1 then d.uid else null end) / count(distinct d.uid) as &quot;次日留存率&quot;,count(distinct case when 时间间隔 = 3 then d.uid else null end) / count(distinct d.uid) as &quot;3日留存率&quot;,count(distinct case when 时间间隔 = 7 then d.uid else null end) / count(distinct d.uid) as &quot;7日留存率&quot;from (select *,timestampdiff(day,a_date,b_date) as &quot;时间间隔&quot;from(select a.uid,a.login_date as a_date,b.login_date as b_datefrom userinfo as a join userinfo as bon a.uid = b.uidwhere a.app_name = &quot;相机&quot;) as c) as dgroup by a_date;","link":"/2021/10/07/%E7%95%99%E5%AD%98%E7%8E%87%E6%A6%82%E5%BF%B5&MySQL%E6%A1%88%E4%BE%8B/"},{"title":"数据的相关性分析 &amp; 统计分析","text":"摘要怎样判别数据中的变量是否存在相关性？得到数据样本又如何进行整理归档？ 前言前面两篇文章列举了数据特征分析中的三种常见的分析方法：分布分析、对比分析和帕累托分析。接下来介绍另外两种分析方法。 相关性分析在数据特征分析中，研究两个或两个以上随机变量之间相互依存关系的方向和密切程度的方法，就称为相关性分析。而相关性的元素之间需要存在一定的联系或者概率才可以进行相关性分析，所以需要先确定两者之间是否存在相关性。 一般有以下几种方法： 图示处判 两个变量可以通过线性相关进行分析： 1、k&gt;0：正相关，随着另一个变量的增大而增大； 2、k&lt;0：负相关，随着另一个变量的增大而减小 多个变量可以通过散点图矩阵初判多变量之间的关系 123456# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 12345678910111213141516171819# 通过绘制散点图初步判断两个变量之间的线性相关性data1 = pd.Series(np.random.rand(50)*100).sort_values()data2 = pd.Series(np.random.rand(50)*50).sort_values()data3 = pd.Series(np.random.rand(50)*500).sort_values(ascending = False)# 正线性相关fig = plt.figure(figsize = (16,9))ax1 = fig.add_subplot(1,2,1)ax1.scatter(data1,data2)plt.grid()# 负线性相关ax2 = fig.add_subplot(1,2,2)ax2.scatter(data1,data3)plt.grid() 12345678910# 通过绘制散点图初步判断多变量间关系data = pd.DataFrame(np.random.randn(200,4)*100,columns = list('abcd'))pd.plotting.scatter_matrix(data,figsize = (16,9), c = 'k', marker = '*', diagonal= 'hist', alpha=0.7, range_padding=0.1)data.head() 皮尔逊相关系数皮尔逊相关系数，又称皮尔逊积矩相关系数是用于度量两个变量X和Y之间的相关（线性相关） 其值介于-1与1之间,o代表无相关性，负值为负相关，正值为正相关 0 &lt; IrI &lt; 1 ：表示存在不同程度线性相关 IrI &lt;= 0.3 ： 不存在线性相关 0.3 &lt; IrI &lt;= 0.5 ： 低度线性相关 0.5 &lt; IrI &lt;= 0.8 ： 显著线性相关 IrI &gt; 0.8 ： 高度线性相关 tips：前提是数据必须满足正态分布 1234567891011121314151617181920212223242526# 皮尔逊相关系数推导from scipy import statsdata1 = pd.Series(np.random.rand(100)*100).sort_values()data2 = pd.Series(np.random.rand(100)*50).sort_values()data = pd.DataFrame({'A':data1.values,'B':data2.values})print(data.head())print('------------------------')jz1,jz2 = data['A'].mean(),data['B'].mean()bzc1,bzc2 = data['A'].std(),data['B'].std()print('A正态性检验：\\n',stats.kstest(data['A'],'norm',(jz1,bzc1)))print('B正态性检验：\\n',stats.kstest(data['B'],'norm',(jz2,bzc2)))print('------------------------')# 正态性检验 - pvalue &gt; 0.05data['(x-jz1)*(y-jz2)'] = (data['A'] - jz1) *(data['B'] - jz2)data['(x-jz1)**2'] = (data['A'] - jz1) ** 2data['(y-jz2)**2'] = (data['B'] - jz2) ** 2print(data.head())print('------------------------')r = data['(x-jz1)*(y-jz2)'].sum() / (np.sqrt(data['(x-jz1)**2'].sum() * data['(y-jz2)**2'].sum()))print('Pearson相关系数为：%.4f' % r) 1234567891011121314151617181920 A B0 1.857207 0.4870411 6.543581 0.4988902 7.895966 0.6502853 7.992037 0.8375124 9.657230 1.412577------------------------A正态性检验： KstestResult(statistic=0.06887931747546477, pvalue=0.7404141658459595)B正态性检验： KstestResult(statistic=0.11163895057364082, pvalue=0.1532773076744851)------------------------ A B (x-jz1)*(y-jz2) (x-jz1)**2 (y-jz2)**20 1.857207 0.487041 1219.195525 2588.810639 574.1778511 6.543581 0.498890 1106.353230 2133.883984 573.6101302 7.895966 0.650285 1067.174612 2010.768847 566.3811903 7.992037 0.837512 1056.510686 2002.162152 557.5047104 9.657230 1.412577 992.418885 1855.915085 530.679045------------------------Pearson相关系数为：0.9890 Pearson算法123456789101112# Pearson相关系数 - 算法data1 = pd.Series(np.random.rand(100)*100).sort_values()data2 = pd.Series(np.random.rand(100)*50).sort_values()data = pd.DataFrame({'A':data1.values,'B':data2.values})print(data.head())print('------------------------')data.corr()# pandas相关性方法：data.corr(method='pearson', min_periods=1) → 直接给出字段的相关系数矩阵# method默认为'pearson' 1234567 A B0 2.935213 0.0976181 3.352383 0.7747132 5.230225 0.8463523 5.456244 1.0101884 6.479587 1.077713------------------------ 斯皮尔曼相关系数当数据源不服从正态分布的变量、分类的关联性时，可采用斯皮尔曼相关系数，也称为等级相关系数。计算逻辑：对两个变量成对的取值按照从小到大顺序编秩，Rx代表Xi的秩次，Ry代表Yi的秩次如果两个值大小一样，则秩次为(index1 + index2) / 2 di = Rx - Ry Sperman系数和Pearson系数在效率上等价 0 &lt; IrI &lt; 1 表示存在不同程度线性相关 IrI &lt;= 0.3 → 不存在线性相关 0.3 &lt; IrI &lt;= 0.5 → 低度线性相关 0.5 &lt; IrI &lt;= 0.8 → 显著线性相关 IrI &gt; 0.8 → 高度线性相关 1234567891011121314151617181920212223# Sperman秩相关系数 - 推导data = pd.DataFrame({'智商':[106,86,100,101,99,103,97,113,112,115], '每周看电视小时数':[7,0,27,50,12,29,20,28,6,17]})print(data)print('----------------------')data.sort_values('智商',inplace = True)data['range1'] = np.arange(1,len(data)+1)data.sort_values('每周看电视小时数',inplace = True)data['range2'] = np.arange(1,len(data)+1)print(data)print('-----------------------')data['d'] = data['range1'] - data['range2']data['d2'] = data['d'] ** 2print(data)print('-----------------------')n = len(data)rs = 1 - 6 * (data['d2'].sum()) / (n * (n**2 - 1))print('Sperman相关系数为：%.4f' % rs) 1234567891011121314151617181920212223242526272829303132333435363738 智商 每周看电视小时数0 106 71 86 02 100 273 101 504 99 125 103 296 97 207 113 288 112 69 115 17---------------------- 智商 每周看电视小时数 range1 range21 86 0 1 18 112 6 8 20 106 7 7 34 99 12 3 49 115 17 10 56 97 20 2 62 100 27 4 77 113 28 9 85 103 29 6 93 101 50 5 10----------------------- 智商 每周看电视小时数 range1 range2 d d21 86 0 1 1 0 08 112 6 8 2 6 360 106 7 7 3 4 164 99 12 3 4 -1 19 115 17 10 5 5 256 97 20 2 6 -4 162 100 27 4 7 -3 97 113 28 9 8 1 15 103 29 6 9 -3 93 101 50 5 10 -5 25-----------------------Sperman相关系数为：0.1636 Sperman算法12345678# Sperman秩相关系数 - 算法data = pd.DataFrame({'智商':[106,86,100,101,99,103,97,113,112,115], '每周看电视小时数':[7,0,27,50,12,29,20,28,6,17]})print(data)print('----------------------')data.corr(method = 'spearman') 12345678910111213 智商 每周看电视小时数0 106 71 86 02 100 273 101 504 99 125 103 296 97 207 113 288 112 69 115 17---------------------- 统计分析 统计分析，指对收集到的有关数据资料进行整理归类并进行解释的过程 统计指标对定量数据进行统计描述，常从集中趋势和离中趋势两个方面进行分析 集中趋势算数平均数12345678910111213141516171819# 集中趋势度量：指一组数据向某一中心靠拢的倾向，核心在于寻找数据的代表值或中心值，即统计平均数# 算数平均数data = pd.DataFrame({'value':np.random.randint(100,120,100), 'f':np.random.rand(100)})data['f'] = data['f'] / data['f'].sum()data.head()print('-------------')# 简单算数平均值 = 总和 / 样本数量 (不涉及权重)mean = data['value'].mean()print('简单算数平均值为：%.2f' % mean)#加权算数平均值 = (x1f1 + x2f2 + ... + xnfn) / (f1 + f2 + ... + fn) mean_w = (data['value'] * data['f']).sum() / data['f'].sum()print('加权算数平均值为：%.2f' % mean_w) 123-------------简单算数平均值为：109.20加权算数平均值为：109.26 位置平均数123456789101112131415161718192021222324252627# 位置平均数data = pd.DataFrame({'value':np.random.randint(100,130,100), 'f':np.random.rand(100)})data['f'] = data['f'] / data['f'].sum()print(data.head())print('-----------------')m = data['value'].mode().tolist()print('众数为' % m)med = data['value'].median()print('中位数为%i' % med)plt.rc('font',family = 'simhei',size = 15)plt.figure(figsize = (16,6))data['value'].plot(kind = 'kde',style = '--k',grid = True)plt.axvline(mean,color = 'r',linestyle = '--',alpha = 0.6)plt.text(mean + 5,0.005,'简单算数平均值为：%.2f' % mean,color = 'r')plt.axvline(mean_w,color = 'b',linestyle = '--',alpha = 0.6)plt.text(mean + 5,0.01,'加权算数平均值为：%.2f' % mean_w,color = 'b')plt.axvline(med,color = 'g',linestyle = '--',alpha = 0.6)plt.text(med + 5,0.015,'简单算数平均值为：%.2f' % med,color = 'g') 123456789 value f0 105 0.0025971 118 0.0005802 122 0.0044483 122 0.0058034 104 0.000088-----------------众数为中位数为116 1Text(121.0, 0.015, '简单算数平均值为：116.00') 离中趋势极差与分位差1234567891011121314151617181920212223242526# 离中趋势度量 - 指一组数据中各数据以不同程度的距离偏离中心的趋势# 极差、分位差data = pd.DataFrame(np.random.rand(30,2)*1000, columns = ['A_sale','B_sale'], index = pd.period_range('20180801','20180830'))print(data.head())print('---------------')a_r = data['A_sale'].max() - data['A_sale'].min()b_r = data['B_sale'].max() - data['B_sale'].min()print('A销售额的极差为：%.2f,B销售额的极差为：%.2f' % (a_r,b_r))print('---------------')sta = data['A_sale'].describe()stb = data['B_sale'].describe()a_iqr = sta.loc['75%'] - sta.loc['25%']b_iqr = stb.loc['75%'] - stb.loc['25%']print('A销售额的分位差为：%.2f,B销售额的分位差为：%.2f' % (a_iqr,b_iqr))print('---------------')color = dict(boxes = 'Green',whiskers = 'Orange',medians = 'Blue', caps = 'Gray')data.plot.box(vert = False,grid = True,color = color,figsize = (16,8))plt.xlim(0,1000) 1234567891011 A_sale B_sale2018-08-01 284.722111 866.7028502018-08-02 443.579648 448.1901612018-08-03 365.901111 680.0839812018-08-04 87.793742 984.7903482018-08-05 544.437922 90.517866---------------A销售额的极差为：954.10,B销售额的极差为：953.65---------------A销售额的分位差为：455.72,B销售额的分位差为：637.73--------------- 方差与标准差1234567891011121314151617181920212223242526# 方差、标准差a_std = sta.loc['std']b_std = stb.loc['std']a_var = data['A_sale'].var()b_var = data['B_sale'].var()print('A销售额的标准差为：%.2f,B销售额的标准差为：%.2f' % (a_std,b_std))print('A销售额的方差为：%.2f,B销售额的方差为：%.2f' % (a_var,b_var))# 方差 → 各组中数值与算数平均数离差平方的算数平均数# 标准差 → 方差的平方根# 标准差是最常用的离中趋势指标 → 标准差越大，离中趋势越明显fig = plt.figure(figsize = (16,6))ax1 = fig.add_subplot(1,2,1)data['A_sale'].plot(kind = 'kde',style = 'k--',grid = True,title = 'A密度曲线')plt.axvline(sta.loc['50%'],color = 'r',linestyle = '--', alpha = 0.6)plt.axvline(sta.loc['50%']-a_std,color = 'b',linestyle = '--', alpha = 0.6)plt.axvline(sta.loc['50%']+a_std,color = 'b',linestyle = '--', alpha = 0.6)ax2 = fig.add_subplot(1,2,2)data['B_sale'].plot(kind = 'kde',style = 'k--',grid = True,title = 'B密度曲线')plt.axvline(stb.loc['50%'],color = 'r',linestyle = '--', alpha = 0.6)plt.axvline(stb.loc['50%']-b_std,color = 'b',linestyle = '--', alpha = 0.6)plt.axvline(stb.loc['50%']+b_std,color = 'b',linestyle = '--', alpha = 0.6) 12A销售额的标准差为：270.43,B销售额的标准差为：315.20A销售额的方差为：73133.58,B销售额的方差为：99350.72 总结数据特征分析的几个基本分析思维已学习完，分别有：分布分析、对比分析、帕累托分析、相关性分析以及统计分析。这五种分析思路是对数据特征进行分析的基础，可以引导自己快速将数据整理归类，记录下来以供随时翻阅，感谢阅读～","link":"/2020/10/22/%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/"},{"title":"什么是监督学习和非监督学习？","text":"摘要监督学习与非监督学习的基本认识，以及常用的几种基础算法的介绍～ 前言本文将记录下学习数据分析中几个最为常见的基本算法，先来了解一下其算法概念 监督学习 监督学习，是一个机器学习中的方法。通过已有的样本数据的样本值x和结果值y去训练得到一个最优模型,再利用这个模型将所有的输入映射为相应的输出。 根据输出数据又分为回归问题和分类问题。回归问题通常输出是一个连续的数值,分类问题的输出是几个特定的数值。 回归问题 概念：在统计学中,回归分析指的是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法 按照自变量和因变量之间的关系类型,可分为线性回归分析和非线性回归分析，线性回归是监督学习中最为常用，也是最为重要的一个方法 分类问题 监督学习中针对分类问题最常用的算法是：最邻近分类算法，简称KNN。 核心逻辑：在距离空间里，如果一个样本的最接近的k个邻居里，绝大多数属于某个类别，则该样本也属于这个类别 非监督学习 无监督学习，是人工智能网络的一种算法。其目的是去对原始资料进行分类，以便了解资料内部结构。 简言之，就是根据未知类别的训练样本解决模式识别中的各种问题。一般来说非监督学习的训练样本全是特征量x,无结果值y；非监督学习更多时候做聚类或降维 PCA主成分分析 PCA主成分分析是最广泛的无监督算法，也是最基础的降维算法 通过线性变换将原始数据变换为一组各维度线性无关的表示,用于提取数据的主要特征量 → 高维数据降维 K—means聚类 最常用的机器学习聚类算法,且为典型的基于距离的聚类算法 K均值：基于原型的、划分的距离技术，它试图发现用户指定个数(K)的簇，以欧式距离作为相似度测度。 随机算法蒙特卡罗算法 蒙特卡罗算法，又称随机抽样或统计实验方法,是以概率和统计理论方法为基础的一种计算方法 使用随机数(或更常见的伪随机数)来解决很多计算问题,将所求解的问题同一定的概率模型向联系,用电子计算机实现统计模拟或抽样,以获得问题的近似解 蒙特卡罗算法特点：采样越多，越近似最优解。 以下是几种常见算法的基本应用 线性回归线性回归使用最佳的拟合直线在因变量(Y)和一个或多个自变量(X)之间建立一种关系。 简单线性回归(一元线性回归)表达式： Y = a + b * X多元线性回归表达式：Y = a + b1 * X + b2 * X , 可根据给定的预测变量S来预测目标变量的值 tips：核心在于拟合直线与样本值的误差项满足均值为0,方差为某个特定值的正态分布 一元线性回归123456# 导入模块import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 12345678910111213141516171819202122232425262728293031from sklearn.linear_model import LinearRegression# 构建数据rng = np.random.RandomState(1)xtrain = 10 + rng.rand(30)ytrain = 8 + 4 * xtrain + rng.rand(30)fig = plt.figure(figsize = (17,9))ax1 = fig.add_subplot(1,2,1)plt.scatter(xtrain,ytrain,marker = '.',color = 'k')plt.grid()plt.title('样本数据散点图')model = LinearRegression()model.fit(xtrain[:,np.newaxis],ytrain)# 斜率print(model.coef_)# 截距print(model.intercept_)xtest = np.linspace(10,11,1000)ytest = model.predict(xtest[:,np.newaxis])ax2 = fig.add_subplot(1,2,2)plt.scatter(xtrain,ytrain,marker = '.',color = 'k')plt.plot(xtest,ytest,color = 'k')plt.grid()plt.title('线性回归拟合') 123# 输出[4.04484138]7.9992457345747 123456789101112131415161718192021# 误差rng = np.random.RandomState(8)xtrain = 10 * rng.rand(15)ytrain = 8 + 4 * xtrain + rng.rand(15) * 30model.fit(xtrain[:,np.newaxis],ytrain)xtest = np.linspace(0,10,1000)ytest = model.predict(xtest[:,np.newaxis])plt.figure(figsize = (17,9)) # 拟合直线 plt.plot(xtest,ytest,color = 'r',linestyle = '--')plt.scatter(xtrain,ytrain,marker = '.',color = 'k')ytest2 = model.predict(xtrain[:,np.newaxis])plt.scatter(xtrain,ytest2,marker = 'x',color = 'g')plt.plot([xtrain,xtrain],[ytrain,ytest2],color = 'gray')plt.grid()plt.xlim(0,10)plt.title('误差') 1Text(0.5, 1.0, '误差') 多元线性回归123456789101112131415161718192021222324# 构建数据rng = np.random.RandomState(3)xtrain = 10 * rng.rand(150,4)# ytrain = 20 + np.dot(xtrain,[1.5,2,-4,3])ytrain = 20 + np.dot(xtrain,[1.5,2,-4,3]) + rng.rand(150)df = pd.DataFrame(xtrain,columns = ['b1','b2','b3','b4'])df['y'] = ytrainpd.scatter_matrix(df[['b1','b2','b3','b4']],figsize = (17,9), diagonal= 'kde', alpha=0.5,range_padding=0.1)print(df.head())# 查看4个自变量是否有线性相关model = LinearRegression()model.fit(df[['b1','b2','b3','b4']],df['y'])print('斜率为：',model.coef_)print('截距为：%.4f' % model.intercept_)print('线性回归函数为： \\n y = %.1fx1 + %.1fx2 + %.1fx3 + %.1fx4 + %.1f' % (model.coef_[0],model.coef_[1],model.coef_[2],model.coef_[3],model.intercept_)) 12345678910 b1 b2 b3 b4 y0 5.507979 7.081478 2.909047 5.108276 46.3646561 8.929470 8.962931 1.255853 2.072429 52.7517662 0.514672 4.408098 0.298762 4.568332 42.3659203 6.491440 2.784873 6.762549 5.908628 26.2457854 0.239819 5.588541 2.592524 4.151012 34.592464斜率为： [ 1.5012069 2.00117753 -4.00340344 3.00076621]截距为：20.4865线性回归函数为： y = 1.5x1 + 2.0x2 + -4.0x3 + 3.0x4 + 20.5 线性回归模型评估一般通过以下几个参数验证回归模型: SSE → (和方差、误差平方和)：拟合数据和原始数据对应点的误差的平方和 MSE → (均方差、方差)：预测数据和原始数据对应点误差的平方和的均值 RMSE → (均方差、标准差)：回归系统的拟合标准差,就是MSE的平方根 R-square(确定系数)：SSR与SST的比值 SSR：预测数据与原始数据均值之差的平方和 SST：原始数据与均值之差的平方和 → SST = SSE + SSR tips：SSE越接近0,说明模型选择和拟合越好,数据预测也越成功 ; MSE越小越好R-square(确定系数)的取值范围[0,1],越接近1，表明方程的变量对Y的解释能力越强,这个模型对数据拟合的越好 123456789101112131415161718192021222324from sklearn import metricsrng = np.random.RandomState(1)xtrain = 10 * rng.rand(30)ytrain = 8 + 4 * xtrain + rng.rand(30) * 3model = LinearRegression()model.fit(xtrain[:,np.newaxis],ytrain)ytest = model.predict(xtrain[:,np.newaxis])mse = metrics.mean_squared_error(ytrain,ytest)rmse = np.sqrt(mse)# ssr = ((ytest - ytrain.mean())**2).sum()# sst = ((ytrain - ytrain.mean())**2).sum()# r2 = ssr / sstr2 = model.score(xtrain[:,np.newaxis],ytrain)print('均方差MSE为：%.5f' % mse)print('均方根RMSE为：%.5f' % rmse)print('确定系数R-square为：%.5f' % r2) 123均方差MSE为：0.78471均方根RMSE为：0.88584确定系数R-square为：0.99465 KNN-最邻近分类在距离空间里，如果一个样本的最接近的k个邻居里，绝大多数属于某个类别，则该样本也属于这个类别 以下是几个KNN-最邻近分类的案例 1234567891011121314151617181920212223242526272829303132# 导入KNN分类模块from sklearn import neighbors# 构建一组电影数据data = pd.DataFrame({'name':['北京遇上西雅图','喜欢你','疯狂动物城','战狼2','力王','敢死队'], 'fight':[3,2,1,101,99,98], 'kiss':[104,100,81,10,5,2], 'type':['Romance','Romance','Romance','Action','Action','Action']})knn = neighbors.KNeighborsClassifier()knn.fit(data[['fight','kiss']],data['type'])# cat = np.array([[18,90]])print('预测电影类型为：' , knn.predict([[18,90]]))plt.figure(figsize = (17,9))plt.scatter(data[data['type'] == 'Romance']['fight'], data[data['type'] == 'Romance']['kiss'], color = 'r',marker = 'x',label = 'Romance')plt.scatter(data[data['type'] == 'Action']['fight'], data[data['type'] == 'Action']['kiss'], color = 'g',marker = 'x',label = 'Action')plt.grid()plt.legend()plt.scatter(18,90,color = 'r',marker = 'o',label = 'Romance')plt.ylabel('kiss')plt.xlabel('fight')plt.text(18,90,'&lt;你的名字&gt;',color = 'y',fontsize = 18) 1预测电影类型为： ['Romance'] 1Text(18, 90, '&lt;你的名字&gt;') 123456789101112131415161718192021222324# 随机值模拟data2 = pd.DataFrame(np.random.randn(200,2)*50,columns=['fight','kiss'])data2['typetest'] = knn.predict(data2)data2.head()plt.figure(figsize = (17,9))plt.scatter(data[data['type'] == 'Romance']['fight'], data[data['type'] == 'Romance']['kiss'], color = 'r',marker = 'x',label = 'Romance')plt.scatter(data[data['type'] == 'Action']['fight'], data[data['type'] == 'Action']['kiss'], color = 'g',marker = 'x',label = 'Action')plt.grid()plt.legend()plt.scatter(data2[data2['typetest'] == 'Romance']['fight'], data2[data2['typetest'] == 'Romance']['kiss'], color = 'r',marker = 'o',label = 'Romance')plt.scatter(data2[data2['typetest'] == 'Action']['fight'], data2[data2['typetest'] == 'Action']['kiss'], color = 'g',marker = 'o',label = 'Action')plt.grid()plt.legend() 12345678910111213141516171819202122232425262728# 多参数分类from sklearn import datasetsiris = datasets.load_iris()print(iris.keys())print('数据长度为：%i条' % len(iris['data']))print(iris.feature_names)# 特征分类：萼片长度、萼片宽度、花瓣长度、花瓣宽度print(iris.target_names)# print(iris.target)data = pd.DataFrame(iris.data,columns= iris.feature_names)data['target'] = iris.targetty = pd.DataFrame({'target':[0,1,2], 'target_names':iris.target_names})df = pd.merge(data,ty,on = 'target')knn = neighbors.KNeighborsClassifier()knn.fit(iris.data,iris.target)# 多参数进行分类pre_data = knn.predict([[0.2,0.1,0.3,0.4]])print(pre_data)df.head() 1234dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])数据长度为：150条['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']['setosa' 'versicolor' 'virginica'] PCA主成分分析 通过线性变换将原始数据变换为一组各维度线性无关的表示,用于提取数据的主要特征分类 123456789101112131415161718# 定义随机种子rng = np.random.RandomState(8)# 构建数据data = np.dot(rng.rand(2,2),rng.randn(2,200)).Tdf = pd.DataFrame({'X1':data[:,0], 'X2':data[:,1]})print(df.head())print(df.shape)plt.figure(figsize = (16,6))plt.scatter(df['X1'],df['X2'],alpha=0.7,marker = '.')plt.axis('auto')# plt.xlim(-1,2)plt.grid() 1234567 X1 X20 -1.174787 -1.4041311 -1.374449 -1.2946602 -2.316007 -2.1661093 0.947847 1.4604804 1.762375 1.640622(200, 2) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 二维数据降维 - 构建模型,分析主成分&quot;&quot;&quot;PCA参数：('n_components=None', 'copy=True', 'whiten=False', &quot;svd_solver='auto'&quot;,'tol=0.0', &quot;iterated_power='auto'&quot;, 'random_state=None')&quot;&quot;&quot;from sklearn.decomposition import PCApca = PCA(n_components= 1)pca.fit(df)# pca.fit(X, y=None) → 调用fit方法的对象本身,如pca.fit(X),表示用X对pca这个对象进行训练# n_components :PCA算法中所要保留的主成分个数n,也即保留下来的特征个数n# copy :默认为True, → 表示是否在运行算法时,将原始训练数据复制一份# 特征值print(pca.explained_variance_)# 特征向量 - 具有最大方差的成分print(pca.components_)# 特征值个数 - 返回所保留的成分个数nprint(pca.n_components)# 2.797 * (0.779 * x1 + 0.627 * x2)# 数据转换x_pca = pca.transform(df)# 将降维后的数据转换成原始数据x_new = pca.inverse_transform(x_pca)print('original shape：',df.shape)print('transformd shape：',x_pca.shape)print(x_pca[:5])print('-------------------------------')plt.figure(figsize = (17,6))plt.scatter(df['X1'],df['X2'],alpha=0.7,marker = '.')plt.scatter(x_new[:,0],x_new[:,1],alpha=0.8,marker = '.',color = 'r')plt.axis('equal')plt.grid() 1234567891011[2.79699086][[-0.7788006 -0.62727158]]1original shape： (200, 2)transformd shape： (200, 1)[[ 1.77885258] [ 1.8656813 ] [ 3.14560277] [-1.67114513] [-2.41849842]]------------------------------- 12345678910111213141516171819202122232425262728293031323334353637# 多维数据降维from sklearn.datasets import load_digitsdigits = load_digits()print(digits.keys())print('数据长度为：%i条' % len(digits['data']))print('数据形状为：%i条' , digits.data.shape)print(digits.data[:2])print('---------------------------------------')pca = PCA(n_components= 2)pca.fit(digits.data)prj = pca.transform(digits.data)print('original shape：',digits.data.shape)print('transformd shape：',prj.shape)# prj = pca.fit_transform(digits.data)print(prj[:5])print('---------------------------------------')# 查看特征值print(pca.explained_variance_)print('--------------------------------------')# print(pca.components_) #特征向量 - 具有最大方差的成分# print(pca.n_components) #特征值个数 - 返回所保留的成分个数nplt.figure(figsize = (13,6))plt.scatter(prj[:,0],prj[:,1], c = digits.target,edgecolor = 'none',alpha = 0.6, cmap = 'Reds',s = 5)plt.axis('equal')plt.grid()plt.colorbar() 12345678910111213141516dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])数据长度为：1797条数据形状为：%i条 (1797, 64)[[ 0. 0. 5. 13. 9. 1. 0. 0. 0. 0. 13. 15. 10. 15. 5. 0. 0. 3. 15. 2. 0. 11. 8. 0. 0. 4. 12. 0. 0. 8. 8. 0. 0. 5. 8. 0. 0. 9. 8. 0. 0. 4. 11. 0. 1. 12. 7. 0. 0. 2. 14. 5. 10. 12. 0. 0. 0. 0. 6. 13. 10. 0. 0. 0.] [ 0. 0. 0. 12. 13. 5. 0. 0. 0. 0. 0. 11. 16. 9. 0. 0. 0. 0. 3. 15. 16. 6. 0. 0. 0. 7. 15. 16. 16. 2. 0. 0. 0. 0. 1. 16. 16. 3. 0. 0. 0. 0. 1. 16. 16. 6. 0. 0. 0. 0. 1. 16. 16. 6. 0. 0. 0. 0. 0. 11. 16. 10. 0. 0.]]---------------------------------------original shape： (1797, 64)transformd shape： (1797, 2)---------------------------------------[179.0069301 163.71774688] 123456789101112131415161718192021# 主成分筛选pca = PCA(n_components= 10)pca.fit(digits.data)prj = pca.transform(digits.data)print('original shape：',digits.data.shape)print('transformd shape：',prj.shape)# prj = pca.fit_transform(digits.data)prj[:5]print('---------------------------------------')s = pca.explained_variance_c_s = pd.DataFrame({'b':s, 'b_sum':s.cumsum() / s.sum()})c_s['b_sum'].plot(style = '--ko',figsize = (16,6))plt.axhline(0.85,color = 'r',linestyle = '--',alpha = 0.6)plt.text(6,c_s['b_sum'].iloc[6] - 0.08,'第七个成分累计贡献率超过85%',color = 'r')plt.grid() 123original shape： (1797, 64)transformd shape： (1797, 10)--------------------------------------- K-means聚类聚类分析：是一种将研究对象分为相对同质的群组的统计分析技术 将观测对象的群体按照相似性和相异性进行不同群组的划分,划分后每个群组内部各对象相似度很高, 而不同群组之间的对象彼此相异度很高 聚类分析后会产生一组集合,主要用于降维 K均值算法实现逻辑：K均值算法需要输入待聚类的数据和欲聚类的簇数K1、随机生成k个初始点作为质心2、将数据集中的数据按照距离质心的远近分到各个簇中3、将各个簇中的数据求平均值,作为新的质心,重复上一步,直到所有的簇不再改变 12345678910111213141516171819202122232425262728293031323334353637383940414243# make_blobs 聚类数据生成器from sklearn.datasets.samples_generator import make_blobsx,y_ture = make_blobs(n_samples= 300, centers= 4, cluster_std= 0.5, random_state= 0)&quot;&quot;&quot;参数解析：n_samples : 待生成的样本总数centers : 类别数cluster_std : 每个类别的方差,如多类数据不同方差,可设置为区间类似[1.0,3.0]这里针对2类数据random_state : 随机数种子x → 生成数据值 , y → 生成数据对应的类别标签&quot;&quot;&quot;print(x[:5])print(y_ture[:5])print('-------------------------------------')# plt.figure(figsize = (16,6))# plt.scatter(x[:,0],x[:,1],s = 10 , alpha = 0.6)# plt.grid()from sklearn.cluster import KMeanskmeans = KMeans(n_clusters= 4)kmeans.fit(x)y_kmeans = kmeans.predict(x)centroids = kmeans.cluster_centers_plt.figure(figsize = (16,6))plt.scatter(x[:,0],x[:,1],c = y_kmeans,cmap = 'Dark2',s = 50,alpha = 0.5,marker = 'x')plt.scatter(centroids[:,0],centroids[:,1],c = [0,1,2,3],cmap = 'Dark2',s = 70,marker = 'o')plt.title('K-means 300 points\\n')plt.xlabel('value1')plt.ylabel('value2')plt.grid() 1234567[[ 1.03992529 1.92991009] [-1.38609104 7.48059603] [ 1.12538917 4.96698028] [-1.05688956 7.81833888] [ 1.4020041 1.726729 ]][1 3 0 3 1]------------------------------------- 写在最后以上是在数据分析过程中常用到的几种基本算法，可以快速的定位数据分析的方向和数据类型。其中线性回归和主成分分析会更常见一下，这份笔记也是为了随时翻阅而记录，欢迎指正，感谢阅读～","link":"/2020/10/29/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"title":"随机算法之蒙特卡罗","text":"摘要随机算法的基础认知以及简单推导应用～ 随机算法蒙特卡罗算法 蒙特卡罗算法，又称随机抽样或统计实验方法,是以概率和统计理论方法为基础的一种计算方法 使用随机数(或更常见的伪随机数)来解决很多计算问题,将所求解的问题同一定的概率模型向联系,用电子计算机实现统计模拟或抽样,以获得问题的近似解 以一个概率模型为基础,按照这个模型所描绘段得过程,通过模拟实验的结果,作为问题的近似解1、构造或描述概率过程2、实现从已知概率分布抽样3、建立各种估计量 优点：简单快速特点：随机采样上计算得到近似结果,随着采样的增多,得到的结果是正确结果的概率逐渐加大 π的计算123456#导入模块import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline 1234567891011121314151617181920212223242526272829303132333435363738394041# 设置投点次数n = 10000# 设置半径r = 1.0# 圆心a,b = (0.0,0.0)#区域边界xmin,xmax = a-r,a+rymin,ymax = b-r,b+r# 在正方形区域内随机投点# numpy.random.uniform(low,high,size) → 从一个均匀分布[low,high)中随机采样，均匀分布x = np.random.uniform(xmin,xmax,n)y = np.random.uniform(ymin,ymax,n)fig = plt.figure(figsize = (6,6))axes = fig.add_subplot(1,1,1)plt.plot(x,y,'ro',markersize = 1)plt.axis('equal')plt.xlim(-1,1)plt.ylim(-1,1)d = np.sqrt((x - a)**2 + (y - b)**2)res = sum(np.where(d&lt;r,1,0))pi = 4 * res / nprint(pi)# 导入绘制圆形 Circle (椭圆Ellipse)from matplotlib.patches import Circlecircle = Circle(xy = (a,b),radius = r,alpha = 0.5,color = 'gray')axes.add_patch(circle)plt.grid(True,linestyle = '--',linewidth = '0.8')plt.show() 13.1256 积分计算123456789101112131415161718192021222324252627282930313233343536# 设置投点次数n = 10000# 矩形区域边界x_min,x_max = 0.0,1.0y_min,y_max = 0.0,1.0#在矩形区域内随机投点x = np.random.uniform(x_min,x_max,n)y = np.random.uniform(y_min,y_max,n)#统计落在函数图像下方的点的数目def f(x): return x**2res = sum(np.where(y &lt; f(x),1,0))#计算定积分的近似值integral = res / nprint('integral:',integral)fig = plt.figure(figsize = (6,6))axes = fig.add_subplot(111)axes.plot(x,y,'ro',markersize = 1)plt.axis('equal')plt.xlim(0,1)plt.ylim(0,1)xi = np.linspace(0,1,100)yi = xi ** 2plt.plot(xi,yi,'--k')plt.fill_between(xi,yi,0,color = 'gray',alpha = 0.5,label = 'area')plt.grid() 1integral: 0.3367 实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 厕所排队问题# 1、两场电影结束时间相隔较长,互不影响# 2、每场电影结束之后会有20个人想上厕所# 3、这20个人会在0到10分钟内全部到达厕所# 4、每个人上厕所时间在1-3分钟之间# 首先模拟最简单的情况,也就是厕所只有一个位置,不考虑俩人共用的情况则没人必须等上一人完毕方可进行# 参数：到达时间 / 等待时间 / 开始上厕所时间 / 结束时间arrivingtime = np.random.uniform(0,10,size = 20)arrivingtime.sort()workingtime = np.random.uniform(1,3,size = 20)# np.random.uniform 随机数：均匀分布的样本值print(arrivingtime)print('--------------------------')startingtime = [0 for i in range(20)]finishtime = [0 for i in range(20)]waitingtime = [0 for i in range(20)]emptytime = [0 for i in range(20)]startingtime[0] = arrivingtime[0]finishtime[0] = startingtime[0] + workingtime[0]waitingtime[0] = startingtime[0] - arrivingtime[0]print(startingtime[0],workingtime[0],finishtime[0],waitingtime[0])print('---------------------------')# 判断：如果下一个人在上一个人完成之前到达,则 开始时间 = 上一个人的结束时间# 否则：开始时间 = 到达时间,且存在空闲时间 = 到达时间 - 上一个人的完成时间for i in range(1,len(arrivingtime)): if finishtime[i-1] &gt; arrivingtime[i]: startingtime[i] = finishtime[i-1] else: startingtime[i] = arrivingtime[i] emptytime[i] = arrivingtime[i] - finishtime[i-1] finishtime[i] = startingtime[i] + workingtime[i-1] waitingtime[i] = startingtime[i] - arrivingtime[i] print('第%d个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间\\n' % i, arrivingtime[i], startingtime[i], workingtime[i], finishtime[i], waitingtime[i], '\\n')print('arerage waiting time is %f' % np.mean(waitingtime))print('-----------------------------') fig = plt.figure(figsize = (16,7))plt.plot(waitingtime,'-go')plt.grid(True,linestyle = '--',color = 'gray',linewidth = '0.8')plt.title('蒙特卡罗模拟 - 厕所排队问题')plt.show() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[0.16587897 0.62204463 1.12297214 1.61646409 2.2734175 3.58188608 4.06180627 4.42582219 4.92397497 5.00023146 6.3967079 6.82771899 6.96481151 7.21465387 7.34371744 7.51496538 8.68568567 8.94934034 9.61589877 9.7940074 ]--------------------------0.16587897242361538 1.9163595239913254 2.0822384964149405 0.0---------------------------第1个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 0.6220446336305152 2.0822384964149405 2.188198872364582 3.998598020406266 1.4601938627844253 第2个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 1.122972137882483 3.998598020406266 2.764457045410645 6.1867968927708485 2.875625882523783 第3个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 1.6164640948717424 6.1867968927708485 1.8619334107218273 8.951253938181495 4.570332797899106 第4个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 2.27341749721095 8.951253938181495 1.9558472451168596 10.813187348903321 6.677836440970545 第5个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 3.5818860753684123 10.813187348903321 2.4715009771412073 12.76903459402018 7.231301273534909 第6个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 4.061806266415359 12.76903459402018 1.0395021167184366 15.240535571161388 8.70722832760482 第7个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 4.425822187022732 15.240535571161388 2.9938063833706754 16.280037687879826 10.814713384138656 第8个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 4.923974971103213 16.280037687879826 1.5688516924561942 19.2738440712505 11.356062716776613 第9个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 5.0002314565754515 19.2738440712505 1.6350976155013934 20.842695763706693 14.273612614675049 第10个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 6.396707898323078 20.842695763706693 2.78313954407255 22.477793379208087 14.445987865383614 第11个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 6.827718993543048 22.477793379208087 1.4873508450792379 25.260932923280638 15.65007438566504 第12个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 6.96481151330325 25.260932923280638 2.7352000597223616 26.748283768359876 18.29612140997739 第13个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 7.21465386868163 26.748283768359876 2.1947890646530044 29.48348382808224 19.533629899678246 第14个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 7.3437174368142575 29.48348382808224 1.2871138118247274 31.678272892735244 22.139766391267983 第15个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 7.514965384306672 31.678272892735244 1.2507700564206545 32.96538670455997 24.163307508428574 第16个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 8.685685670740416 32.96538670455997 1.9506767657028061 34.216156760980624 24.279701033819553 第17个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 8.949340336055485 34.216156760980624 1.8000774818307035 36.16683352668343 25.26681642492514 第18个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 9.615898768155551 36.16683352668343 1.1238441426072674 37.96691100851414 26.55093475852788 第19个人：到达时间 开始时间 &quot;工作&quot;时间 完成时间 等待时间 9.794007397518186 37.96691100851414 1.6867760138470005 39.09075515112141 28.172903610995952 arerage waiting time is 14.323308----------------------------- 写在结尾以上是对随机算法中的蒙特卡罗算法的一些初步认知。在数据分析中，这种随机算法应用的十分广泛，也是初学者必知必会，有任何不对的地方，恳请指正～感谢阅读！","link":"/2020/11/13/%E9%9A%8F%E6%9C%BA%E7%AE%97%E6%B3%95-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97/"}],"tags":[{"name":"A&#x2F;B-test","slug":"A-B-test","link":"/tags/A-B-test/"},{"name":"数据分析项目","slug":"数据分析项目","link":"/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/"},{"name":"Python库","slug":"Python库","link":"/tags/Python%E5%BA%93/"},{"name":"MySQL练习题","slug":"MySQL练习题","link":"/tags/MySQL%E7%BB%83%E4%B9%A0%E9%A2%98/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"python数据类型","slug":"python数据类型","link":"/tags/python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"name":"技术分享","slug":"技术分享","link":"/tags/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"},{"name":"可视化图表","slug":"可视化图表","link":"/tags/%E5%8F%AF%E8%A7%86%E5%8C%96%E5%9B%BE%E8%A1%A8/"},{"name":"对比分析","slug":"对比分析","link":"/tags/%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/"},{"name":"二八定律","slug":"二八定律","link":"/tags/%E4%BA%8C%E5%85%AB%E5%AE%9A%E5%BE%8B/"},{"name":"名词解析","slug":"名词解析","link":"/tags/%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/"},{"name":"数据指标","slug":"数据指标","link":"/tags/%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87/"},{"name":"数据分布","slug":"数据分布","link":"/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/"},{"name":"正态性检验","slug":"正态性检验","link":"/tags/%E6%AD%A3%E6%80%81%E6%80%A7%E6%A3%80%E9%AA%8C/"},{"name":"相关性分析","slug":"相关性分析","link":"/tags/%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90/"},{"name":"统计分析","slug":"统计分析","link":"/tags/%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/"},{"name":"算法基础","slug":"算法基础","link":"/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"}],"categories":[{"name":"数据分析","slug":"数据分析","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"项目实操","slug":"数据分析/项目实操","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%93%8D/"},{"name":"名词解析","slug":"数据分析/名词解析","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"SQL","slug":"SQL","link":"/categories/SQL/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"技术分享","slug":"技术分享","link":"/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"},{"name":"Matplotlib","slug":"python/Matplotlib","link":"/categories/python/Matplotlib/"},{"name":"可视化","slug":"数据分析/可视化","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"MySQL","slug":"SQL/MySQL","link":"/categories/SQL/MySQL/"},{"name":"数据特征","slug":"数据分析/数据特征","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81/"},{"name":"Seaborn","slug":"python/Seaborn","link":"/categories/python/Seaborn/"},{"name":"post","slug":"blog/post","link":"/categories/blog/post/"},{"name":"算法","slug":"数据分析/算法","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E7%AE%97%E6%B3%95/"},{"name":"data_type","slug":"python/data-type","link":"/categories/python/data-type/"}]}